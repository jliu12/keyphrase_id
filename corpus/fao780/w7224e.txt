<title>Management of agricultural drainage water quality</title>
<section>1</section>
Preface
Irrigation and drainage play a critical role in meeting the food requirements of the world's population on a sustainable basis. More than one third of the total global food harvest comes from an estimated 260 million ha of irrigated lands which is about one sixth of the total cultivated land. The World Food Summit convened by FAO in Rome in 1996, the United Nations Conference on the Environment and Development (UNCED) held in Rio de Janeiro in 1992, and the International Conference on Water and Environment, Dublin 1992, have all emphasized the importance of protecting the world's land and water resources in order to produce the necessary food and preserve the environment.
In humid regions, agricultural drainage is required to remove excess soil water in the plant root zone. In irrigated agriculture, drainage is of critical importance in controlling salinity and waterlogging. In seeking these benefits, drainage systems have sometimes led to adverse environmental impacts. Specifically, the disposal of low quality drainage effluent to water bodies has damaged some aquatic ecosystems, and inhibited the beneficial uses of the receiving water. Public pressure is growing for drainage systems to be planned in ways that ensure that important ecological habitats are preserved, and that agricultural practices do not impair water quality.
This technical document was prepared in response to public statements and societal concerns for environmental protection, and to provide for a safe, secure and sustainable food production system. The need for this publication was identified by the Working Group on Drainage of the International Commission on Irrigation and Drainage (ICID) and was fully supported by FAO.
The publication is intended for use by planners, irrigation and drainage engineers and environmental specialists. It would be useful in project identification and to assess potential adverse water quality impacts of drainage disposal practices and identify alternative mitigation technologies. In addition, monitoring programmes to collect desired information are suggested. The individual papers, prepared by specialists, were edited into a volume by Professor Chandra Madramootoo, McGill University, Quebec, Canada; Mr. William R. Johnston, California, USA, and Professor Lyman S. Willardson, Utah State University, Utah, USA.
The publication is a welcome addition to FAO's Water Reports series. It is hoped that the document will serve to improve the design, installation and operation of drainage systems. This in turn will improve the productivity of poorly drained cropland, and ensure that the twin objectives of food production and preservation of water quality are harmoniously achieved.
FAO welcomes comments, if any, from the readers and the practitioners who use the publication in their work. Comments should be addressed to: Chief, Water Resources, Development and Management Service, Land and Water Development Division, Food and Agriculture Organization, Viale delle Terme di Caracalla, 00100 Rome, Italy.
<section>2</section>
Acknowledgements
The following persons served as members of an ICID ad hoc committee, responsible for guiding the document: Chandra A. Madramootoo (Chair), Martin K. Fritsch, William R. Johnston, Walter J. Ochs, Lambert K. Smedema, Lyman S. Willardson and Daniel Zimmer. Several of the committee members also served as authors, and were responsible for soliciting contributions from other writers. The contributions of all authors and co-authors are deeply appreciated.
Members of the ICID Working Group on Drainage and of the ICID Working Group on Environmental Impacts of Irrigation, Drainage and Flood Control Projects reviewed and provided useful comments on the draft chapter outlines and the final document. We are grateful to Robert Bos, Herman Bouwer, Heiko Diestel, Flemming Konradsen, Wayne Skaggs and Peter Widmoser for serving as external peer reviewers of the document. Thanks are due to the staff of the Water Quality and Environment Group, Water Resources, Development and Management Service of FAO, for the final review of the text.
The financial support provided by FAO to prepare the document is greatly acknowledged. Special thanks are due to Dr. Hans W. Wolter, Chief, Water Resources Development and Management Service, FAO, for his encouragement and support during the preparation of this volume. Dr. M.A. Chitale, Secretary-General of the ICID, and staff of the ICID Central Office in New Delhi are thanked for their support.
The Department of Agricultural and Biosystems Engineering, McGill University, Montreal, Canada, provided logistical and office support during the preparation of the document; Georges Dodds and Maria Gabriel were especially helpful, and their contributions are gratefully acknowledged. The manuscript was edited by Mr. Julian Plummer to whom we are very grateful.
Chandra A. Madramootoo, William R. Johnston and Lyman S. Willardson, Editors
<section>3</section>
List of symbols, abbreviations and acronyms
Ag
Silver
AOP
Advanced oxidation process
As
Arsenic
ASAE
American Society of Agricultural Engineers
ASCE
American Society of Civil Engineers
B
Boron
BOD
Biological oxygen demand
Ca
Calcium
Cd
Cadmium
CFR
Code of Federal Regulations (US)
Cl^-
Chloride ion
cm
Centimetre
cm/year
Centimetres per year
CNA
Comisión Nacional del Agua (Mexico)
CO[3]^2-
Carbonate ion
Cr
Chromium
Cu
Copper
dam³
Cubic decametres
DBCP
Dibromochloropropane
DMSe
Dimethylselenide
DRI
Drainage Research Institute (Egypt)
dS/m
DeciSiemens per metre
EC
Electrical conductivity
EIA
Environmental impact assessment
EIS
Environmental impact study
ERS
Economic Research Service of the USDA
ET
Evapotranspiration
FAO
Food and Agriculture Organization of the United Nations
Fe
Iron
GAC
Granular activated carbon
ha
Hectares
HCO[3]^-
Bicarbonate ion
Hg
Mercury
ICID
International Commission on Irrigation and Drainage
ILRI
International Institute for Land Reclamation and Improvement (The Netherlands)
K
Potassium
kg
Kilogram
LR
Leaching requirement
Mg
Magnesium
m
Metre
m³
Cubic metre
m g/g
Micrograms per gram
m g/litre
Micrograms per litre
md
Millidarcies
mg/kg
Milligrams per kilogram (= parts per million)
mg/litre
Milligrams per litre
mm/day
Millimetres per day
mm/year
Millimetres per year
Mn
Manganese
Mo
Molybdenum
MPa
MegaPascals
N
Nitrogen
Na
Sodium
Ni
Nickel
NO[3]^-
Nitrate ion
N0[3]-N
Nitrate N
O&M
Operation and maintenance
Percent
PO[4]^3-
Phosphate ion
P
Phosphorus
Pb
Lead
PEEM
Panel of Experts for Environmental Management
PVC
Polyvinyl chloride
R&D
Research and development
SAR
Sodium adsorption ratio
SCARP
Salinity control and reclamation project (Pakistan)
Se
Selenium
SO[4]^2-
Sulphate ion
S
Sulphur
t
Ton
TDS
Total dissolved solids
THM
Trihalomethane
TSS
Total soluble salts
U
Uranium
UNCED
United Nations Conference on Environment and Development
USDA
United States Department of Agriculture
USEPA
United States Environmental Protection Agency
UV
Ultraviolet
V
Vanadium
WHO
World Health Organization of the United Nations
WRDP
Water resources development project
Zn
Zinc
<section>4</section>
Glossary of technical terms
Anadromous fish: Fish which at certain seasons ascend rivers from the sea or ocean to spawn, e.g., salmon.
Aquifer: A water bearing stratum of permeable rock, sand or gravel.
Cavernous geology: Underground stratum or zones with rock or soil conditions having caverns or cavities.
Constructed evaporation pond (basin): Area for impoundment of saline drainage water for evapoconcentration (desiccation). Typically constructed by excavating soils from the interior of basins to build up their embankments. Such ponds may contain more than one cell.
Constructed wetland: Conversion of an area into a wetland by building dikes, small dams and/or shaping land to provide an appropriate water regime for hydrophytic vegetation.
Controlled drainage: The operation of a water table management system with water table control structures adjusted to limit drainage system discharge, and to maintain a free water surface (water table) within the soil profile.
Conventional drainage: Subsurface drainage without water table control structures.
Designated salt sinks: A depository for saline waters and salt deposits such as oceans, saline lakes, wetlands or evaporation ponds.
Flow-through wetlands: Constructed wetlands or natural depressional areas prepared to facilitate the controlled movement of surface waters through specially selected vegetation to enhance removal of pollutants and improve water quality.
Free outlet drainage: See conventional drainage.
Geohydraulic characteristics: Physical properties of the soil and subsoil that have an impact on the internal movement of water, nutrients and pollutants.
Helminths: Parasitic worms of the phylum platyhelminthes including digenetic flukes (class Trematoda, e.g., Schistosoma spp.) and tapeworms (class Cestoidea, e.g., Taenia solium, pork tapeworm).
Hydrophytic vegetation: Plants which must be in water, either completely submerged or emersed, for part of their life cycle.
Hypersaline water: Water with excessive or supersaturated salt content.
Imhoff tank: Two-stage wastewater treatment tank combining sedimentation of settleable solids in an upper compartment and anaerobic digestion of the settled solids in a lower compartment.
Improved drainage: Any technique aimed at improving the natural drainage capacity of land; it can be either surface drainage or subsurface drainage, or a combination of both.
Interactive management programme: Real time monitoring and evaluations to operate an agricultural water supply system efficiently and effectively.
Pond mineralogy: Classification of evaporite minerals precipitated out of impounded water. These include calcite, gypsum, halite and thenardite.
Preferential flow: Movement of water and solutes through large cracks or worm holes in the soil profile.
Schistosomiasis: Also known as bilharziasis, this a disease caused by blood flukes (see helminths). The flukes live in the venules of the large intestine (Schistosoma mansoni), small intestine (S. japonicum) and urinary bladder (S. haematobium), and have limited pathogenic effects. Symptoms appear upon the release of eggs which cause ulceration and abscesses of the intestinal or urinary walls, leading to bloody diarrhoea or urine and abdominal pain. S. mansoni is the most common causative organism and S. haematobium the rarest and least severe. S. japonicum causes the most severe symptoms and is the most likely to be fatal.
Sub-irrigation: Operation of a water table management system by adding water to a subsurface drainage system to maintain a free water surface in the soil profile at a desired elevation (see controlled drainage).
Subsurface drainage: Any drainage system (either drainage wells, open ditches or buried drain pipes) that controls the water table.
Surface drainage: Planned removal of excess water from the land surface by means of natural or constructed channels, including the shaping and grading of land surfaces where necessary.
Vertical drainage: A system of deep wells designed to control the water table.
Waterfowl habitat: Vegetation and water regimes which facilitate the breeding, nesting, feeding and cover required for the production and proliferation of ducks, geese and other waterfowl.
Water table management system: See controlled drainage and sub-irrigation.
Wetlands: Lands that are inundated or saturated by surface water or groundwater at a frequency and duration sufficient to support a prevalence of hydrophytic vegetation typically adapted for plant and animal life in saturated soil conditions.
<section>5</section>
Chapter 1 - Introduction
Chandra A. Madramootoo
McGill University, Quebec, Canada
Need for artificial drainage
Agricultural, environmental and socio-economic benefits of drainage
Types of drainage systems
Environmental impact assessment
Water quality issues
Drainage water management and disposal options
Planning and designing drainage systems to protect and enhance water quality
There are several concerns about the sustainability of irrigation and drainage projects, and there are water quality problems related to the disposal of drainage water. There are also problems with land degradation due to irrigation induced salinity and waterlogging. There have been instances where saline or high nutrient drainage water has damaged aquatic ecosystems. Drainage continues to be a vital and necessary component of agricultural production systems. In order to enhance the net benefits of drainage systems, more attention will need to be given to the water quality impacts of drainage water disposal. This document identifies potential problems and management options in the development, production, treatment and disposal of agricultural drainage water.
Need for artificial drainage
Excess water in the crop root zone soil is injurious to plant growth. Crop yields are drastically reduced on poorly drained soils, and, in cases of prolonged waterlogging, plants eventually die due to a lack of oxygen in the root zone. Sources of excess soil water that result in high water tables include: high precipitation in humid regions; surplus irrigation water and canal seepage in the irrigated lands; and artesian pressure. Waterlogging in irrigated regions may result in excess soil salinity, i.e., the accumulation of salts in the plant root zone. Artificial drainage is essential on poorly drained agricultural fields to provide optimum air and salt environments in the root zone. Drainage is regarded as an important water management practice, and as a component of efficient crop production systems. World food supply and the productivity of existing agricultural lands can only be maintained and enhanced if drainage improvements are undertaken on cropland currently
affected by excess water and high water tables.
Drainage (both surface and subsurface) is not simply the conversion of wetlands, but the improvement of naturally inadequately drained cropland. It is complementary to irrigation and is viewed as an essential component of irrigated agriculture. The objective is to increase production efficiency, crop yields and profitability on naturally poorly drained agricultural lands.
Agricultural, environmental and socio-economic benefits of drainage
The primary benefits of drainage go beyond the control of excess soil water and accumulation of excess salts in the crop root zone (Fausey et al., 1987). The coincident environmental and socio- economic benefits associated with disease vector control and public health must be fully recognized. One of the major environmental benefits of drainage is its positive impact on improving the health of humans, plants and farm animals. Drainage of wet, swampy areas has led to a reduction in mosquito breeding sites in all parts of the world. The effect has been a drop in the incidence and prevalence of important water related and mosquito transmitted diseases, e.g., malaria, yellow fever and filariasis. Furthermore, drainage of stagnant water can eliminate foot-rot in large animals and, to a certain extent, the breeding environment of aquatic and semi-aquatic snails, which are the intermediate host of schistosomiasis. Drainage also reduces or eliminates mildew infections and various
root rots of plants. The overall impact of improved drainage has been an improvement in hygienic conditions, in the health sector and in the productivity of human beings. By growing high value food crops in well-drained soils, the health, nutrition and economic status of rural populations can be improved. There are also increased opportunities for employment, as new industries may develop in prosperous areas.
Where drainage is used to reclaim salinized and waterlogged lands, it is an environmentally beneficial practice, because the land is returned to its full productive potential. The adaptation of subsurface drainage systems to serve as sub-irrigation or controlled drainage systems leads to other benefits, i.e., the reduction of nitrate pollution.
The field-scale benefits of drainage can be summarized as follows:
i. Drainage promotes beneficial soil bacteria activity and improves soil tilth.
ii. There is less surface runoff and soil erosion on drained land.
iii. Improved field machine trafficability reduces soil structural damage. Soil compaction is reduced and less energy is required for field machine operations. Drainage also allows for more timely field operations. Consequently, the growing season can be lengthened and crops can achieve full maturity.
iv. Crop yields are increased because of improved water management and uptake of plant nutrients.
v. Higher value crops can be planted, and there is flexibility to introduce new and improved cropping systems.
vi. In general, land value and productivity are increased.
vii. Farm income is increased and income variability reduced.
viii. Drainage maintains favourable salt and air environments in the crop root zone.
Types of drainage systems
Surface drainage
Subsurface drainage
Secondary drainage treatments
Agricultural land drainage usually consists of surface or subsurface systems, or a combination of both. At the field scale, subsurface drain pipes and field ditches normally exit to an open main or collector drain. At the regional level, the latter then empties into a river or its tributaries. In some instances, depending on the character of the hydrological basin, main drains may dispose of drainage water to an evaporation pond, to a wetland, or to a saline agriculture/ agriculture-forestry system. A schematic of drainage system components and several options for drainage water disposal within a watershed are shown in Figure 1. Not all of these management options will necessarily be used in a single watershed.
FIGURE 1 Drainage water disposal options within a watershed
Surface drainage
Surface drainage is often achieved by land forming and smoothing to remove isolated depressions, or by constructing parallel ditches. Ditches and furrow bottoms are gently graded and discharge into main drains at the field boundary. Although the ditches or furrows are intended primarily to convey excess surface runoff, there is some seepage through the soil to the ditches, depending on the water table position. This could be regarded as a form of shallow subsurface drainage. Surface drainage is especially important in humid regions on flat lands with limited hydraulic gradients to nearby rivers or other disposal points. There is also a need for good surface drainage in semi-arid regions which are affected by monsoons.
Subsurface drainage
Surface drainage alone is seldom sufficient to remove excess water from the crop root zone. Deep ditches or subsurface pipe drainage systems enable a more rapid water table drawdown. The downstream ends of the laterals are normally connected to a collector drain. The required diameter of the pipe collectors increases with the area drained. Drain spacing is usually dependent on soil hydraulic conductivity and a design drainage rate coefficient. Depending on topography, land formation and proximity of a water receiving body, the collector may outlet by gravity to an open main drain or into a sump. In the latter case, the discharge is then pumped to another drain, or ultimately to a lake or stream.
In some flatter parts of eastern Canada, the eastern and mid-western United States, and parts of Europe, subsurface pipe drains are also used for sub-irrigation. In this case, in dry periods, surface water is introduced into the drain pipe system from an external source, and the water table is raised. Moisture then moves upward by capillary action to the root zone. Sub-irrigation is regarded as a highly energy and water efficient method of irrigation. In another process, known as controlled drainage, elevated water tables can be maintained with a control structure on the collector pipe.
Horizontal subsurface drainage systems are used in irrigated arid and semi-arid regions to reclaim saline and waterlogged lands, and to maintain favourable long-term salt and water balances in the crop root zone. Salinity and waterlogging are caused by a build up of the water table due to deep percolation of normal excess water and canal seepage. Buried pipe drains are generally installed deeper in arid regions than in humid regions in order to control salinity. Water in excess of plant evapotranspiration (ET) needs is always unavoidably applied during irrigation. This additional quantity of water applied is known as the leaching fraction. Naturally occurring as well as applied salts are then leached from the root zone by this water, and removed from the field via the pipe drains. Deeper drain installation ensures that salts do not rise too rapidly to the soil surface due to capillary action. Drainage also prevents waterlogging of the root zone. The amount of irrigation
water to be removed is generally less in arid than in humid regions. Vertical drainage by means of tube-wells is also used to control waterlogging and salinity in some parts of the world, e.g., India, Pakistan and central Asian republics. The primary purposes of tube-wells are the same as those of horizontal drains, and at the same time to extract groundwater for irrigation. As a result of pumping, the water table is lowered, and salinization due to capillarity is minimized. This situation is ideal where the groundwater is not very brackish or saline, and is therefore suitable for irrigation. In areas where the groundwater is highly saline, the pumped water may be too saline for irrigation, unless mixed with fresher or less saline water. Where the groundwater is too saline for crop production, it must be disposed of. Drainage does not have a direct impact on groundwater quality. It only serves to collect and transport excess water.
Secondary drainage treatments
Methods of improving the internal drainage of low permeability soils include: subsoiling, deep tillage, mole drainage, and biological practices, viz., cropping with deep rooted legumes (e.g., alfalfa) and crop rotations. In some parts of the world, deep rooted trees are used to lower the water table. There are usually no water quality hazards associated with these supplemental drainage practices.
Environmental impact assessment
Many institutions require an environmental impact assessment (EIA) prior to construction activities associated with new projects or the rehabilitation of existing projects. The objectives are to identify potential adverse environmental effects, the magnitude of these effects, and to develop mitigative measures. Positive benefits are also identified. In cases where the adverse impacts far outweigh the expected benefits, the project may be completely redesigned or suspended. This is also the case where mitigative measures would be either too costly or technically not feasible. The EIA should be conducted in the earliest stages of decision making, when crucial decisions are still being deliberated.
According to Ochs and Bishay (1992), environmental effects can be classified as:
i. direct and indirect, or first order and higher order. These are chain effects which are felt throughout, and possibly downstream of a catchment.
ii. secondary. The primary activity of a drainage project may be extended to include secondary activities.
iii. synergistic. These effects include an increased threat to the survival of certain species of wildlife that are under pressure in several ways as a result of the same project.
The ICID has developed an 'Environmental Checklist to Identify Environmental Impacts of Irrigation, Drainage and Flood Control Projects' (ICID, 1993). The World Bank has prepared an Environmental Assessment Sourcebook (World Bank, 1991). FAO has produced a paper on the steps in the EIA process and the major environmental impacts of irrigation and drainage projects (FAO, 1995). The purpose of these documents is to enable detailed environmental impact assessments of irrigation and drainage projects to be conducted. The ICID checklist provides a comprehensive list of environmental parameters which must be evaluated in an EIA. Table 1 shows the ICID checklist of possible environmental impacts of irrigation, drainage and flood control projects.
Ochs and Bishay (1992) listed the main steps in the EIA process as:
i. Scoping. This is a public process, involving the participation of all parties. It results in specific guidelines for inclusion in the environmental impact study (EIS). (EIS refers to the document, whereas EIA refers to the entire process).
ii. Drawing up the EIS. Potential adverse and beneficial impacts are identified. Proposed actions with alternatives are indicated and mitigative measures are presented.
iii. Submitting the EIS for public review.
iv. Receiving advice from the reviewing agency.
v. Accepting the EIS.
vi. Choosing project components. The decision-maker chooses the project option to be implemented and the mitigative measures.
vii. Implementing the project.
viii. Monitoring. During and after project implementation, the actual environmental impacts are monitored and compared with the EIS predictions. This is useful for improving future predictions and project designs.
Matrices and checklists are some of the tools most commonly used in an EIA to assess the magnitude or relative weight of both positive and adverse environmental effects. It is hoped that the criteria on drainage water quality provided in the publication will further assist in the evaluation of EIA of irrigation and drainage projects.
Water quality issues
The installation of drainage systems may result in changes to the associated ecosystem. These changes may be either beneficial or adverse. The positive environmental benefits were listed earlier in this chapter. However, there are potential adverse water quality impacts associated with drainage. The concentrations of salts, nutrients and other crop-related chemicals in drainage discharge vary with time and discharge rate. The use of fertilizers and pesticides in intensive agricultural production has sometimes led to damage to downstream aquatic ecosystems (FAO, 1996). Drainage planners therefore need to analyse effluent for nutrients and pesticides. The nutrients of most concern are N and P. In addition, from time to time, natural trace elements from the soil itself may be harmful to the ecosystem. Effluent laden with N and P stimulates eutrophication in receiving water bodies. In addition to agricultural chemicals and trace elements, drainage water from irrigated areas
frequently contains salts. The impact of salts on downstream users needs to be evaluated. Some soils are abundant in trace elements, and these could leach to the drainage system. Small amounts of trace elements such as As, Cd, Hg, Pb, B, Cr and Se are harmful to aquatic species because of biological magnification. The environmental consequences of disposing of drainage water from California's irrigated San Joaquin Valley into the 470 ha Kesterson Reservoir (a closed basin) are well known. The reservoir was a waterfowl habitat, and concentrating Se in the drainage water caused fish species to disappear, and resulted in deformities in waterbird embryos. The failure to construct an adequate outfall drain to the sea, the use of a waterfowl habitat for drainage water disposal, and the lack of proper water quality monitoring were the major reasons for these negative environmental impacts. Care must be taken to ensure that the disposal of drainage water does not interfere with the
ecosystem's aquatic and terrestrial species. Concern has been expressed about damage by agricultural drainage water to estuarine fisheries in some countries.
TABLE 1 ICID checklist of possible environmental impacts of irrigation, drainage and flood control projects
Site:
Date:
Hydrology
1-1 Low flow regime
1-2 Flood regime
1-3 Operation of dams
1-4 Fall of water table
1-5 Rise of water table
Pollution
2-1 Solute dispersion
2-2 Toxic substances
2-3 Organic pollution
2-4 Anaerobic effects
2-5 Gas emissions
Soils
3-1 Soil salinity
3-2 Soil properties
3-3 Saline groundwater
3-4 Saline drainage
3-5 Saline intrusion
Sediments
4-1 Local erosion
4-2 Hinterland effect
4-3 River morphology
4-4 Channel structures
4-5 Sedimentation
4-6 Estuary erosion
Ecology
5-1 Project lands
5-2 Water bodies
5-3 Surrounding area
5-4 Valleys and shores
5-5 Wetlands and plains
5-6 Rare species
5-7 Animal migration
5-8 Natural industry
Socio-economic
6-1 Population change
6-2 Income and amenity
6-3 Human migration
6-4 Resettlement
6-5 Women's role
6-6 Minority groups
6-7 Sites of value
6-8 Regional effects
6-9 User involvement
6-10 Recreation
Health
7-1 Water and sanitation
7-2 Habitation
7-3 Health services
7-4 Nutrition
7-5 Relocation effect
7-6 Disease ecology
7-7 Disease hosts
7-8 Disease control
7-9 Other hazards
Imbalances
8-1 Pests and weeds
8-2 Animal diseases
8-3 Aquatic weeds
8-4 Structural damage
8-5 Animal imbalances
Drainage water management and disposal options
On-farm source control
Re-use of drainage water
Disposal and management of drainage water in closed basins
Water table management
Drainage water can be disposed directly to open surface water bodies, e.g., rivers, lakes, outfall drains, seas or oceans. Wetlands, evaporation ponds and solar evaporators are used as discharge points where there is no direct outlet to one of the open surface water bodies mentioned above. This is generally the situation in hydrologically closed drainage basins. Some re-use of drainage water for irrigation is another management option, where sequential irrigation of increasingly salt-tolerant plants, ending with Salicornia, occurs. Drainage water can also be disposed of by injection to the deep groundwater.
If drainage water is disposed of to large, open surface water systems with significant dilution or assimilative capacity, then water quality problems are minimized. However, water quality problems may develop with repeated re-use, disposal in closed basins and injection to deep wells. In this last case, the concern is that if the drainage water contains sufficient amounts of salts, nitrates, bacteria and trace elements, the groundwater could become contaminated. The problem may be especially severe if the aquifer is also used for drinking water supplies downslope.
On-farm source control
The most efficient method of minimizing environmental problems is to implement source control practices at the farm level. In irrigated areas, this can be achieved by improved irrigation water management. Higher irrigation efficiencies and lined irrigation conveyance structures will reduce the amount of drainage water which needs to be removed. Furthermore, timely and efficient applications of fertilizers and pesticides, used only when necessary, will reduce chemical leaching. Research by Madramootoo et al. (1995) has shown that intercropping corn with a legume or ryegrass reduces the amount of nitrates found in drainage water.
Re-use of drainage water
In many regions where irrigation water is scarce, drainage water is used to meet crop water requirements. Re-use is only sustainable if the drainage water is of sufficiently good quality. Some of the water quality concerns about drainage water re-use with plants of increasing salt tolerance are that: the effluent may be high in salt content (in irrigated lands); the drainage water can be contaminated with trace elements, toxic organic substances, industrial waste and municipal waste in open main drains. Contaminated drainage water could lead to various problems including: impairment of soil physical and chemical properties, water related health problems, and possible contamination of food products.
Disposal and management of drainage water in closed basins
Ultimate disposal of drainage water to a river or sea is not always possible. Closed drainage basins present a unique environmental or water quality challenge. In such situations, evaporation ponds may be an appropriate means for disposing of drainage water. However, these ponds may eventually lead to other environmental problems. For example, toxic substances could accumulate in the ponds. Furthermore, in arid climates, as pure water evaporates from the pond, the concentration of the remaining water approaches that of brine. The health of waterfowl, fish and other aquatic biota which use the pond could be negatively affected. Other environmental problems associated with evaporation ponds include: use of the pond to collect wastewater from homes; human health problems caused by consuming water from the ponds; and the need to ultimately dispose of the accumulated concentrated chemicals in the ponds. In some cases, dry toxic materials may be spread by the wind. Furthermore,
these ponds could become habitats for snails and mosquitoes, thereby causing malaria and schistosomiasis epidemics. In addition, if not properly managed, new waterlogged and saline areas will develop adjacent to the ponds.
There is now increasing interest in the utilization of natural and constructed wetlands to manage drainage water. Wetlands are particularly effective for removing sediment, N and P. Plant, soil and hydrologic parameters interact in a complex way to filter and trap pollutants, and to recycle nutrients. Certain tree and plant species have the potential to absorb pollutants. Residence time, flow rate, hydraulic roughness and wetland size and shape are some of the factors which influence treatment efficiency. The water supply to the wetland must be sufficient to provide an excess to discharge and prevent salt accumulation.
In areas where soils, geologic and hydrologic conditions do not permit constructed wetlands, the saline agriculture/agriculture-forestry system may be appropriate for the disposal of drainage water. The operating principle is to successively re-use saline drainage water to irrigate crops and trees of increasing salt tolerance, and to discharge the final much reduced volume of water into a solar evaporator for salt crystallization. The depth of water ponded in the evaporator is regulated to match the daily evaporation rate. The goal is to make the crystallization pond unattractive to flora and fauna. Another challenge of evaporation ponds is that evaporation is reduced as the ponds become more concentrated. This necessitates the use of additional land to maintain disposal (evaporation) capacity.
Water table management
While there are instances where ecosystems have been damaged by poor quality drainage water, it is also possible to use subsurface drainage systems beneficially to improve water quality. By managing the water table, through controlled drainage or sub-irrigation, nitrate concentrations in drainage effluent can be significantly reduced. Downstream flood flows can also be reduced. Water table management could therefore be viewed as a best management practice. Skaggs and Gilliam (1981) and Madramootoo et al. (1993) have shown that sub-irrigation and controlled drainage not only increase crop yields, but also enhance denitrification. Water table management is best suited to flat lands, and can be achieved by a simple modification to the outlet of subsurface drainage systems.
Planning and designing drainage systems to protect and enhance water quality
It is important to recognize that while technologies may be available to minimize water quality impacts from drainage effluent, institutional mechanisms must also be put in place to ensure that the technologies can be implemented. Policies and programmes, including legal and monitoring aspects, require an institutional framework. These are discussed in detail in Chapter 8. Chapter 2 covers the physical, chemical and biological constituents of drainage water which are of environmental concern. Drainage water management options are also included in Chapter 2. Drainage water re-use for various purposes is discussed in Chapter 4. Systems for treating drainage water are presented in Chapter 5, and possibilities for the disposal of drainage water are discussed in Chapter 6. Water table management systems are covered in detail in Chapter 3. Health issues related to drainage water management are discussed in Chapter 7.
It is recognized that, in some instances, there may be insufficient data in the project design and planning stages to enable precise conclusions to be drawn about possible environmental impacts. For this reason, most chapters contain sections on environmental monitoring, so that project planners and designers can elicit the information required for proper decision making. For more detailed information on water quality monitoring, the reader is referred to the United States Environmental Protection Agency (USEPA, 1982).
<section>6</section>
Chapter 2 - Drainage water quality
Dennis W. Westcot
California Regional Water Quality Control Board
Sacramento, California, USA
Water quality characteristics
Protection of beneficial uses
The management goal of agricultural drainage is to maintain a salt balance in the crop root zone in arid areas and proper soil water balance in humid areas (ASCE, 1990; Smedema, 1990). Drainage water from different locations and/or facilities will have different quality characteristics. Poor quality water should be separated from good quality water. If drainage water is unsuitable for re-use, it should be disposed of in a sink of lower quality water. Highly saline subsurface drainage water from arid areas, re-used for irrigation, could affect the growth of salt sensitive crops. In humid areas, most subsurface drainage water has the potential to be reused. There are several factors to consider when determining the constraints for the management of surface or subsurface agricultural drainage water. The amount and quality of drainage water managed, changes in the rate of flow, and chemical concentrations need to be determined. There are valid references available to provide
information on how to conduct field investigations to estimate the volume of drainage water that must be conveyed from the drained area (Luthin, 1957; ILRI, 1994).
Water quality characteristics
Pesticides
Toxic trace elements
Nutrients
Sediment
Bacteria
Temperature
Salinity and major ions
Sulphurous compounds
Water quality is relative and is defined as the characteristic of a water that influences its suitability for a specific use. Quality is defined in terms of physical, chemical and biological characteristics. Drainage water is no different from any other water supply and is always usable for some purpose within certain quality ranges. Beyond these limits, drainage water must be disposed of in a manner that safeguards the usability or quality of the receiving water for present established and potential uses.
Surface and subsurface drainage water from irrigated agriculture is normally degraded compared with the quality of the original water supply. Drainage water that flows over or through the soil will pick up a variety of dissolved and suspended substances including salts, organic compounds and soil particles. Management for safe re-use and disposal requires an understanding of the characteristics of the drainage water, and a matching of those characteristics to the environmental protection needs of the re-use or disposal area. The discussion in this section focuses on the characteristics of drainage water that make it a potential environmental contaminant.
Both surface and subsurface drainage effluent contains substances that are potential pollutants. These pollutants may be:
FIGURE 2 Hierarchical complexity of drainage water management as related to water quality problems (Rickert, 1 993)
i. purposely introduced into irrigation water;
ii. mobilized by the practice of irrigation and/or drainage; or
iii. concentrated as a result of ET.
Each of these three processes requires a different management approach to mitigate its impact. For example, in the first instance, the pollutant source could be removed or limited. By contrast, the concentration of dissolved solids by ET is a natural process that results from consumptive water use and is unavoidable. One or more of these processes may be occurring, thus increasing the complexity of assessing the problem and developing solutions to a given water quality problem. Figure 2 illustrates the differences in the complexity of assessing water quality problems that may be associated with drainage water management.
Pesticides
Numerous types of pesticides may occur in drainage water. This makes it difficult to assess their potential impacts on water quality. Most pesticides are synthetic organic compounds and there are documented instances where organic pesticides in irrigation runoff have caused downstream water quality problems. Recent studies in both the San Joaquin and Imperial valleys of California have shown that surface runoff carries pesticides that cause toxicity problems in aquatic organisms in surface streams (Foe and Connor, 1991; Connor et al., 1993; Di Giorgio et al., 1995). These pesticide problems are the result of farming practices, not the design or functioning of the drainage system. Drainage discharges from irrigation may present elevated concentrations of pesticides, but only for a relatively short period of time. The best solution is improved irrigation water and pesticide management. The drainage system should not be considered a mechanism for controlling pesticide runoff.
Those designing and building a surface water drainage re-use or disposal system need to consider whether a pesticide runoff problem is likely to occur, the type of pesticides expected, and the mitigation steps needed to avoid downstream pollution.
High pesticide concentrations in subsurface drainage water are less probable because of the filtering action of the soil. Few recent surveys of subsurface drainage water have been conducted, but groundwater surveys in California show few pesticide detections (California Department of Pesticide Regulation, 1994). Because of infrequent detections, pesticides in subsurface drainage water should not be considered a priority for intensive evaluation unless general water quality surveys show detected levels of concern. However, data summarized by MacKenzie and Viets (1974) show that pesticides have been found in subsurface drainage systems. In the event that pesticides are found, the source is probably a farming practice which can be altered to prevent the continued deep leaching of the pesticide. The presence of a pesticide in subsurface drainage water indicates that the pesticide is in the shallow groundwater and will continue to be discharged with the subsurface drainage for a
long period.
Toxic trace elements
Inorganic trace elements are different from synthetic organic compounds (pesticides) in that they are commonly present at low levels in nature and there is already a natural level of tolerance. There is, however, a fine division between natural tolerance and toxicity. It is therefore essential to have good information on the concentration of trace elements in the drainage water in order to develop safe re-use and disposal methods.
High concentrations of inorganic trace elements in irrigated soils and groundwater pose a threat to the environment if they are mobilized by irrigation and drainage practices. They can be collected in the drainage water and then discharged at relatively high concentrations into the environment or at low concentrations and bioaccumulated in the food chain. These trace elements pose a threat to agriculture, wildlife, drinking water and human health.
The trace elements of most importance are those designated as priority pollutants by the USEPA and documented as pollutants associated with irrigated agriculture. Deverel and Fujii (1990) divided these priority pollutants into four categories (Table 2) and reviewed the factors influencing their occurrence and mobility in irrigated soils and groundwater. Recent surveys of subsurface drainage water in the San Joaquin, Sacramento and Imperial valleys of California, have shown that trace element concentrations can be elevated and appear to be strongly associated with the geologic setting of the irrigated area (Deverel et al., 1984; Westcot et al., 1989 and 1993). Deverel and Fujii (1990) also concluded that high concentrations of trace elements in soils and groundwater in arid irrigated areas can occur concomitantly with high soil and groundwater salinity and be affected by the same processes that affect the soil and groundwater salinity. They concluded that high trace element
concentrations may also occur independently of salinity such as in humid area soils. Therefore, trace elements should be a priority for evaluation in all drainage projects.
TABLE 2 Geochemical behaviour of trace elements (Deverel and Fujii, 1990)
Alkali and alkali earth metals
Transition metals
Non-metals
Heavy metals
Barium
Chromium
Arsenic
Cadmium
Lithium
Molybdenum
Boron
Copper
Vanadium
Selenium
Lead
Mercury
Nickel
Zinc
There are no data to show that trace element levels are higher in the surface drainage flow than in the applied water before it moves across a field. Increases in trace element concentrations in surface runoff are generally not expected. Therefore, they are not considered a priority for evaluation.
It is important to be able to predict the potential occurrence of high trace element concentrations in subsurface drainage water. Bowen (1979) and Shacklette and Boerngen (1984) developed range and geometric mean values in soils for the priority pollutant trace elements (Table 3). Comparison of actual soil data with these background levels would give an initial screening as to the potential for trace element leaching. This evaluation should be conducted in conjunction with shallow groundwater sampling for the same potential elements of concern.
TABLE 3 Typical normal acceptable non-harmful trace element concentrations in soils of the western United States (Bowen, 1979; Shacklette and Boerngen, 1984)
Element
mg/kg
Baseline (geometric mean)
Baseline (range)
Arsenic
Barium
Boron
Cadmium
ND
ND
Chromium
Copper
Lead
Mercury
Molybdenum
Nickel
Selenium
Strontium
Uranium
Vanadium
Zinc
ND = no data
Nutrients
The two major nutrients in drainage water are N and P. Both contribute to eutrophication of surface waters.
Nitrogen can be in either the organic form (ammonium) or the inorganic form (nitrate). The predominant form in surface drainage is organic N. This comes from the organic matter being washed off the field and should be the focus of any water quality evaluation for surface drainage. This is a persistent problem in higher rainfall areas but is not usually an issue in arid areas. Ammonia is adsorbed on clay particles due to their positive charge. It can also volatilize. Recent sampling of surface runoff from irrigated areas in the San Joaquin Valley of California showed almost no nitrate in the surface runoff. This was attributed to little nitrate N (NO[3]-N) fertilizer being applied to the soil surface.
Nitrate is the dominant form of N in subsurface drainage water. Nitrate N should be the focus of any evaluation of water quality from a subsurface drainage system. High nitrate concentrations in subsurface drainage can originate from a number of sources: geologic deposits, natural organic matter decomposition and deep percolation of nitrate resulting from fertilizer applications. Nitrate contamination of subsurface drainage water has been documented by Madramootoo et al. (1992). Mineral N, in the form of nitrate or nitrite, is transported in a dissolved form. The proportion of these various forms in drainage water depends on the predominant form of drainage. Nitrite is the most dangerous form of N. It is in general a transient form of N, present in small quantities.
Agricultural drainage water also contains phosphate in both organic and inorganic forms. Most of the phosphate in surface drainage is in the organic form. Little phosphate has been found in subsurface drainage water because of its strong adsorption in arid zone soils (Johnston et al., 1965; MacKenzie and Viets, 1974) and in humid regions (Madramootoo et al., 1992).
Sediment
While rainfall induced erosion is a significant source of sediment, irrigated agriculture may also cause erosion directly through application of irrigation water, or indirectly through sub-optimal land management. Sediment contained in surface runoff from agricultural lands may carry P and certain pesticides to surface waters where they may contaminate the food chain or affect other beneficial uses of water. Excess sedimentation can also degrade the stream environment, diminish the health and diversity of fish and wildlife habitats, limit recreational uses and add to the costs of flood management and drinking water treatment.
Because subsurface drainage water is primarily groundwater, it is not expected to carry significant amounts of sediment. No data are readily available on sediment loads in subsurface drainage water, but sediment from subsurface drains is not normally expected to be a significant issue. However, from time to time a subsurface drain line will be poorly constructed, or a drain line will fail after construction and a significant amount of sediment will collect in the drains. This will then either plug the drains so they will not function or be discharged at the drain outlet. Moreover, in certain soils, drains with inadequate or improperly installed envelopes can accumulate fine sands and silts and these sediments will eventually either plug the drains or be discharged with the drainage water. In any event, the normally relatively sediment free water from subsurface drains might erode the banks of unlined surface drains and increase the sediment load of the drainage water.
Erosion is a natural geologic process. However, although sediment cannot be eliminated, its movement can be controlled to reasonable levels.
Bacteria
Bacteria are a potential pollutant where surface return flows come from land that has received applications of human or animal waste. Bacterial pollution may also originate from wetland discharges. The focus of a bacteriological contamination assessment is on the measurement of coliform and faecal coliform levels in the water. A normal irrigated farming operation would not be expected to produce adverse bacteriological levels in surface drainage water. The presence of coliform or faecal coliform would be an indication that other types of wastewater, such as municipal, industrial or animal waste, may have entered the surface drainage system.
Soil is a biological filter. Therefore, it is not expected that micro-organisms will move through the soil from surface water to a subsurface drainage system. There is no record of coliform or faecal coliform levels being measured in a subsurface drainage system, nor are they expected to be a problem.
Temperature
Elevated temperatures occur where irrigated fields or wetlands are warmed by the sun, and tailwater from these areas is then discharged into a stream, causing a rise in the stream temperature. This problem is often aggravated where diversions for irrigation and wetland management also reduce the total stream flow. Elevated temperatures have a direct impact on stream aquatic life especially in certain cold water streams or those with anadromous fisheries. Temperature surveys should be an essential component of any surface water monitoring if elevated water temperatures are expected. Power plants sometimes affect the temperature of downstream waters.
In contrast to surface drainage, subsurface drainage is mainly groundwater and has a relatively constant water temperature. The temperature of subsurface drainage waters can be easily predicted as they approach average seasonal or annual temperatures. Measurements in the San Joaquin Valley of California, show that subsurface drainage discharges average 17°C, with a variability of less than 2°C (Chilcott et al., 1988). These cool discharge temperatures would not be expected to have an impact on warm water fisheries as they quickly come into equilibrium with the ambient air temperature.
Salinity and major ions
The practice of irrigation results in consumptive uses of water, leaving behind salts concentrated in a smaller volume of water. During irrigation, two types of drainage may occur: surface drainage and subsurface drainage. Reeve et al. (1955) showed that the salt content of water changes little in flowing over soil even where there is a visible salt crust. Studies of tailwater throughout the San Joaquin Valley of California have also shown that the chemical composition of the surface drainage is essentially that of the supply water.
Salts are a major water quality factor in choosing disposal options for subsurface drainage in arid irrigated areas. Salinity can restrict the urban or agricultural re-use of drainage water, as it is the most significant long-term water quality concern for managing irrigated agriculture in arid zones. Salinity has not been noted as a serious concern with subsurface drainage waters from humid areas. This is generally due to the higher rainfall, higher dilution capacity in surface waters and lower initial salt content in the soil.
In arid irrigated areas, irrigation practices mobilize naturally occurring salt in the soil and concentrate those salts already present in the supply water. The salts captured by a subsurface drainage system are often highly concentrated with the major cations being Na^+, Ca^2+, Mg^2+ and, to a lesser extent, K^+. The major anions are Cl^-, SO[4]^2-, HCO[3]^-, NO[3]^- and CO[3]^2-. Most subsurface drainage waters are NaSO[4] and NaCl dominated. These are common non-toxic elements that only become problematic when concentrated in the soil. However, some MgSO[4] waters have been observed. The predominant cations and anions influence the re-use and disposal methods chosen.
Subsurface drainage water from arid areas always has a higher salinity than the supply water, a higher proportion of Na and Cl, an increased hardness and a higher sodium adsorption ratio (SAR). The higher salinity and higher levels of specific ions often reflect the characteristics of the soil through which the irrigation water has percolated. This in turn is influenced by the shallow groundwater quality, by the ionic composition of the irrigation water, and by the irrigation efficiency. Salt species are also influenced by a number of interdependent, multi-phase chemical interactions. There are a number of models available to predict solute migration in soils (Jurinak and Suarez, 1990). A full salinity appraisal is an essential component of any subsurface drainage water re-use or disposal scheme.
Sulphurous compounds
Water quality problems are often encountered when sulphaquepts or acid sulphate soils are drained. Such soils are mostly found in tropical river deltas. Improved drainage of sulphaquepts results in an increased discharge of acid water. High quantities of sulphuric acid may be released into receiving streams. There is also a reduced pH which can affect aquatic life. Furthermore, iron and aluminium in these soils can be mobilized and may cause human health problems if the receiving water bodies are used for domestic drinking water.
Protection of beneficial uses
Domestic and drinking water
Industrial supply
Agricultural supply
Aquatic life
Recreation
The re-use and disposal of agricultural drainage water must be environmentally sustainable. This means allowing other uses to be made of the water resources that will not be adversely affected by the drainage project. Understanding the water quality needs of the various downstream uses can assist in developing mitigation methods. The primary needs to be considered are drinking water, industrial supply water, agricultural supply water, recreational uses and aquatic life.
Domestic and drinking water
Surface and subsurface drainage water discharges can degrade the potability of drinking water and increase the cost of testing, treatment and delivery. An additional concern is human contact with the water during bathing, washing clothes and recreational use. Domestic use and drinking water safety is a primary consideration for most nations and for the general public. Drinking water supplies can be affected by sediment, nutrients, micro-organisms, total salts, sulphate, chloride and pesticide levels. Sediment increases water treatment costs. High nutrient levels, especially nitrate above 45 mg/litre as nitrate, are a potential public health problem. High nutrient levels may also cause algae problems which can lead to a disagreeable taste and the formation of THM precursors. Bacteria levels indicate a high potential for disease problems and may increase water treatment costs. High total dissolved solids (TDS) can cause taste problems. Recommended levels of salinity are less
than 0.9 dS/m with a maximum suggested at 2.0 dS/m. Sulphate and chloride levels are recommended between 250 and 500 mg/litre (Marshack, 1993). The higher salinity of subsurface drainage water may also increase treatment costs due to the increased water hardness.
Pesticide levels in drainage water should be checked if a discharge is planned into a known drinking water supply. Trace elements may also cause toxicity. Recommended trace element levels in drinking water are shown in Table 4.
TABLE 4 Summary of measured concentrations of trace elements in shallow groundwater from the San Joaquin Valley, California (Deverel et at,, 1984; Chilcott et al., 1988 and 1990), and established US water quality criteria for drinking water, aquatic life protection (Marshack, 1 993) and irrigation (Ayers and Westcot, 1985)
Constituent
Measured
Criteria
m g/litre
Median
Maximum
Drinking water standard
Irrigation guideline limits
Aquatic life protection limits
Arsenic
Boron
Cadmium
Chromium
Copper
Iron
Lead
Lithium
Manganese
Mercury
Molybdenum
Nickel
Selenium
Uranium
Vanadium
Zinc
The median value for TDS in Deverel et al. (1 984) was 2 350 mg/litre. Values shown in italics for the median and maximum values are from Chilcott et al. (1988 and 1990); the other values are from Deverel et al. (1984).
Industrial supply
Water supplies for industrial processing are affected primarily by high salinity levels. The major concern is increased hardness as this could cause scaling or precipitation and increase costs for treatment prior to use. The discharge of drainage water into a known industrial supply should focus on the salinity and hardness limits of the water.
Secondary effects of drainage waters on industrial usage might be related to nutrients, sediment, micro-organisms and pesticides. These may restrict the type of use the water is put to, or may increase treatment costs prior to use.
Agricultural supply
Agricultural water users are primarily affected by salinity or the individual salts in the water supply. Total salinity can cause crop yield reductions and increased costs for salt management. Specific salts such as boron, chloride and sodium also cause plant toxicity problems. In addition, sodium can cause soil permeability problems. As subsurface drainage water itself is commonly higher in total salts, sodium, chloride and boron, agricultural water users can be affected by its discharge into usable water supplies. There are several published guidelines on agricultural use of higher salinity water (Ayers and Westcot, 1985; ASCE, 1990; and Rhoades et al., 1992).
Trace elements are of concern for irrigation or animal drinking water use. Suggested guideline values for irrigation use are given in Table 4. Animal drinking water guidelines are given in Ayers and Westcot (1985) along with a discussion of the guideline values for irrigation. In most instances, trace element levels acceptable to irrigated agriculture may be lower than those required for other uses, such as aquatic life, so agricultural use may not be the most limiting. The exceptions are molybdenum and selenium which can accumulate in crops and cause toxicity to the consumer of the crop (Page et al., 1990).
Aquatic life
Public values for the protection of aquatic and wetland wildlife are concerned with the survival, health and diversity of fish, wildlife and aquatic plants. If discharged incorrectly, pollutants in subsurface drainage may be acutely toxic or bioaccumulate. They may also damage plant and animal health in the long term, or impair the reproductive ability of aquatic species.
The pollutants of primary concern in surface drainage water are pesticides, sediment and temperature. Elevated temperatures can cause damage to cold water fisheries. Sediment levels can cause instream damage to habitat or restrict light penetration and aquatic food development. With surface drainage, the pollutant of greatest concern is pesticides. However, there are few criteria available on allowable pesticide levels in water. It is therefore suggested that pesticide surveys and analysis concentrate on aquatic toxicity. This can be defined by the USEPA's three species test (USEPA, 1994).
The pollutants of primary concern in subsurface drainage are trace elements. Table 4 lists recommended water quality limits for trace elements for aquatic life protection. In most instances, trace element levels in natural waters are very low (Westcot et al., 1990b). Recent experience shows that trace element levels in subsurface drainage may be elevated (Tables 3 and 4) and often exceed those found in seawater or inland salt lakes (Westcot et al., 1990a).
In the San Joaquin Valley of California, impacts on aquatic life due to drainage water discharges have occurred. In this area, there are six potentially toxic trace elements known to exist in concentrations sufficient to cause impacts on aquatic life: As, B, Hg, Mo, Se and U. It is recommended that all drainage areas be surveyed for these six trace elements due to their potential harmful impacts on aquatic life. Mercury and Se are of particular concern for aquatic life because of their bioaccumulative nature, even at very low concentrations in the water. Boron is known to be harmful to agricultural crops and similar impacts on aquatic plants would be expected. Arsenic is a known human carcinogen at very low levels and could have a similar impact on aquatic life and wildlife, though no data are available (Deverel and Fujii, 1990). Plants tolerate high levels of Se and Mo, but both elements are known to concentrate in forage crops and can have an impact on the livestock or
wildlife that consume the crop. Little or no data are available for U. Trace elements such as Cu, Fe, Mn and Zn have little solubility in saline and alkaline soils and would not be expected to be present under such conditions (Page et al., 1990).
Recreation
Recreation in water is becoming an important part of many economies. The discharge of subsurface drainage water could affect the aesthetic or recreational value of a water body and impair that component of the economy. The impairment, in addition to the effects on aquatic life as discussed above, would be to restrict human contact with the drainage water due to high pesticide, sediment or bacteria levels. These impacts would generally come from surface drainage systems. A secondary concern with recreation and associated with subsurface drainage water is the discharge of nutrients that may cause eutrophication and stimulate aquatic plant growth. This leads to a loss of aesthetic values in recreational waters.
<section>7</section>
Chapter 3 - Water table management
Daniel Zimmer, CEMAGREF, Antony, France
Chandra A. Madramootoo, McGill University, Quebec, Canada
General features
Water quality benefits
Operational aspects
Monitoring requirements
There are some agronomic and water management practices which may be implemented at the field and watershed scales to reduce the concentration of chemicals (primarily nutrients and pesticides) in agricultural drainage water. Agronomic practices include tillage, cropping methods, and improved and timely fertilizer and pesticide applications.
One water management practice which has been shown to significantly reduce NO[3]-N in drainage water is water table control. The technique has been tested in the humid regions of eastern Canada, and the eastern and mid-western United States. It is currently promoted in these regions as a combined drainage/irrigation practice for increasing crop yields and reducing NO[3]-N pollution from relatively flat, artificially drained cropland. The quality of receiving waters is thus improved. The water quality benefits of water table management are well described in Gilliam (1987) and Gilliam et al., (1979). Water table management systems are currently being tested in some parts of Europe and Asia. The practice is yet to be evaluated in the arid and semi-arid regions, although there is potential for water table management in these regions. However, proper management of the water table is required to ensure that re-salinization of the crop root zone does not occur. Early attempts at
sub-irrigation in arid and semi-arid regions failed because of soil salinization. More information is required on the effects of different water table levels on crop yields and salt movement in the root zone in various climatic locations and soil types. For these reasons, the focus of this chapter is on water table management in humid regions.
In the humid regions of North America, water table management has evolved on lands which were first improved with surface and subsurface drainage. Drainage improvements were initially required on these poorly drained soils to increase crop productivity and provide better soil conditions for field machine trafficability. With concerns about the quality and impacts of agricultural drainage water on marine ecosystems, as well as nitrate contamination of drinking water, the drainage systems were modified to reduce the concentration of pollutants in the drainage water. Water table management has been practised for several years on organic and sandy soils in humid regions to reduce drought stress of high value crops.
General features
Water table control structures
There are two forms of water table management: controlled drainage and sub-irrigation. Controlled drainage restricts the discharge from a subsurface drain outlet or open main drain, resulting in a higher field water table. The water table drops naturally over time due to evaporation and deep seepage, and is only raised if there is precipitation. With sub-irrigation, water is pumped slowly and nearly continually into open ditches or a subsurface drainage system to maintain a near constant water table. When large rainfalls occur and the field water table rises above the desired level, the irrigation pump is stopped. The excess water then drains from a control structure in the ditch or drain outlet. Such systems are shown in Figure 3.
FIGURE 3 Water table management systems
As a result of higher water tables, water is provided for plant uptake through capillary rise or upward flux. This water helps to meet plant transpiration requirements and reduce drought stress. Corn and soybean yield increases of 10-15% have resulted from sub-irrigation in eastern Canada (Madramootoo et al., 1993; Madramootoo et al., 1995). An added benefit of the higher water table is that it enhances denitrification, thereby reducing nitrate leaching. Denitrification is a microbial process whereby nitrate, rather than oxygen, is used to reduce NO[3]^- to N and NO[x]. Nitrates retained in the soil profile are available for plant use. Several researchers, including Willardson et al. (1972), Bengtson et al. (1984), Gilliam and Skaggs (1986), Evans et al. (1992) and Lalonde et al. (1995), have shown that water table management can reduce NO[3]-N in drainage water by over 60%. Denitrification also occurs in water stored in ditches. Other benefits of water table management
include: reduced downstream peak flows and drainage volumes, and conservation of soil water to supplement irrigation water requirements. There are, however, some instances where water table management may increase peak flows (Konyha et al., 1992).
Water table control structures
In areas with open canal drainage systems (with or without subsurface drainage), flashboard riser control structures may be used. These are installed in the ditches, and by either inserting or removing a flashboard, the water level in the ditch is either raised or lowered. These structures can also be used to store runoff in canals, which can later be used for sub-irrigation. Surface runoff from drained fields can also be controlled with the aid of the flashboard risers.
FIGURE 4 Side view of an adjustable water table control outlet (Madramootoo, 1 994)
Discharge from a subsurface drainage system can be regulated with a PVC control structure similar to that shown in Figure 4. This adjustable water table control structure, attached to the collector outlet, has a vertical riser through which water can be pumped for sub-irrigation.
With pumped drainage outlets, the pumping system can be designed and managed to maintain a desired water level in the sumps, and thus in the field. It is possible to automate the system using a water level sensor installed in a dipwell at the midpoint between subsurface drain laterals (Fouss et al., 1990). The sensor can switch the pump on or off.
Water quality benefits
Drainage hydrology and water quality
Nutrients
Pesticides
Drainage hydrology and water quality
Water table management systems may be designed to control drainage volumes, peak flow rates or chemical losses from agricultural fields or catchments. Once the design objective has been determined, the potential hydrologic impacts and the resulting changes in water quality must be determined. Hydrologic impacts will depend on the degree and method of drainage improvement already attained in the proposed project area.
High water tables will result in low infiltration capacities of the soils and high surface runoff rates, and will increase the loss of chemicals located at or near the soil surface. On the other hand, unrestricted drain discharge (i.e., no outlet control) will produce low water tables and higher infiltration capacities. Consequently, during periods of high drain discharge, there is increased potential for the leaching of soluble chemicals existing in the soil profile.
The reduction of drainage volumes may be a first step towards improved drainage water quality. Evans et al. (1995) have indicated that water table management may reduce total drainage outflow by over 30% compared to conventional drainage systems without outlet control.
Outflows, however, vary widely, depending on soil type, rainfall and type of drainage system. During very dry years, water table management may totally eliminate outflow. In wet years, it may have little or no effect on total outflow. During the growing season, water table management typically reduces outflow by less than 15 %; much of the reduction occurs during the non-growing period, i.e., winter and early spring. The decrease in outflows is not due to a significant increase in ET. A slight increase may be observed during the growing season, but this would not explain differences during the winter, when most of the reduction occurs. It is possible that most of the water is lost by lateral seepage to remote sinks, or by deep seepage to groundwater. Depending on the topography of the watershed, this water may reappear downstream and it should be quantified.
Generally, with improved drainage, there is a tendency for the planting of higher value crops, and increased chemical applications. This may lead to increased chemical loads in surface runoff and subsurface drainage discharge. Crop production methods should, therefore, focus on more efficient water, fertilizer and pesticide applications. Water quality benefits associated with water table management will only be achieved if sound agricultural practices are followed.
Nutrients
Nutrient losses in agricultural drainage water raise many environmental concerns. Eutrophication, due to high N and P concentrations, increases algal growth and reduces oxygen in surface waters. From agronomic and economic standpoints, nutrient losses represent a decrease in the efficiency of the crop production system. There are also human and animal health concerns, because nutrients, especially NO[3]-N, can contaminate drinking water supplies. Concentrations exceeding 10 mg/litre can be harmful to humans.
Nitrogen, P and K are the three major elements of fertilizer and manure. Nitrogen is easily converted to NO[3]-N. Table 5 shows the order of magnitude of total annual losses and peak concentrations of these three nutrients which may be expected in surface runoff and subsurface drainage. It can be seen that P and K are of less concern than NO[3]-N.
Nutrient concentrations in drainage water are influenced by many factors, including: rate of fertilization; soil and crop type; and soil water regime during the growing season. These factors affect the efficiency of nutrient uptake by the crop roots, and residual nutrient levels in the soil during the fallow or dormant season.
Nitrogen
As discussed in Chapter 2, nitrogen is transported in different forms.
In most cases, water table management aims at reducing NO[3]-N leaching. When developing a water table management strategy, losses of N in its different forms should be taken into account. A decrease in one form of N may result in an increase in another form. This is particularly the case if a decrease in subsurface drainage discharge results in an increase of surface runoff. In such a case, there might be an important decrease in NO[3]-N concentration in drainage water, while a small increase in loss of organic N and NH[4] may occur.
TABLE 5 Orders of magnitude of peak concentrations and annual losses of NO[3]-N, P and K in drainage water at field level
Losses
NO[3]-N
P
K
Surface runoff
Peak concentration (mg/litre)
Annual (kg/ha)
Subsurface drainage
Peak concentration (mg/litre)
Annual (kg/ha)
Knowledge of the N cycle, and the impact of agronomic practices on this cycle, will help with the implementation of a water table management strategy. Temperature and the soil water regime influence nitrification and denitrification. At the field level, the potential risks must be assessed by comparing the water table regime to: (i) fertilizer rates and practices during the growing season; (ii) the periods of high mineralization rate; and (iii) the periods of possible denitrification. These factors are illustrated in Table 6.
High water tables due to water table management may significantly decrease NO[3]-N concentrations in drainage water. Annual losses of total N from fields with water table management could be 40-50% lower than for conventionally drained fields (Evans et al., 1991). This decrease is due to both a reduction in drainage discharge and an increase in denitrification. If controlled drainage is used during the early spring or late winter, when drainage is not needed for crop production, one can, if temperatures are warm enough, dramatically increase denitrification and reduce NO[3]-N losses.
TABLE 6 Factors influencing the nitrogen cycle in agricultural soils
Factor
Effect on crop
Effect on nitrogen cycle
Temperature
Higher ET which enhances vegetative growth and thus nitrogen uptake.
Higher temperatures (in the range of 20-30°C) increase the rate of most biochemical processes, especially mineralization and denitrification.
Soil water regime
High water tables restrict crop growth and nitrogen uptake by roots.
Soil water contents below field capacity increase the rates of all processes, except denitrification. Saturated conditions due to higher water tables enhance denitrification.
Fertilizer and agricultural practices
Application of fertilizers increases the rate of crop growth and development.
Drainage improvements are generally linked to changes in cropping systems, which may lead to increased N fertilizer inputs. The conversion of grassland to cropland results in mineralization of N accumulated in the soil profile.
The design of a water table management strategy to control N losses should take the following into account:
i. The timing and amounts of drainage discharge, before, during and after the growing season.
ii. The potential denitrification. Denitrification requires low oxygen concentration, due to shallower water tables, as well as the presence and activity of bacteria. Temperature and soluble organic carbon content in the soil influence denitrification. Soil temperatures above 10°C are necessary to induce significant denitrification.
iii. N[2]O is a by-product of denitrification. This is a greenhouse gas, which is of environmental concern, and may only be a problem on very large watersheds.
iv. The scale at which the water table management is implemented. At the watershed scale, there is the possibility of less vegetation in and along channels. This may yield higher NO[3]-N levels at the field edge.
v. Continually high water levels in drainage canals and/or ditches can cause problems of ditch bank stability and also promote problems with beavers and other rodents.
Phosphorus
Phosphorus losses are closely tied to sediment loss. Surface runoff is, therefore, the primary factor influencing both sediment and P transport. Shallow water tables during high rainfalls could increase surface runoff, and hence P movement. However, at the watershed scale, controlled drainage has been shown to reduce P losses (Evans et al., 1991).
Pesticides
Pesticides are generally transported with soil particles in surface runoff. According to Munster et al. (1995), pesticide concentrations are generally higher in surface runoff than in subsurface drainage water. However, Kalita et al. (1992) stated that high pesticide concentrations in subsurface drainage water could be expected if preferential flow occurs.
The amount of pesticide in subsurface drainage water is typically less than 0.1% of the amount applied. The pesticide most studied is atrazine, or compounds of the same family. This is a herbicide for maize crops. Results for aldicarb, alachlor and metolachlor are also available. The concentration of these chemicals is generally very low, in the range of 1-3 m g/litre.
Although further investigation is needed, a water table management strategy that aims to reduce pesticide losses should preferably reduce the amount of surface runoff. The water table level should therefore be lowered if a rainfall occurs immediately after pesticide application. However, Michaelsen (1995) showed preferential transport of several herbicides being enhanced in drained plots in northern Germany. Most pesticides have a relatively short half-life (measured in days), and therefore do not persist in the soil or water for long periods.
Operational aspects
Farm or catchment scale
Topography and soils
Water table management systems can be operated by farmers, water boards or water users associations. However, there is also a need for an institution to monitor water quality. Sometimes a compromise may be required between agricultural and water quality benefits.
Depending on the type and severity of water quality constraints, water table management will seek an optimum for either agricultural production or environmental protection.
System constraints may be either physical, technical or human. The most critical physical aspects include topography, the existing drainage network, water availability (for sub-irrigation) and soil conditions. Detailed information on the design, installation and operation of water table management systems can be found in the ASAE Standards (1994).
Water table management requires greater farmer involvement than does conventional drainage. The operation of subsurface drainage systems requires few management decisions. However, with water table management, operational indicators are difficult to see, as the response to outlet adjustments is not immediately evident in the field. Furthermore, the operational incentive may not be solely increased crop yield, but also improved water quality and increased water quantities for downstream water users. These factors must be clearly defined for system operators at the start of the project.
Farm or catchment scale
There are two approaches to the implementation of water table management: the catchment level (or large tracts of land) and the field scale. These two approaches are complementary. The scale of implementation depends on: water quantity and quality objectives; farming systems; regional hydrology; institutional arrangements; and topography.
Water quality is linked to farming practices and is therefore to be controlled primarily at the field level. However, field systems may necessitate control structures at the catchment level where water quality control depends on a reduction of drainage volumes and sediment transport. Location of the control structures within the catchment will depend on the discharge and drainage area above the project site. The upstream drainage area may be several thousand hectares. In this case, it may be more economical to control the water level in field ditches with smaller structures. This may cost less than larger structures on main channels.
The existing drainage network is also an important consideration. The depth and width of the channels govern water storage capacity. If a dual controlled drainage/sub-irrigation system is to be implemented, channel storage capacity has to be sufficiently large to provide irrigation water.
Topography and soils
Water table management is best suited to relatively flat lands. Sloping lands would require many control structures, to provide uniform water tables. Land slopes of less than 1% are recommended (Shirmohammadi et al., 1992). However, as pointed out by Evans and Skaggs (1989), while there is no absolute limit, slopes of less than 0.1% are the most practical. At the watershed scale, a slope of 0.1% would require a control structure every 300-1 000 m of channel length.
Soil permeability is a key factor governing the rate of water movement from ditches to adjacent fields, and the upward movement of water from the water table to the plant roots. Good lateral movement of water will occur in moderate to highly permeable soils. While Evans and Skaggs (1989) recommended a permeability of about 0.45 m/day, experience in eastern Canada has shown that water table management is also feasible on soils with permeabilities of 0.3 m/day. A restrictive soil layer, not far from the bottom of the subsurface drain pipes, will reduce deep seepage losses.
Monitoring requirements
Generally, a monitoring system needs to be implemented, both for system management and to measure water quality changes. Monitoring involves several factors and has to be performed on a regular basis, both at the field level by the farmer and at the catchment level by the water authorities.
Water quality impacts
Actual and potential detrimental impacts of drainage water quality both within the watershed and downstream should be identified. These investigations will help determine the principal water quality constraints and the most sensitive periods for control. They are best conducted by water authorities. The data may be useful in guiding fertilizer application.
Water table levels
Water table management requires periodic monitoring of the water table at the midpoint between drain laterals or ditches. It is best done by the farmer. One observation well per field is the minimum recommended. For a given soil and drain spacing, the frequency of observation and adjustment of the control structure depend mainly on the weather and crop development stage. For example, when crops are in their early stages of development, a shallow water table may impede proper root development, and this could make the crop more susceptible to drought later in the year. Research by Madramootoo et al. (1993 and 1995) indicates that water table levels between 0.50 m and 0.75 m from the soil surface are appropriate for most crops.
The response of the water table to rainfall and control structure adjustments may be slow, depending on the soil properties and drain spacing. It may take several years to fully understand the response patterns and successfully operate the system. Accurate weather forecasts and the ability of the farmer to make use of climatic data could improve system operation.
Assessing system efficiency
Water agencies should organize a medium or long-term monitoring programme of the impacts of water table management on the hydrology, water quality and crop performance in the project area. Some of the main parameters to be monitored are shown in Table 7.
TABLE 7 Monitoring requirements
Impacts
Parameters to monitor
Monitoring interval
Peak flows
Surface runoff, subsurface drain discharge, water levels in streams
1-hour intervals with automatic devices
Soil erosion
Sediment in surface runoff
Accumulation after a rainstorm, using sediment samplers
Nutrient losses
Nitrate, ammonium, phosphorus
Monthly, during the growing season
Pesticide losses
Pesticide concentrations
Monthly, during the growing season
<section>8</section>
Chapter 4 - Drainage water re-use
Michael C. Shannon, USDA Salinity Laboratory, Riverside, California, USA
Vashek Cervinka, California Department of Food and Agriculture, California, USA; and Dick A. Daniel, CALFED Bay Delta Program, Sacramento, California, USA
Re-use for crop irrigation
Re-use for saline agriculture and forestry
Re-use in a natural wetland
Re-use is an important and natural method of managing drainage water. In order to develop the maximum benefit from a water supply and to help dispose of drainage waters, strategies for water re-use have evolved. Water re-use must be balanced against both short and long-term needs, with consideration for both local and off-site effects. In regions where irrigation water supplies are limited, drainage water can be used to supplement them. However, the quality of the drainage water determines which crops can be irrigated. Highly saline drainage water cannot be used to irrigate salt-sensitive crops. It could, however, be re-used on tolerant forages or in a saline agriculture-forestry system. Saline drainage water is being successively re-used for the irrigation of salt-tolerant crops and trees. Where an irrigation project is located near a natural wetland, the drainage water can be re-used in the wetland. However, precautions will have to be taken to ensure that the quality of
the drainage water does not harm fish, waterfowl or other wildlife in the wetland and that the amount of water passing through the wetland is sufficient to prevent toxic concentrations from developing.
Re-use for crop irrigation
Effects of salinity on crop growth and yield
Agricultural management practices
Managing cycling and blending strategies
The major degradation factor of re-used waters is the high concentration of ions. Waters with low ionic concentrations provide plants with an adequate supply of many of the essential nutrients needed for growth. However, as salinity increases, specific ions may become toxic or interfere with the uptake of other nutrients. In soils, the accumulation of ions increases the osmotic potential against which plants extract water. It can also degrade soil structure. Drainage and leaching of salts from the root zone are key factors in the management of salinity in agriculture. Another management factor is control of the range of salt tolerance expressed in crop species. Water re-use for agricultural crops has distinct economic incentives and a number of crops are known to be highly tolerant to salinity. However, as salinity increases in the irrigation water, there is a greater need to monitor and manage irrigation and drainage practices and to consider the sustainability of the
system.
It is possible to safely re-use agricultural drainage water if the characteristics of the water, soil, and the intended crop plants are known and can be economically managed. Poor quality water requires selection of crops with appropriate salt tolerances, improvements in water management, and maintenance of soil structure and permeability (tilth, hydraulic conductivity). When sensitive crop growth stages such as germination and early growth are excluded, the temporal weighted mean root zone salinity has been found to be a valid measure for evaluating crop response to salt. The arithmetic mean root zone salinity within the rooting depth integrated over the time of exposure is an effective approximation for estimating crop response. Plants respond to the weighted mean salinity within a specific growth period.
Effects of salinity on crop growth and yield
Salinity in water or soil is an environmental factor that, in general, reduces crop growth. At relatively low salinity, especially among crop species such as cotton or the halophytic sugar beet, some salinity may actually improve crop production. This effect has been attributed in some instances to an improvement in water use efficiency of the plant (Letey, 1993). However, as salinity increases beyond some threshold tolerance, yield decline is inevitable. Usually, at low to moderate salinities, plant growth is simply reduced and there is a slight darkening in leaf colour. These effects are difficult to detect without comparisons with non-saline controls. When salt concentrations in the soil water reach toxic levels, leaves or shoots may exhibit visible symptoms of tip or edge burning or scorching due to high internal concentrations of salts. Other visible symptoms may be associated with nutrient imbalances caused by competitive interactions between Na^+ and Ca^2+ or K^4+, or
between Cl^- and nitrate (Grattan and Grieve, 1992). Interactions of salinity with nutrients such as PO[4]^3-, Mg and micronutrients have also been reported, but these observations seem to be specific to certain crops and waters of specific ionic composition. Depending upon crop species and salinity concentration, salt in the crop root zone may also influence the rate of plant development by increasing or decreasing the time to crop maturity (Shannon et al., 1994). In most crops there are some notable differences in root/shoot ratio, a response that would not be identifiable in the field. In some crops, salinity changes plant growth habit or increases succulence (Luttge and Smith, 1984; Shannon et al., 1994).
There is a wide range in plant species response to salinity. Sugar beet, sugar cane, dates, cotton and barley are among the most salt tolerant; whereas beans, carrots, onions, strawberries and almonds are considered sensitive (Maas, 1986). In general, salinity decreases both yield and quality in crops, and previous research has led to the development of large data bases on the salt tolerances of many crop species and varieties (Francois and Maas, 1978 and 1985; Maas, 1990;
Maas and Hoffman, 1977). Salt tolerance can be represented most simply based on two parameters: the threshold salinity (t) which is expected to cause an initial significant reduction in the maximum expected yield (Y); and the slope (s) of the yield decline. Slope is simply the rate that yield is expected to be reduced by for each unit of added salinity beyond the threshold value. The formula to calculate relative yields is:
YR = Y - s (EC[e] -t) where EC[e] > t [1]
Figure 5 shows the general relationship between relative crop yield and soil salinity. The threshold value and the slope of the yield decline line for beans, corn, wheat and barley are shown to illustrate different crop responses in each of the broad tolerance classifications.
Usually, salinity is measured in units of electrical conductivity of a saturated soil paste extract (EC[e]) taken from the root zone of the plant as averaged over time and depth. Soil paste extracts are water taken from soil samples which are brought up to their water saturation points (Ayers and Westcot, 1985). Electrical conductivities are measured on the filtered water extracts from these samples in units of deciSiemens per metre (dS/m). For most natural waters, 1 dS/m is equivalent to about 640 mg/litre salts. New methods use electronic probes or electromagnetic pulses to estimate EC[e] with less time and effort (Rhoades, 1976 and 1993).
FIGURE 5 Relative crop yield versus EC[e]
Where there is uncertainty in the determination of the t and s parameters in Equation 1, crops have been classified in a general range from sensitive to tolerant based upon available data (Francois and Maas, 1994; Maas, 1986). Crop salt-tolerance values and classifications are extremely useful in predicting the effects of re-using a water of a specific salinity on a particular crop.
Most salt-tolerance data are based upon the effects of saline waters predominated by sodium chloride, sometimes with varied amounts of calcium as needed to avoid the development of soil permeability problems associated with soil sodicity. Drainage waters or waters re-used from agricultural processing or manufacturing operations may predominate in other chemical species and/or have high concentrations of B, Se, As or other ions that may be environmental hazards (Francois and dark, 1979b; dark, 1982).
Agricultural management practices
Irrigation systems and scheduling
Traditionally, the goal of agriculture has been to maximize yield and profit, defined by the net difference between inputs and outputs. To maximize yields, soil salinity is reduced by leaching. The kind of management directed toward this goal is mediated by costs of water, drainage, applied nutrients and amendments to soils and waters. Where water quality and/or quantity is limited and where there are restrictions on drainage, the hazardous effects of salinity on crop growth and the dangers of either insufficient or excessive leaching are serious concerns. However, crops can be grown with saline waters provided that suitable irrigation and cropping strategies are used (Rhoades, 1988). Management needs to be more intensive and more precise methods should be used for water application and distribution. Water requirements needed for crop use and leaching should be accurately assessed and provided in a timely manner. Most crop water use coefficients have been developed for
non-saline situations (Erie et al., 1982). With saline water, growth and consumptive water use are reduced. However, preliminary evidence indicates that some crop water use coefficients may apply over a range of salinities because the water unused by the crop is needed to offset the increase in the leaching fraction requirement (Letey and Dinar, 1986). The greater the salinity of the irrigation water, the greater the need for adequate irrigation and drainage. Rates of salt accumulation in the soil are dependent upon the amount and concentration of the saline water applied and the amount remaining after plant water needs have been met. In a properly managed sustainable system, there may be a high salt content but no continuing accumulation.
Soil salinity is spatially and temporally non-uniform. Salinity often rises during the summer and then decreases with winter rains. This is a desirable condition which helps reduce permanent salt accumulation. However, under such circumstances, correlating yield responses to such variable salinity is difficult. Best management practices include land grading as a method to provide uniform surface irrigation, or the use of sprinkler or drip irrigation.
Crops irrigated with sprinklers are subject to injury not only from salts in the soil but also from salts absorbed directly through wetted leaf surfaces (Maas, 1985). In general, plants with waxy leaves are less susceptible than others. When saline sprays come into direct contact with leaf surfaces, salts can accumulate to toxic levels in leaf tissue and cause decreased yields. In tree and vine crops, the extent to which leaves are wetted can be minimized by sprinkling under the canopy. However, even with under-canopy sprinklers, severe damage to the lower leaves can occur (Harding et al., 1958). The extent of foliar injury depends on the concentration of salt in the leaves, weather conditions and water stress. For example, salt concentrations that cause severe leaf injury and necrosis after a day or two of hot, dry weather may not cause any symptoms when the weather remains cool and humid. As foliar injury is related more to frequency of sprinkling than to duration
(Francois and dark, 1979a), infrequent heavy irrigations should be applied rather than frequent light irrigations. Slowly rotating sprinklers that allow drying between cycles should be avoided, as this approach increases the wetting-drying frequency. Sprinkling should be done at night or in the early morning when evaporation is less. In general, poorer quality water can be used for surface-applied irrigation than for sprinkler irrigation.
Drip irrigation gives the greatest advantages when saline water is used. Drip irrigation avoids wetting of the leaves with saline water and can be managed to maintain relatively high soil water potentials. As drip irrigation is normally applied frequently, there is a continuous leaching of the soil volume from which the plant extracts water. General leaching can be provided intermittently between growing seasons and it is supplemented by seasonal rainfall.
Leaching requirement
In order to calculate the leaching requirement (LR), root zone salinity estimates are typically weighted to account for water uptake at different depths. The root zone may be conceptually divided by depth into four quarters, with a typical water uptake distribution pattern of 40:30:20:10 starting with the most shallow profile. This assumes a 'normal' rooting distribution but modifications should be made according to the frequency or type of irrigation (e.g., drip, sprinkler, alternate furrow). Of the various methods for calculating LR, the simplest is the equation proposed by Rhoades (1974) used in conjunction with salt-tolerance yield parameters:
LR = EC[i] / (5EC-EC[i]) [2]
where ECi is the electrical conductivity of the irrigation water, and EC is either EC[e] at t or at the acceptable yield level below t, and LR is the fraction of the irrigation water that must be passed through the root zone to control salinity.
Other management techniques
There are a number of cultivation and management methods which may offset yield loss when saline water is used for crop production. Some simple methods include: using more vigorous cultivars; using screens to select seed of larger size and higher seed weight; increasing population densities to offset smaller plant size and reductions in tiller number; and placing seed on sloped or modified seed-beds so that salts drawn to the surface by evaporation accumulate away from the seed line. Another way to move salts away from the seed or plant base is to irrigate in alternate furrows. An additional method of ameliorating the effects of moderately saline drainage water is to match fertilization requirements with the chemical composition of the water and soils. As drainage waters often contain some of the essential fertilization requirements of plants, applied fertilizers can be reduced. On the other hand, as ion interaction and competition reactions affect plant uptake, additions of
calcium, potassium or some other element may be required for better crop growth. An important consideration is that high sodium concentrations in soils and waters may decrease soil permeability and prevent effective leaching of salts. The suitability of soils and waters is measured using a parameter called the sodium adsorption ratio (SAR). Soil amendments are sometimes a necessary part of salinity management schemes to reduce SAR. On the other hand, high EC (salt concentrations) of the water increase the soil permeability.
Managing cycling and blending strategies
Cyclic strategies for using waters of different salinities have been proposed and it has been demonstrated that cyclic irrigations can be successfully applied to crops during different growth stages or can be used with crop rotations between tolerant and sensitive crops (Rhoades, 1987 and 1989). The feasibility of cyclic and blended applications of high quality water with drainage water depends on both supply and the availability of storage, mixing and delivery systems. Where non-saline waters are available for critical irrigations, growers can take advantage of the fact that many crops are most salt sensitive during the germination and seedling stages and are much more tolerant during later growth stages.
Plant salt tolerance varies with phenological growth stage, and beneficial effects can be obtained by managing irrigations to take advantage of this fact. For example, at salt-sensitive growth stages, salinity effects can be reduced by providing higher quality water or by minimizing salt accumulation through increasing the number of irrigation cycles. In some crops, higher salinities applied during the later stages of development will improve yield quality by increasing the content of sugars or soluble salts in the fruit. The timely application of saline water during fruit development has been used as a strategy to improve both sugar content in melons and soluble solid content in tomatoes (Shannon and Francois, 1978). In response to some moderate levels of salt and drought stress, fruit trees sometimes exhibit significant increases in fruit set and yield, but usually at a cost in subsequent years due to reduced biomass production. Another disadvantage may be decreases in
shipping quality. Francois and dark (1980) found that increasing salt stress delayed fruit maturation in 'Valencia' oranges but did not effect fruit quality. Their results also indicated that salinity had no effect on total soluble salts (TSS), but Bielorai et al. (1988) measured slight increases in TSS and sugar contents of 'Shamouti' oranges when the chloride concentration in the irrigation water was 450 mg/litre (m g/g).
For cyclic use strategies, factors that should be considered include the effects of changes in salinity during the growing season, the average salinity distribution in the root zone, the interactions with climatic variables, and the effects of different soil types. After crop establishment, the salinity in the root zone averaged over time may be considered as the effective salinity exposure (Shalhevet, 1994). This means that in most cases, once crops are established, there will probably be little measurable difference in a field situation as a result of applying 4 and 10 dS/m water in alternate irrigations applied over a crop cycle as opposed to using a water of 7 dS/m in each irrigation. This is the result of compensating factors in both soils and plants. Models are being developed to predict the effects of different irrigation water salinities during the growing season on crop yield.
Blending is the mixing of poor quality drainage water with good quality irrigation water. Provided that the blended water is sufficiently low in total salinity and toxic ions, this is the most economic and environmentally acceptable means of disposing of drainage water. This strategy can potentially increase the salinity of the groundwater over time. Often, separation of waters of different qualities is the best strategy.
Egypt has an extensive drainage water re-use programme. Over 4 000 million m^3 of agricultural drainage water, produced in the upper Nile Delta, are re-used to supplement irrigation water requirements in the lower delta. Pumping stations lift the better quality drainage water into irrigation canals, where mixing occurs. The Ministry of Public Works and Water Resources has an extensive re-use quantity and quality monitoring programme.
Re-use for saline agriculture and forestry
Concept of agriculture-forestry systems and solar evaporators
System design and planning
Concept of agriculture-forestry systems and solar evaporators
The agriculture-forestry system and solar evaporator are still at the development stage (Tanji and Karajeh, 1992 and 1993; Jorgensen et al., 1993). The concept compares favourably with other intermediate disposal technologies. It manages drainage water as a resource instead of directly disposing of it into evaporation ponds, rivers or the ocean. Agriculture-forestry systems produce energy (biomass) while some other technologies for the treatment of drainage water have high energy demands. By significantly reducing the final volume of non-re-usable drainage water, agriculture-forestry systems provide an opportunity for seasonal discharging of drainage water into rivers during high water flows. Solar evaporators are preferable from the standpoint of wildlife safety as they provide for better bird control than do evaporation ponds. The trees create wildlife habitats, reduce air pollution, and enhance the overall environmental quality of fanning regions.
FIGURE 6 Agriculture-forestry and salt management systems
The agriculture-forestry method has been developed and tested in California for salt management on irrigated farmland. It has two objectives:
i. to utilize the drainage water as a resource to produce marketable crops;
ii. to reduce the volume of discharged drainage water directly on the farm.
The agriculture-forestry system for the productive use of drainage water and its disposal into a solar evaporator has been developed in semi-arid conditions in California and other regions. A diagram of the system is shown in Figure 6. The main function of the trees is to use and evaporate large volumes of drainage water. This can be achieved not only through sequential re-use but also by uptake of the drainage water from shallow water tables, or by intercepting the flow of drainage water from upslope. The trees can be viewed as biological pumps. The tree biomass offers a number of marketing options ranging from electricity generation to the production of biofuels or biochemicals. Drainage water is further concentrated through the irrigation of halophytes that also have marketing potential as food or industrial crops. The salt concentration in the drainage water in each succeeding stage is increased, but the volume of water is reduced.
About 80-85% of the initial volume of drainage water produced while growing salt-sensitive crops is sequentially re-used to produce salt-tolerant crops. The remaining 15-20% of drainage water, with increased concentration of salt, evaporates in solar evaporators. The design of the solar evaporator consists of a levelled area lined with a plastic liner on which the crystallized salt is collected The daily amount of drainage water discharged into the solar evaporator is correlated with daily evaporation rates to prevent water ponding. This makes the facility unattractive to wildlife. This is important as the concentration of Se has proven to be a hazard to wildlife. The agriculture-forestry system for the sequential re-use and reduction of drainage water volume can be viewed as the flow of water from salt-sensitive crops to salt-tolerant trees to more salt-tolerant halophytes and to a solar evaporator (Figure 6). The water flow is programmed and automated using a system of
sensor-controlled equipment.
The agriculture-forestry system operating in California for sequential re-use of water and salt removal utilizes drainage water with an initial salt concentration of about 7 000 mg/litre, following the irrigation of salt-tolerant crops. This concentration of drainage water has been conventionally disposed of into evaporation ponds, rivers and/or the ocean. The agriculture-forestry system concentrates salt in a significantly reduced volume of drainage water. This technology offers management options of salt crystallization in a relatively small area on farms (solar evaporators) or the discharge of a reduced volume of drainage water (brine) into solar ponds or natural sinks (e.g., the ocean). The crystallization of salt in solar evaporators provides management options for salt marketing or its (short/long-term) storage on farms or designated landfills for storage in perpetuity.
The system for sequential re-use of drainage water is also effectively reducing the level of Se. A typical Se concentration is 0.5 mg/litre in drainage water applied to trees, and 0.9 mg/litre in the reduced volume of drainage water applied to halophytes. Consequently, the estimated quantity of Se is about 50 kg per 100 000 m^3 of drainage water applied to trees, and it is reduced to about 27 kg per 30 dam^3 of drainage water sequentially re-used to irrigate halophytes. Selenium is reduced through volatilization and is also taken up by trees and halophytes. Selenium removal by trees is mainly concentrated in leaves, and the measured values (in terms of mg/kg) were 0.5-0.9 for eucalyptus, 0.6-1.0 for casuarina, 2.6-3.6 for athel, and 0.6-1.8 for halophytes. Successful experiments were conducted on the transfer of Se through harvesting halophytes and safely using this forage for cattle feeding (Frost, 1990).
System design and planning
The design of a solar evaporator is fundamental to the development of an integrated agriculture-forestry system. The size of the solar evaporator is a function of evaporation rates typical for the farming region. The area of the solar evaporator is calculated by:
where
Ae area of the solar evaporator (m^2)
DW[h] drainage water discharged from halophytes into the solar evaporator (thousand m^3)
EV annual evaporation rate (m)
C[e] coefficient of evaporation, for reduced evaporation of saline water.
Further research data are needed to estimate the coefficient of evaporation (Ce). Experimental data indicate that the salt concentration in the drainage water discharged from halophytes into a solar evaporator ranges from 32 000 to 41 000 mg/litre. The final concentration of the crystallized salt exceeds 180 000 mg/litre. The major components of crystallized salt are SO[4]^2- (56.70 %), Na (23.50 %), and Cl^- (8.40 %). The Se content ranges from 2 to 8 mg/kg.
The area of halophytes that can be grown is a function of the quantity and quality of drainage water recovered from the trees. Experimental data collected during a period of six years indicate that the increase of salt concentration in water drained from trees is about 2.8 times higher than of the drainage water (from conventional farm crops) applied to trees. This indicates that approximately 65% of the re-used drainage water is consumed by trees. The applied 'tree' water (drained from farm crops) is typically EC 8-10 dS/m and the applied 'halophyte' water (drained from trees) is about 28-30 dS/m.
To maintain adequate control of soil quality, the leaching fraction (the percentage of the infiltrated irrigation water that percolates below the root zone) must be sufficiently high to prevent a build-up of salts, Se and B in the soil profile. The SAR should also be monitored.
An agriculture-forestry area of 16 ha has the capacity to process about 110 000 m^3 of drainage water per year. The size of a farm that can be serviced by the agriculture-forestry system (including solar evaporator) is a function of several factors, such as cropping system, quality of irrigation water, irrigation water management, soil salinity, and the use of trees for controlling groundwater conditions (water uptake from high water tables or subsurface flows). It is estimated that 30 ha of trees, halophytes and solar evaporator can utilize and process drainage water from about 1 000 ha of conventional farm crops. The 30 ha area would consist of 18.75 ha of trees, 7.5 ha of halophytes, and 3.75 ha of solar evaporator. The relative size of areas is as follows: irrigated area, 1 000 ha; trees, 10 ha; halophytes, 4 ha; and solar evaporator, 2 ha.
The management of biomass from the trees and halophytes is essential for the performance of agriculture-forestry systems and salt removal. The trees and halophytes need to be selected for their salt tolerance, which should range from about 9 000 to 18 000 mg/litre for trees and 18 000 to 41 000 mg/l for halophytes. The halophytes should preferably be perennial plants. The other required characteristics of trees and halophytes include: high water demands, tolerance to frequent flooding, frost tolerance, and marketability of harvested biomass.
Eucalyptus camaldulensis is the species of choice for this particular salt management system because of its salt tolerance and high water requirements. To improve the quality of eucalyptus trees for agriculture-forestry sites in the San Joaquin Valley, a selective breeding programme was initiated in 1987. Selected trees have been systematically evaluated each year, and 22 trees have been chosen for tissue culture propagation. The programme is seeking to achieve a higher diversification of salt-tolerant trees. Additional experimental trees planted include casuarina, athel, acacia, mesquite and poplar.
The selection of halophytes has been based on literature review, field evaluation trials, and a survey of salt-tolerant plants in California. These plants are being selected not only for salt management purposes, but also with a consideration for their biological interaction with conventional farm crops. This is to avoid introducing species that could be potential weeds or host plants for insect vectors or plant viruses. Halophytes have been selected for salt tolerances ranging from an EC of 20 to 45 dS/m. Based on current field evaluations, the most promising plants include salicornia, iodine bush (Allenrolfea occidentalis), salt grass (Dichtilis spicata), and cordgrass (Spartina gracilis). Other promising halophytes include fivehook bassia (Bassia hyssopifolia), Jose tall wheatgrass (Agropyron elongatum), fat-hen (Atriplex patula), red sage (Kochia americana), Atriplex nummularia and Atriplex lentiformis.
Re-use in a natural wetland
Re-use of surface drainage water
Re-use of subsurface drainage water
Drainage water can also be used for wildlife or wetland habitat irrigation. However, prior to using agricultural drainage water for this purpose, the following questions should be addressed:
i. Is the water from a surface or subsurface drain system or both?
ii. What types of vegetation are to be grown?
iii. What constituents are in the water?
iv. Is the water available when it is needed?
v. Is an adequate volume of water available?
vi. Will it have positive impacts on wildlife and the environment?
vii. Will there be adequate runoff from the wetland?
viii. Is the wetland sustainable?
Re-use of surface drainage water
Where drainage water is derived from only surface drainage or tailwater sources, the main question is whether or not the water contains applied and persistent pesticides. In areas where strong environmental safeguards exist and pesticide container label restrictions are followed, there is little risk associated with the re-use of surface runoff or tailwater drainage water. Rice field drainage water accounts for a very large percentage of the water supply for managed natural wetlands in the Sacramento Valley in California and is generally safe for re-use.
Most of the surface derived drainage water is used to flood wetlands in the early autumn. In the rice growing regions, the fields are drained in the late summer or early autumn. This drainage pattern coincides with the autumn migration of the waterfowl to their wintering grounds.
Ideally, the winter waterfowl habitat is flooded to a depth of 20-50 cm. Depending on soil type, seepage and evaporation rates, the drainage water required for initial flooding will range from 500 to 1 500 m³/ha. Where local supplies of surface derived drainage water are available, water is used to maintain ponds from October to March. In warm climates, the annual evaporation or consumptive use is approximately 2 500 m³/ha, which causes a further salt concentration increase in the wetland outflow.
Typical native marsh plants grown with surface agricultural drainage waters include: smartweed (Polygonum lapothefolium), swamp timothy (Heleochloa schenoides), tule or hardstem bulrush (Scirpus fluviatilis), and cattails (Typha spp). These plants are grown under a moist soil water management regimen. Water is applied in the autumn of the year and held until spring soil temperatures begin to warm. This occurs in March or April in the Central Valley of California. When the soil begins to warm, the ponds are drained to mudflat conditions. This stimulates seed germination. In some areas they require no additional water until the autumn flood-up period. However, where summers are hot and dry, they will require one or more irrigations in July or early August to provide for optimal seed production for migrating wildlife food.
Re-use of subsurface drainage water
The re-use of subsurface saline agricultural drainage water for wetland management poses substantial challenges and can generate problems which could result in wildlife losses and habitat reductions. Although subsurface saline drainage water is typically free from contamination by applied pesticides or herbicides, it may contain soil or naturally derived toxicants or trace elements such as salts, nitrates, As, B, Cd, Cr, Pb, Hg, Mo, Ni, Se, Ag, U and V. Each of these constituents is potentially toxic independently, in combination with other constituents, or through the process of biomagnification in wildlife through the food chain. Careful analysis of subsurface agricultural drainage water during several periods of a yearly cycle is required before any plans for re-use as a water supply for wetlands can be considered. The provision of an adequate volume of flow-through water is important to minimize concentration of toxic elements due to evaporation.
At present, there are no comprehensive standards which establish safe levels of trace elements in water used for wetland habitat management. However, because of the high potential for food-chain magnification, most wetland managers intentionally refuse to use subsurface agricultural drainage waters which contain levels of trace elements above background levels. The potential for, and the costs of, clean up or remediation of a contaminated wetland dictate a conservative approach.
Where trace element contamination is not a concern, saline drainage water can be used to support a productive wetland habitat. The major consideration is the management of soil and water salinity. The maintaining of a salt balance between the applied water and the soil/water interface is key to the production of brackish water native marsh plants.
In general, water with a TDS level of 2 500 mg/litre or less is preferable for wetland management. Sometimes, water with a TDS level as high as 5 000 mg/litre can be used for short periods. Standard management practices involve an autumn flooding to a depth of about 20-50 cm, with these depths being maintained until January or February. In late winter, the ponds are drained to discharge drainage water and accumulated salts. The ponds are then refilled with new water as deeply as practicable. After approximately 14 days, the water is drained again. The drainage cycle is repeated two to three times before the cycle is completed. This process removes salt concentrated in the surface water through evaporation and allows for a rebalancing of the water/soil salt equilibrium.
The ponds are drawn down to a mudflat state in March or April to facilitate germination of desirable salt-tolerant marsh plants. Typical plants grown under this regimen include: alkali bulrush (Scirpus robustus), brass buttons {Cotula corinopifola), salt grass (Distichilis spicata) and tules {Scirpus acutus). Depending on the local soil and climate, one or more shallow irrigations may be necessary to bring germinated plants to full maturity.
In all cases, in order to prevent excess salt accumulation, water circulation is maintained when the ponds are filled with saline drainage water, and some constant rate of discharge or outflow from the ponds is necessary. By maintaining water circulation and some constant rate of water flow, outbreaks of waterfowl disease can also be avoided. Any wetland habitat supported principally with saline agricultural drainage water must be carefully managed and monitored to have productive wetlands. In addition, there must be an environmentally safe way (ocean, large river, salt lake) of disposing of the water as it is drained from any wetland area.
<section>9</section>
Chapter 5 - Drainage water treatment
Lawrence Owens, California State University Fresno, Visalia, California, USA
Walter J. Ochs, Water Management Consultant, Virginia, USA
Physical, chemical and biological treatment processes
Treatment in constructed wetlands
Physical, chemical and biological treatment processes
Selection of treatment process
Methods of treatment
Physical/chemical treatment
Biological treatment
A treatment example
The first portion of this chapter concerns the direct treatment of agricultural drainage water to improve its quality. The general process of selecting a treatment process is described, followed by a description of treatment methodologies.
Selection of treatment process
The first steps in the selection of any treatment process for improving drainage water quality are to thoroughly define the problem and to determine what the treatment process is to achieve. In most cases, either regulatory requirements or the desire to re-use the water will be the driving force in defining the treatment issue(s) to be selected for a particular drainage water. A thorough knowledge and understanding of these water quality criteria is required prior to selecting any particular treatment process. Most of the treatments discussed will not reduce the salt concentration in the water and some may result in increased salt concentration.
Historical data of the chemical constituents in the drainage water are used for decision making. Once the framework that governs the selection of the treatment process has been identified (e.g., regulatory requirements, re-use, or both), then the available data must be evaluated as to its adequacy for the preliminary selection of possible treatment options. In general, a more complete data record, in terms of both the length of record and the constituents monitored, will make the task of treatment selection easier. As a minimum, one would need the ranges of expected flow volume and concentrations of the constituents of concern. In addition, knowledge of any seasonal variations of flow or constituents would be highly desirable. If data are not available, a portion of the project budget should be allotted to obtain this information in order to avoid costly mistakes at a later stage in the process. It is also necessary, when using historical data, to establish the reliability
of the data. This can be done by reviewing the analytical procedures used, by comparing data with an independent source, or by comparing historical values with more recent data.
The next step in developing a successful treatment programme is to identify the selection criteria. This will require input from all parties involved in the project including the funding source(s), users (farmers, industries or municipalities), water districts, regulatory agencies and the public. Criteria that will affect process selection include: capital cost, O&M costs, land requirements, level of treatment required and regulatory constraints. These criteria need to be defined as closely as possible in the earliest stages of process selection. However, it is important to note that there will undoubtedly be modifications to the criteria as the project progresses.
Once the selection criteria have been defined, possible treatment options can be identified. A good way to begin is with one or more 'brainstorming' sessions where any possible options are first identified without any suggestion being rejected. It is important to have input from individuals with experience and expertise in a variety of areas, so as not to 'zero-in' on a particular type of technology initially. This list of all possible options should then be evaluated against the identified selection criteria and ranked according to how each option meets the criteria. It is also appropriate to evaluate the known advantages and disadvantages of each option. From these evaluations, there may be one clear option to pursue or there may be several options which meet the selection criteria. If there are several options, then additional, more stringent criteria should be used to narrow the field down to two or three preliminary selections.
Unless the preliminary option or options are standard processes in use with drainage water or have been successfully used in applications with water very similar to the water in question, it is highly advisable to conduct pilot-scale tests. In general, the larger the pilot system, the better it will indicate the efficiency of the process when built at full scale. The single most important issue in pilot-scale operation is to use the water which will actually be treated by the full-scale process. Drainage water is such a complex solution that synthetic mixes, while useful for initial and fundamental laboratory studies, are poor substitutes for evaluating actual treatment processes. Conditions in the pilot-scale system should approximate as closely as possible the operating conditions of a full-scale system. Hydraulic retention time (residence time), flow velocities, applicable loading rates and removal efficiencies of target constituents are typical parameters of concern to
be measured and evaluated in the pilot testing. If the pilot tests do not show that the treatment process can achieve the desired results, then the process must be modified and retested or other options must be investigated.
Once pilot testing is complete and there is at least one process that has been shown to be effective, then the best option can be selected for full-scale implementation. The options should be judged against the selection criteria developed early in the process. At this point, other factors concerning the processes should also be evaluated such as: complexity; number and expertise of personnel required to operate and maintain the system; waste products produced and their disposal; expandability of the system; ability of the system to remove the target constituents to below the currently desired levels (e.g., in case regulations become more stringent); and ability of the system to remove constituents which are not currently of concern (e.g., in case new constituents become regulated). The consensus of as many of the involved parties as possible is desirable in the final selection of the best treatment option. Achieving consensus at this point will avoid conflict in the future.
Methods of treatment
While a detailed description of each type of process is beyond the scope of these Guidelines, an overview will be given of the most common treatment methods which have application in treating agricultural drainage water. More detailed descriptions and design details of common treatment processes can be found in water and wastewater treatment texts, such as those by Montgomery (1985) and Metcalf and Eddy (1991).
Treatment approaches can be divided into three general types: physical, chemical and biological. Many processes exhibit both physical and chemical aspects and so are sometimes called physical/chemical or physicochemical treatment.
Physical/chemical treatment
Particle removal
Several physical processes aim to remove suspended particulate matter. While subsurface drainage water itself is usually low in suspended particles, these processes might be used in an overall treatment process for the removal of particulates formed in other stages of the treatment, such as removal of bacteria from a biological system or removal of precipitates formed in a chemical treatment process. Particle removal unit processes include sedimentation, flotation, centrifugation and filtration. Filtration further includes granular media beds, vacuum filters, belt presses and filter presses.
Adsorption
Adsorption is the process of removing soluble contaminants by attachment to a solid. A common example is the removal of soluble organic compounds via adsorption onto granular activated carbon (GAC). GAC is useful for its ability to remove a wide range of contaminants. Certainly, if pesticides were a concern for the drainage water being examined, the use of GAC adsorption would be a leading candidate for treatment.
Air stripping
Another possible treatment for removing volatile compounds from water is air stripping. In a conventional countercurrent air stripping operation, the contaminated water is distributed at the top of a tall reactor vessel that is packed with materials or structures with a high surface area and void ratio. As the water advances downward, clean air is introduced at the bottom of the reactor and moves upward. As the water and air make contact, volatile compounds are transferred from the liquid phase to the gas phase according to gas transfer theory.
Membrane processes
If removal of salts and production of a high quality water is the treatment objective, membrane processes or distillation (discussed below) will be leading process candidates. The separation of salts and organic compounds can be accomplished by using a selectively permeable membrane. Membrane processes are also finding use in the water treatment industry for removing particulates and microbial contaminants such as Giardia and Cryptosporidium (Jacangelo et al., 1991). Membrane processes can be divided into three main categories: dialysis, electrodialysis and reverse osmosis. Each of these processes requires some type of driving force energy to separate the contaminants from the clean water. For dialysis, the driving force is the difference in concentrations of the contaminant across the membrane (a concentration gradient). For electrodialysis, the driving force is an electrical potential. For reverse osmosis, the driving force is applied pressure. The use of membrane
processes for salt removal has generally been considered too expensive for drainage water, but new developments in membrane technology may make this an attractive option for treating at least a portion of the flow to reduce the total salt concentration. The use of a membrane process such as microfiltration for particle removal may also be a cost-effective alternative as part of an overall treatment system.
Distillation
Distillation is a thermal process used for salt removal. Heat is used to vaporize the water, leaving the salts behind. The water vapour is condensed to a high quality water. Distillation is energy intensive and has largely been replaced by reverse osmosis for desalination applications.
Coagulation and flocculation
Coagulation and flocculation are used to remove particles of all types from water. The particles might be colloids present in the drainage water that are too small to remove by gravity settling or filtration, or they might be colloidal precipitates formed during a treatment process. Coagulation is the process by which these small particles are destabilized and the initial aggregation of the destabilized particles into larger particles called floes. Coagulation is accomplished by the addition of a coagulant, which can be either an inorganic metallic salt such as alum (aluminium sulphate) or ferric chloride, or a high molecular weight organic polymer. The coagulant serves to neutralize interparticle charge repulsion and to enmesh the particles into an aggregated floe. Flocculation is a slow mixing of the particles to bring them into contact with one another to form even larger particles. The objective is to produce a large fast-settling floe.
Chemical precipitation
Certain compounds, especially metals, can be removed by changing their solubility to cause their precipitation. Many metals can be precipitated as a metal hydroxide by increasing pH with lime or caustic soda to achieve the pH of minimum solubility. These would include Cr, Ni, Cu, Fe, Pb and Hg, and other elements such as As. The pH of minimum solubility varies according to the metal in question. Precipitation can also occur by the formation of insoluble compounds through adding certain chemicals, or from chemical species formed during another treatment step. Such is the case with sulphides and carbonates, which can be formed during biological treatment and combine with metals and some cations to form precipitates. Precipitation can lead to a slight net decrease in TDS.
Ion exchange
Ion exchange involves the chemical exchange of ions (charged dissolved molecules or atoms) in solution with ions on a solid phase. The solid phase, usually a synthetic organic resin, is chosen to specifically adsorb the constituent(s) of interest. To maintain the electrical charge balance, the resin must release an equal amount of charge into solution. For example, in water softening, a cation exchange resin initially holds Na^+, but releases them as it adsorbs Ca^2+ and Mg^2+ from the water stream moving through the resin. Anion exchange resins are also available. For either type of resin, there is a preferential adsorption of either the cations or the anions if more than one type is present. For example, calcium is more strongly adsorbed than magnesium, and sulphate is more strongly adsorbed than nitrate. Certain resins have been developed to maximize their affinity for a specific ion (e.g., borate).
Advanced oxidation processes
One major disadvantage of both GAC adsorption and air stripping for organic compounds is that they only transfer the contaminant from one phase (water) to another phase (carbon or air, respectively). Advanced oxidation processes (AOPs) are capable of compound destruction, or more accurately, the mineralization of chlorinated organic compounds to non-toxic constituents such as carbon dioxide, water and chlorides. AOPs rely on the production of highly reactive radicals to break down the organic compounds. AOPs are commonly based on the use of hydrogen peroxide (H[2]O[2]) or ozone (O[3]) in combination with ultraviolet (UV) light to cause radical formation. Another type of AOP uses photoactive metal catalysts and UV light to generate the radicals (Suri et al., 1993). Their high cost is the primary disadvantage of most AOPs.
Biological treatment
Biological treatment can be a useful tool in drainage water treatment for the removal of both organic and inorganic contaminants. Biological treatment usually refers to the use of bacteria in engineered reactor systems for effecting the removal or change of certain constituents, such as organic compounds, trace elements and nutrients. Algae have also been used and natural wetlands systems can be used in some cases to replace conventional reactors. The bacterial reactions involved can be divided into two major categories according to the use of oxygen by the bacteria. In aerobic systems, O[2] is provided and used by the bacteria to biochemically oxidize organic compounds to carbon dioxide and water, and possibly to oxidize reduced compounds before their release to the environment. Aerobic systems are usually odour free. In an aerobic system, oxygen is the electron acceptor and organic carbon sources are usually the electron donors in the biochemical reactions that take place.
In an anaerobic system, oxygen is excluded and the bacteria utilize compounds other than molecular oxygen for the completion of metabolic processes.
Biological reactor types can be broadly divided into two types: suspended growth and attached growth. In suspended growth systems, the bacteria are grown and maintained in suspension by mixing of the liquid contents. In attached growth systems, the bacteria grow in a thin layer (a biofilm) on an inert support, such as plastic media or fluidized sand. Both aerobic and anaerobic systems can have either suspended or attached growth depending upon the reactor configuration. For a discussion of specific reactor types, see Metcalf and Eddy (1991).
A treatment example
An example of drainage water treatment in the San Joaquin Valley of California will illustrate the combination of several of the processes discussed above. Work conducted by Squires of EPOC AG (1987), Macy et al. (1993) and Owens et al. (1995) shows the successful use of anaerobic biological treatment for the removal of Se from agricultural drainage water. In the process, an oxidized, soluble form of Se (selenate) is biochemically reduced to the insoluble elemental Se form. The process occurs through anaerobic respiration, with an added carbon source such as methanol or acetate acting as the electron donor and the selenate acting as the electron acceptor. In the process, nitrate is also removed by reduction to nitrogen gas (here the nitrogen in the nitrate is serving as the electron acceptor).
The particulate elemental Se can then be removed by membrane microfiltration (EPOC AG, 1987) and also by slow sand media filtration (Dhaliwal, 1992). Experiments using ferric chloride as a coagulant (Salamor et al., 1996) show improved particulate Se removal. As an added benefit, the iron hydroxide precipitate adsorbs soluble selenite, which is an intermediate formed during the biochemical reduction of selenate.
Treatment in constructed wetlands
Flow-through wetland functions
Planning and design of flow-through wetlands
Hydraulic and geohydraulic characteristics
Soils and geologic characteristics
Vegetative characteristics
Implementation, monitoring and management
The protection of wetlands is one area of public concern where agricultural and environmental values need to be further harmonized, shared and promoted. Ecologically sensitive wetlands need to be distinguished from poorly drained, wet, depressional cropland. The latter is farmland whose productivity can only be improved through better drainage. Wetlands which are transitional zones between terrestrial and aquatic systems serve valuable ecological functions.
The water table is usually near the surface, and such wetlands support fish, wildlife, waterfowl and aquatic plants. They also trap sediment and pollutants, and cycle nutrients. Wetlands are of hydrologic value in that they serve as flood control systems, recharge groundwater, and maintain instream flows. They also have economic value if they support recreation and other economic functions. A challenge is to ensure that agricultural and drainage activities do not negate the environmental value of wetlands. Cropland and wetlands must be viewed as part of a viable and sustainable ecosystem.
Constructed wetlands for domestic and industrial wastewater treatment are gaining acceptance in many countries. They can also be beneficial in providing treatment of agricultural drainage wastewater under many circumstances (Post and Ochs, 1995). Flow-through wetlands are one option for the management of agricultural drainage water. They can be described as constructed wetlands or natural depressional areas prepared to facilitate the movement of surface waters through specially selected vegetation. Flow-through wetlands are an alternative to evaporation ponds, and should be considered. Where the water quality is appropriate, they are preferable to evaporation ponds. Evaporation tends to concentrate the ions contained in the drainage water, and some of these substances can become toxic in the ecosystem. Concentration of the ions reduces the opportunity for beneficial use of the drainage water in downstream areas by degrading receiving waterways or water bodies. Wetlands can
reduce some pollutants such as N, P and sediment, and improve water quality. Wetlands consume water and will reduce the total available water for downstream users even though the general water quality may be improved. Water use by wetlands is approximately equal to potential ET rates.
Flow-through wetland functions
The primary values of flow-through wetlands for drainage water quality improvement from irrigated areas are to:
i. reduce the evaporation opportunity time which will minimize the direct concentration of salts and minor constituents which may become toxic when concentrated;
ii. provide physical filtration and sedimentation of soil particles and attached contaminants; and
iii. provide vegetation to remove excess N, P, K and organic wastes.
Secondary values in the use of flow-through wetlands are for wildlife habitat, recharge of groundwater aquifers, wind erosion control, and fish or shellfish production by:
i. providing breeding, nesting, feeding and cover habitats for invertebrates, insects amphibians, reptiles, birds and mammals;
ii. providing water detention that will facilitate infiltration of surface waters through the soil profile with opportunity for degradation of chemicals and vegetative uptake of pollutants;
iii. reducing wind erosion in the vicinity of wetlands, thus preventing wind blown contaminants in the air; and
iv. providing potential for fish or shellfish production if the pollutants present will not cause health problems to consumers.
Planning and design of flow-through wetlands
Site selection, data collection and the operational needs are of critical importance when planning and designing flow-through wetlands for drainage water management. Wetlands are often created to provide waterfowl habitat. However, consideration must also be given to the physical benefits achievable if the wetland or series of wetlands are developed to improve the quality of drainage waters. The topographic, soil and geologic characteristics will have a great impact on the effectiveness of pollution control benefits from the completed system. The vegetative materials selected for growth within the wetland will determine the success of the effort. The vegetation must be tolerant to the extremes in salinity and pollution anticipated and must be selected to maximize the uptake of nutrients and pollutants. Normally, a series of wetlands with different vegetative and soil materials will facilitate operational management and improve system effectiveness. Considerations for
harvesting the vegetation and management of the vegetation may require interior dikes and gate structures (Post and Ochs, 1995). To provide practical removal of some pollutants, harvesting of the vegetation will be required and economic considerations for disposal or marketing of the vegetation should be evaluated. Plants are being developed that have a superior capability to remove pollutants from soil, such as penny cress and other metal 'scavengers'. These plants can be grown periodically to rejuvenate the wetland and enhance the sustainability of treatment benefits. At the end of the growing season, the plants can be cut, dried and burned. Metals can be extracted from the ashes for recycling.
Hydraulic and geohydraulic characteristics
Site selection involves consideration of hydraulic and geohydraulic characteristics of the soil and subsoil to ensure the success of the system. Flow-through wetlands must be located where the drainage waters to be managed can physically be delivered. The topography must be suitable to allow the construction of dikes and outlet facilities. Proper soil must also be available nearby for embankments and lining the wetted perimeter of embankment areas. The site must have depressional characteristics that are large enough to allow the creation of water bodies with a depth range that will facilitate a vigorous growth of the selected vegetative materials.
Water depths in the wetlands must be controlled to provide the appropriate environment for growing the specific vegetation desired. The water depth and flow regime must also consider vector control and health considerations related to water borne diseases (see Chapter 7). The sizing of each wetland should be related to the opportunity time required for the uptake of the pollutant or pollutants that each vegetative wetland area is designed to reduce. The sizing of the constructed wetlands should also consider the volume of drainage water that will be transported to the wetland system for improving its quality. The water discharged from the wetlands must have a suitable outlet.
Soils and geologic characteristics
Effective constructed wetlands should permit vegetative materials to proliferate. Soils of the wetland should have a permeability which is sufficiently low to minimize percolation. Wetlands should be located in areas that are not porous, as retention of water is the primary characteristic required. If the site has sections of porous soil, then borrow pits with clays or other less porous material may be required to facilitate the lining of these highly permeable areas. As pollutants are removed in the initial wetlands, drainage water is discharged through lower wetlands in the system and it may recharge the groundwater system in the lower areas. Barriers to infiltration thus become an important feature in the design of systems in the upper wetlands and are not necessary in the lower wetlands. The accumulation of dead plant material, suspended solids, algae and other clogging material on the bottom can significantly reduce seepage rates.
Cavernous geologic areas should be avoided unless special precautions for seepage control and dike stability are taken. The expected quality of infiltrated waters from a constructed wetland system and the groundwater must be analysed to ensure that groundwater degradation does not occur.
Vegetative characteristics
As far as is both possible and practical, the vegetative types must be carefully selected to facilitate the reduction of pollutants. Normally, a mix of vegetation will optimize the uptake of nutrients. Consultation with plant material experts is essential for the success of wetland systems designed to remove pollutants. Field trials and laboratory testing of the vegetation will often be necessary prior to its introduction. This is particularly true if the vegetation is new to the region. Harvest considerations must also be made when selecting vegetation. The harvested biomass may have some value that will reduce the operational cost of the system.
Grass and reed types of vegetation are normally the most effective for improving water quality. However, some tree species may add diversity and habitat value to the wetland area if included in the site plans. A good source of information on vegetative design for constructed wetlands, as well as other planning and design parameters, can be found in the wetland restoration chapter of the Soil Conservation Service (1992) publication.
The establishing of the proper vegetation type will probably require a temporary water management plan to facilitate early vegetative establishment. As vegetation selection guidelines have not been developed for every climatic region, or for each pollutant, field trials and tests are strongly recommended prior to constructing wetlands.
Implementation, monitoring and management
Pollutants generally move with the water flow, thus slow, continuous, uniform flow will provide the optimum pollutant uptake opportunity for the vegetation. The flow pattern should be designed to minimize evaporation in order to reduce the risk of developing water toxic to the ecosystem and to avoid channelling from inlet to outlet.
The monitoring of water quality parameters is of utmost importance. For this to be done properly, it is important to establish the baseline situation. This means that an effort will be needed to determine the discharge quantity and quality of the water to be treated throughout the year prior to passage through the wetlands. Monitoring will be required to properly manage the wetland and to determine the effectiveness of individual wetlands.
Improved agriculture technology and irrigation water management will probably change the future composition and quantity of the discharged water. Thus, an interactive management programme should be developed that is effective for the conditions of the designed flow-through wetland.
Institutional responsibility is important to the success of wetland systems for water quality and habitat protection. If the water quality control system is to remain effective, it will require numerous adjustments and proper maintenance. It must be managed in accordance with the designs for the primary function of water quality improvement. However, secondary functions should not be ignored and often require some special management to achieve the benefits desired. The institution responsible for the wetland needs to have the financial means to operate, monitor and maintain the system in a sustainable manner. The one constituent that wetlands cannot remove is salt.
<section>a</section>
Chapter 6 - Drainage water disposal
William R. Johnson, Consulting Engineer, Modesto, California, USA
Kenneth K. Tanji, University of California, Davis, California, USA; and Robert T. Burns, Westland Water District, Fresno, California, USA
Disposal to natural hydrological systems
Land application and retirement
Evaporation ponds
Deep well injection
Disposal to natural hydrological systems
There are a limited number of options available when trying to decide where and how to dispose of agricultural drainage Water into the natural hydrological system. The common option is to return the water either to the land as part of the irrigation water supply, or to rivers and lakes, or to salt sinks, such as the ocean. The options available to any single project may be limited because of water quality concerns. Drainage water quality may vary within a catchment. Frequently, in developing countries, agricultural drains are used for the disposal of domestic and industrial wastewater, or for the disposal of polluted water from other non-agricultural sources. This may adversely affect the quality of agricultural drainage water, and limit its potential for reuse.
Surface water
Downstream beneficial uses of any surface water body to which drainage water is added must be protected. For example, it may not be appropriate to discharge saline drainage water into a river or lake when that surface water body is being used for domestic or agricultural water supplies.
In many cases, it may be possible and fully acceptable to discharge drainage water into a large freshwater body. However, it will be necessary to determine the assimilative capacity of the receiving water and identify the constituents in the drainage water to determine the 'safe level' or discharge requirements for the drainage water. The discharge requirements should specify the maximum allowable concentration of each constituent of concern and the volume of drainage water discharge that will be acceptable. In some cases, there will be a significant difference between the quality of the drainage water and that of the receiving water, while in other cases, there will be little difference between the two. The dilution capacity of the receiving water will vary from place to place and from time to time depending on numerous local conditions and the upstream uses of the receiving water. The discharge of drainage water of a higher quality than the receiving water is generally
acceptable. Pollutants in the drainage water may end up in the channel bed material. Here, they may create subsequent water quality problems, if and when the channels need to be dredged.
Special attention should also be given to the pollution hazards posed by regional seepage flows. This seepage may pollute the receiving surface water with solutes picked up during movement through various soil or rock formations. This may reduce the dilution capacity of surface water. Shallow seepage may cause excessive nutrient loading, while deeper seepage may convey toxic geochemicals.
Mitigation to facilitate the disposal of drainage to surface waters should preferably start at the field level. Measures such as the construction of retention ponds and the establishment of riparian border strips can be taken to reduce the direct inflow of polluted surface drainage water into the receiving system. In the receiving system, chemical concentrations may be further reduced by dilution and mixing. Less polluted or even fresh water from upstream storage can sometimes be used for dilution and flushing, so as to facilitate safe drainage discharge into streams and lakes.
The disposal of drainage water from acid sulphate soils is unique. The pH of this drainage water can be as low as three or four. Generally, this type of drainage water must be highly diluted in the main drainage system. Where dilution is not possible, special arrangements should be made to flush and transport this acid drainage water from the system.
Lakes
In most cases, lakes must be given special consideration, as there may not be an adequate outlet or flow volume from the lake, and contaminants may accumulate. This could cause substantial long-term problems to any beneficial uses of the lake water, such as aquatic habitat.
Rivers
Generally, rivers continually cleanse themselves and can tolerate somewhat higher pollutant constituent loads than lakes. However, it is still essential that the quality of both the drainage water and the river water be evaluated to determine the safe level of constituents that can be placed in the river without affecting downstream beneficial uses.
Estuaries, bays and oceans
Estuaries, bays and oceans have somewhat different water quality requirements. Considerable exchange of water takes place in the estuarine, bay and ocean environments due to the changing tides and ocean currents. In addition, the introduction of saline drainage water into a salt-water environment generally reduces the impact of the discharge of the drainage water on the receiving water. However, each receiving water needs to be analysed and compared with the drainage effluent in order to protect beneficial uses of the receiving water. Nutrients in drainage waters may cause problems in estuaries. If possible, the oceans and seas should be the ultimate natural disposal location for saline drainage water.
Land application and retirement
In irrigated areas, there is often some low quality land available near to where agricultural drainage problems occur. Such land can be used for the disposal of drainage water in evaporation ponds or constructed wetlands, or for re-use in saline agriculture-forestry. However, if the drainage water contains potentially toxic constituents, such as Se, then disposal of drainage water in evaporation ponds or in wetlands will be environmentally hazardous. In California's Central Valley, drainage water containing Se in concentrations greater than 2 m g/litre is not recommended for use as water for ponds or wetlands (California Regional Water Quality Control Board, 1995). Some proportion of the water must be flushed through such wetlands to prevent toxic ion concentration from accumulating to dangerous levels.
Land retirement is sometimes portrayed as a 'solution' to an agricultural drainage problem where salt is allowed to accumulate on the soil surface due to evaporation from a shallow water table. The principal difficulty with land retirement as a 'solution' to drainage problems is that it takes a considerable amount of land out of production. This is obviously a perplexing issue when farmers lose half of their land (the salt sink area) in order to continue production on the remainder. This concept of taking care of a salt balance-waterlogging problem has recently been termed 'dry drainage' (Gowing and Wyseure, 1992). The objective is to have the non-cropped area large enough and the evaporation from this area fast enough to achieve the necessary salt balance and a stable water table over the entire area. The theory is that if inflow balances outflow, then the water table will be stable. The cropped area must be continuously irrigated to avoid salt accumulation in the
cultivated portion of the fields, since the water table in the irrigated lands will be higher than it is in the adjacent lands. Dry drainage is not recommended.
Considerable research, study and economic evaluation must be undertaken before this technique is selected as a 'solution' to an agricultural drainage problem. Gowing and Wyseure (1992) point out three important questions that must be answered:
i. What is the limiting cropping intensity?
ii. What is the limiting water table depth?
iii. What is the long-term impact of salt accumulation in the drainage salt sink area?
'Dry drainage' adds to the evaporative water loss of the area.
Evaporation ponds
Constructed evaporation ponds
Pond hydrology
Pond water chemistry and mineralogy
Pond biology and toxicity
Biological, chemical and physical treatment options
Disposal of runoff and drainage waters into natural depressions has been practised for centuries. The impounded waters are dissipated by evaporation, seepage and transpiration losses. Use of constructed disposal basins for saline agricultural drainage waters are also common worldwide where there are constraints on discharging into natural salt sinks such as the oceans and inland closed basins. Examples of such practices include the salinity control and reclamation projects (SCARP) along the Indus River in Pakistan (Trewhella and Badruddin, 1991); the irrigation projects along the Syr Darya and Amu Darya Rivers in Kazakhstan and Uzbekistan (Micklin, 1991); and those along the Murray River in Australia (Evans, 1989). Constructed basins managed for the evaporation of saline drainage waters are a comparatively new practice (Tanji et al., 1993).
Constructed evaporation ponds
In the Murray-Darling basin in Australia, some of the constructed evaporation basins are intended to hold saline water only temporarily. The stored waters are then released during high river flows. Closed basins are expected to have operating lives of 50-150 years. The surface areas of these basins range from a few hectares to about 2 000 ha. The waters impounded in the Australian basins are typically dominated by NaCl type salts. Data on trace elements have not been reported. Lateral and vertical seepage losses from these basins are substantial, 20-50% in most basins (Evans, 1989).
In the San Joaquin Valley of California, evaporation ponds are used in areas where there are no opportunities for discharging saline subsurface drainage water. The ponds are constructed by excavating soils from the interior of the basins to build up embankments. Drainage water is discharged into the ponds by pumping. One of the first large-scale evaporation facilities established in the United States in 1975 was in the Lake Tulare bed, a hydrologically closed basin in the southern San Joaquin Valley (Summers Engineering, 1995). From the mid-1970s to 1985, 28 basins were installed in this valley. These basins occupy a total surface area of about 2 800 ha and vary in area from 2.5 to 730 ha and in depth from 0.5 to 2 m. Many ponds consist of one to three cells, with a few ponds having from six to eleven cells. In multi-cell systems, water is routed by gravity from cell to cell to optimize evaporation rates in the initial cells and precipitate most of the salts in the terminal
cells (Ford, 1988).
Pond hydrology
The evaporation basins in the San Joaquin Valley receive about 3.9 million m³/year of subsurface drainage water from about 22 700 ha of subsurface drained fields (Ford, 1988). Some ponds or pond cells are allowed to dry, while others are maintained at a minimal water level. The ponds are typically located on clay loam to clay soils. Emergent vegetation is controlled but the submerged rooted vegetation and phytoplankton are not. Rainfall at the evaporation pond facilities is low, from less than 100 to 250 mm/year. The grass ET rate in this region is about 1 500 mm/year. The hydrology of ponds is comparatively simple. The inputs into the ponds include drainage water from croplands, rainfall and pumped drainage water from perimeter drains installed to intercept pond seepage. The outputs include water evaporation, unrecovered pond seepage and ET from aquatic vegetation (Tanji and Grismer, 1989).
Estimates of pond seepage rates have been made by point measurements as well as by mass balance of pond inflows, changes in water surface elevation and evaporation rates (Tanji and Grismer, 1989; Grismer et al., 1993). Mass-balance based estimates for several different ponds have yielded similar results despite differences in the soil texture of the pond bed material. Although initial seepage rates upon filling a newly constructed pond may be as great as 10 mm/day, seepage rates decrease dramatically in a few months to a few millimetres per day. This decrease in seepage is attributed to the plugging of conducting pores in the bed material by microbial slimes and colloidal soil materials.
Seepage losses are generally dependent on local soil, geohydrological and topographical conditions, and excessive seepage could lead to environmental problems. For example, in Pakistan, seepage from evaporation ponds caused considerable waterlogging and salinization in surrounding farmland. Therefore, pond site selection is important, and seepage losses should be controlled. A lining and leachate collection system may be required to ensure that there will be no contamination of usable groundwater. The type of lining, clay or synthetic, should be determined by material availability, local requirements or environmental regulations.
Evaporation from free water surfaces is influenced by many variables such as air temperature, wind speed, humidity, net solar radiation and water salinity. The evaporation rates from ponds have been measured with floating evaporation pans and diurnal monitoring of pond waters and nearby terrestrial climatic data (Tanji et al., 1992). Results from floating evaporation pans containing water of the same salinity in the pond show that evaporation rates from water surfaces decline with increasing water salinities due to a reduction in water surface vapour pressure. For example, pan evaporation rate for an 8-day period in September 1989 was 63.5 mm for water electrical conductivity (EC) of 14 dS/m, 54.5 mm for water EC of 30 dS/m and 52.4 mm for water EC of 47 dS/m. Evaporation rates have also been estimated using reference evapotranspiration, ETo, data: E=Y(ETo), where Y= 1.3234 - 0.0066 EC (dS/m) for water EC up to 60 dS/m. The measured correction factor, Y, was 1.2 for water EC
of about 20 dS/m, 1.07 for water EC of 47 dS/m and 0.92 for water EC of 59 dS/m. Where pond waters are evapoconcentrated to a point where evaporite minerals begin to form, a thin crust of evaporites may form on the water surface during cooler air and water temperatures (night-time), so effectively limiting water evaporation. However, during higher temperatures (daytime), that thin crust of evaporites on the water surface may melt back into solution. With further evapoconcentration, permanent salt deposits will form.
Pond water chemistry and mineralogy
The waters disposed of in evaporation basins or ponds have dissolved mineral salt concentrations ranging from 2 500 to 65 000 mg/litre (m g/g) (Ford, 1988). The estimated annual salt loading into the San Joaquin Valley ponds is about 880 000 t. This is equivalent to about 25 % of the annual salt accumulation in the 0.9 million ha of irrigated lands on the west side of the San Joaquin Valley (Tanji and Grismer, 1989). The waters impounded in the ponds are NaSO[4] or NaSO[4]/NaCl types. The principal sources of salts are the applied water and the chemical weathering of the marine sedimentary rocks in the Pacific Coast Range and the alluvium formed in the valley floor (Tanji et al., 1986).
Impounded drainage waters are evapoconcentrated up to about 388 000 mg/litre, more than ten times the 35 000 mg/litre seawater salinity. As the water evapoconcentrates, a sequence of different evaporite minerals form as their solubility products are exceeded. The suite of evaporites deposited is mainly regulated by the initial chemical composition of the disposed water and the evapoconcentration factor (Smith et al., 1995). A brine chemistry model has been validated to predict the sequence of evaporite formation with desiccation (Smith et al., 1995). The most common minerals to form in copious quantities during the early stages of evapoconcentration are calcite (CaCO[3]) and gypsum (CaSO[4]· 2H[2]O). With further evapoconcentration, glauberite [Na[2]Ca(SO[4])[2]], bloedite [Na[2]Mg(SO[4])[2]· 4H[2]O] and mirabilite (Na[2]SO[4]) are typically formed. The last evaporites to be deposited are halite (NaCl) and thenardite (Na[2]SO[4]· 10H[2]O).
Associated with these saline drainage waters are several trace elements of concern, including As, B, Mo, Se, U and V (Westcot et al., 1989; Tanji and Grismer, 1989). These trace elements are naturally occurring in the Cretaceous geologic formations of the San Joaquin Valley. Of principal concern is Se. Bioaccumulated in the aquatic food chain, it caused the reduced reproduction, the deformity and death of waterbirds at Kesterson Reservoir in the San Joaquin Valley in the 1980s (Ohlendorf et al., 1993). Selenium toxicity of waterbirds is occurring in a number of agricultural evaporation basins elsewhere in the San Joaquin Valley.
A survey of annual sampling of ponds and pond cells in the San Joaquin Valley revealed considerable variability in salinity and trace element contents (Ford, 1988). The geometric means of constituents of concern are 31 000 mg/litre TDS, including 25 mg/litre B, 101 m g/litre (ppb) As, 16 m g/litre Se, 2 817 m g/litre Mo, 308 m g/litre U and 22 m g/litre V. The trace element concentrations in influent pond waters are strongly influenced by physiographic locations (Westcot et al., 1989 and 1993). For example, B and Se are at elevated concentrations in subsurface drainwaters from alluvial fans, and As in waters from lake bed soils. With respect to pond bottom sediments, the geometric means of constituents of concern are 107 mg/kg B, 9 mg/kg As, 0.6 mg/kg Se, 6 mg/kg Mo, 9 mg/kg U and 57 mg/kg V.
Seven evaporite forming ponds and 55 naturally occurring hypersaline waters and their specimens of evaporites were sampled, and 55 pairs of samples were analysed for trace elements (Tanji et al., 1992). Of the evaporite samples analysed, none of the salt deposits contained Se, As, B and Mo exceeding California's hazardous solid waste criteria. By contrast, the concentrations of these trace elements in hypersaline waters approached or exceeded the local hazardous liquid waste criteria. Apparently, these trace elements in waters are somehow excluded or dissipated during the crystallization of salts and accumulate in the liquid phase.
As the evaporation pond facilities mature, salt deposits accumulate at rates of 0.9-15 cm/year (Tanji and Grismer, 1989). There is growing concern that dried salt beds may contribute towards salt dust storms, as in the Aral Sea area in Kazakhstan and Uzbekistan, and at Lake Owens in California. Salt dust storms from the desiccated Aral Sea have damaged downwind vegetation and affected the health of humans and animals (Micklin, 1991). There is a need to develop management strategies for deposited salts subject to aeolian erosion. In the San Joaquin Valley, however, salt dust storms do not constitute a problem at present.
Pond biology and toxicity
Selenium was found to be toxic to fish and waterbirds at Kesterson Reservoir in 1980 and 1981, respectively. The influent Se concentration of drainage water from croplands was about 300 m g/litre and had bioaccumulated in the aquatic food chain. Algae and rooted plants bioconcentrated Se by about 560-fold and 600-fold (69 and 73 mg/kg), respectively. Zooplankton and aquatic invertebrates (insects) feeding on algae and rooted plants, respectively, biomagnified Se by about 1.2-fold and 1.7-fold (85 and 122 mg/kg), respectively. Fish feeding on zooplankton and aquatic insects also biomagnified Se by about 1.5- to 2.2-fold (188 mg/kg). The net result was that the Se bioaccumulation factor from water to fish was 1 540-fold in the Kesterson pond.
A similar degree of bioaccumulation of Se took place in waterbirds, but was dependent on whether their food source was benthic or herbaceous. Due to the elevated salinities and harsh environmental conditions prevalent in many pond facilities, comparatively few species of plants, invertebrates and fish are able to tolerate the high water salinity, widely fluctuating water levels, high water temperatures and low dissolved oxygen (Tanji et al., 1993). However, widgeon grass (Ruppia maritima), waterboatmen (Trichocorixa reticulata) and brine shrimp (Artemia salina) can become quite abundant. These pond food items are eaten by waterfowl and shorebirds.
Selenium becomes toxic to birds when Se substitutes for sulphur in the essential amino acids methionine and cysteine. The hazards of Se to waterfowl may be best evaluated by measuring the Se content in bird eggs and livers (Ohlendorf et al., 1993). In normal non-contaminated aquatic habitats, the Se concentrations in avian eggs rarely exceed 3 m g Se/g. The likelihood of reproductive impairment of birds increases substantially when sets of eggs contain more than 20 m g Se/g.
Arsenic is another constituent of concern. Despite the presence of elevated concentrations of As in ponds located in the Lake Tulare bed, bird eggs do not accumulate As (Ohlendorf et al., 1993). Although B has bioaccumulated in bird eggs up to a maximum of 18 m g/g, B alone does not appear to cause toxicity in bird embryos. Evaporation ponds also contain elevated levels of Mo, but Mo concentrations up to 16 m g/g do not have adverse effects on birds.
Therefore, Se is the element of greatest concern to waterbirds attracted to evaporation basins in the San Joaquin Valley.
Biological, chemical and physical treatment options
The design and management of evaporation pond facilities to enhance desiccation of impounded waters and reduce hazards to wildlife are important considerations. This section addresses the potential biological, chemical and physical treatment options for removing contaminant hazards in evaporation pond waters. Numerous drainwater treatment and disposal options were investigated immediately following the discovery of Se toxicosis of waterbirds at Kesterson Reservoir (Lee, 1993). Among those investigated were: desalination reverse osmosis, microbially-mediated anaerobic reduction of selenate and selenite to elemental Se, microalgal-bacterial removal of Se, biomethylation and volatilization of Se, adsorption of Se to iron filings, chemical reduction of Se, deep well injection, and drainage water re-use. The removal of trace elements in influent and impounded pond waters is inherently difficult due to the saline matrix and extent of removal required to protect wildlife. The
various biological methods of removing Se in waters are capable of reducing Se to about 20-50 m g/litre. With additional chemical and physical treatment processes in tandem, reduction to about 10 m g/litre is possible. These levels of treatment are inadequate to protect wildlife from hazardous bioaccumulation of Se. The recommended criterion for Se in waters is 1 m g/litre for aquatic life and this criterion appears to be unattainable using currently known treatment technologies. Some dilution would also be required to achieve acceptable concentrations of Se.
The impoundment of drainage waters in evaporation ponds can have an effect similar to lagoon treatment of municipal wastewaters. Selenium, the principal constituent of concern in evaporation basins, is subject to a number of sink/dissipation mechanisms within the pond environment. Selenite, but not selenate, is strongly adsorbed by soil materials with exposed hydroxyl groups such as oxides of iron, aluminium and other metals. Inorganic Se may be reduced and immobilized into elemental Se and selenides. Indigenous bacteria, fungi and microalgae can methylate Se to dimethylselenide (DMSe) that may eventually volatilize into the atmosphere. In fact, biomethylation and volatilization of Se is the principal pathway by which Se enters into the atmosphere in the natural biogeochemical cycle of Se.
Other processes that remove Se from the water column are the uptake of Se by aquatic plants and the subsequent deposition of organic Se into bottom mud. However, the Se removed from the water column into immobilized forms may be regenerated back into the water column by oxidation. Moreover, Se bioaccumulated by aquatic biota may become part of the aquatic food chain and pose a hazard to animals at the uppermost level of the food chain. The only mechanisms by which the Se inventory may be dissipated from ponds are by inadvertent seepage losses or by volatilization into the atmosphere. The former is not desirable because of the potential contamination of shallow groundwaters. As for the latter, emission of DMSe does not appear to have adverse impacts.
Deep well injection
Concept and technology
Environmental considerations
Geological considerations
Case study
Concept and technology
Disposal by injection is a process in which liquids are pumped into a well for placement into porous rock or sand formations below the ground surface. The well is generally called a 'deep well' because the proposed injection occurs beneath the lowermost underground source of usable water. Most formations selected for disposal reservoirs in California are old marine sediments containing concentrated saltwater. Any porous and permeable rock formation such as sandstone can act as a disposal reservoir for the injection liquid.
In California, deep well injection technology has been used for more than 60 years for the subsurface disposal of oil field brines. During 1994, about 74.3 million m^3 of brine waters were disposed of in California oil fields by deep well injection as reported by the California Division of Oil and Gas. Oil field practices all across the United States have established deep well injection as a viable alternative method for the disposal of these types of industrial wastes. Injection well disposal of agricultural drainage water is a viable alternative for disposal where receiving formation conditions are adequate and the costs of injection are not prohibitive.
Environmental considerations
Aquifer quality
Prior to formal injection testing, the formation aquifer into which drainage water is to be injected must be sampled and tested to document baseline water quality and to assess chemical compatibility with the drainage water. Drainage water mixed with formation water should be analysed to ensure that the two fluids will not produce precipitates that would clog the injection well. Water from all zones sampled should be analysed for calcium hardness, total alkalinity, TDS, temperature and pH. The results from these analyses will serve as baseline data for later use if monitoring of confining layer containment is necessary.
Aquifer capacity
A suitable formation is usually a sand or sandstone which has sufficient porosity and thickness to receive the injection fluid. Significant space can be created over a large volume of receiving formation, when the formation is compressible. The disposal formation should be extensive enough to offer a sizeable reservoir and should be deep enough to allow adequate injection pressures in the injection wells.
The pressure build-up required for injection occurs at the injection well head and at varying distances from the well over a period of time. The injection fluids begin to displace natural fluids and a resulting pressure rise becomes evident at the well head. This well head pressure rise can be modelled using the Bernard formula for pressure build-up (URS, 1987). The Bernard formula relies on certain site specific data such as porosity, permeability, formation thickness and other data which can only be determined from core samples and well head pressure testing.
Monitoring programme
A continuously monitored leak detection system for the injection system must be incorporated into a Class I well design. A Class I well is designed to protect the entire area above the uppermost confining layer from leaks of the injected drainage water. The monitoring system would automatically shut off the injection pumps and instantly reduce the pressure during a sudden leak. An example of a leak detection system can be seen in Figure 7. The annular space between the injection tubing and the middle casing is filled with a non-corrosive saline fluid. The saline fluid exerts a specified annular fluid pressure along the column. This annular fluid pressure will remain constant if the tubing and cement plugs are not leaking. Pressure in the annular space can now be monitored during testing of the injection system by an electronic pressure sensor.
FIGURE 7 Injection well leak detection system
Permits
The construction of deep injection wells will generally require some form of permit. In the United States, a federal government permit is required from the USEPA pursuant to the Safe Drinking Water Act regulations (40 Code of Federal Regulations (CFR), Section 144 et seq.). The EPA underground injection control programme requirements state that an injection well must be below the lowermost formation that contains (within 400 m) a well used for drinking water and be within a geological formation capable of receiving the liquid. The EPA has designated underground sources of drinking water at a TDS level of 10 000 mg/litre or less. A Class I well under these requirements covers the use of injecting hazardous and non-hazardous wastewaters that are not considered part of oil field operations. The permit also requires extensive geological, construction, testing procedure, monitoring, well abandonment, and pressure build-up data. The information and findings of all relative data
are open to public scrutiny and hearings.
Geological considerations
Location and formation
The most important consideration in the siting of an injection well is to locate a suitable geological formation for injection of the drainage water. The formation overlying the injection formation must be highly impermeable so that it will act as a hydraulically confining barrier which will prevent upward migration of the injection fluid. Both formations should be thick enough to ensure the desired injection rate. The confining formation should be located as far away as possible from any known faults.
Proximity of source
The next most important consideration in the location of an injection well is distance from the drainage source. Long distance from the injection well site to the source results in added costs for construction, pumping and future maintenance. However, this may be required if no suitable injection formation is available in proximity to the drainage water source.
Formation permeability and porosity
The receiving formation for injection must have a high permeability, measured in millidarcies (md), to provide for an adequate injection rate. One millidarcy is about 1 mm/day for water. Core samples of the injection zone and confining layer may be analysed in the laboratory by testing the air permeabilities. The liquid permeabilities can be conservatively estimated to be 50% less than the laboratory measured air permeabilities. Porosity or the percentage of voids in a soil or rock sample should be estimated by using an appropriate method such as sonic log data from the open hole logs (Schlumberger, 1989).
Potential formation plugging problems
The plugging of injection wells by micro-organisms is a common problem encountered in wells that produce oil. Injecting agricultural drainage water which contains nitrate into a formation that contains organic matter and ferrous iron will produce excellent growth conditions for nitrate reducing bacteria. If the drainage water is untreated before injection, the formation will probably clog as the pores of the sandstone formation become filled by an accumulation of biotite.
There are two common chemical treatments used by the oil industry to improve the injection capability of an injection well. The first method is to continuously add sufficient chlorine to produce a chlorine residual of 0.2-1.0 mg/g. The best time to add chlorine to drainage water is just before injection. This treatment may solve only a small portion of the injection problem. If the sandstone formation has a biological nitrate demand, it may also have a chemical chlorine demand, and will remove chlorine from the chlorinated drainage water. A rapid and complete removal of chlorine from the drainage water may suggest a very reactive reducing agent is present in the formation. The second method for treating an injection well is to add a buffered solution of hydrofluoric acid to the well. Laboratory tests have shown that permeability can be increased, but not in all cases.
Case study
Description
Westlands Water District's prototype deep well injection project in the San Joaquin Valley of California was to dispose of up to 4 000 m³ /day of agricultural drainage water from the San Luis Drain. Drainage water was to be injected into two geologic shale and sand formations 1 554 m (Zilch-Temblor) and 2 164 m (Martinez) beneath the ground surface. The goals of the prototype project were to assess the technical, economic and administrative feasibility of deep well injection as a means of managing agricultural drainage water.
FIGURE 8 Well casing and cementing
Drilling and completion
The drilling of the well resulted in a total depth of 2 469 m. Figure 8 shows the final configuration of the district's well casing and cementing. The completion operations were accomplished by perforating the Martinez formation casing with 13 perforations per metre from depths of 2 245 to 2 344 m, and from 2 411 to 2 414 m. The total perforation length was 102 m. The Zilch-Temblor formation was not tested due to the EPA's decision not to allow the district to inject into this formation. Subsequently, the EPA was shown, by computer modelling analysis, that the Zilch-Temblor was also a safe and viable zone for injection.
Injection testing
Following the recovery of some natural formation fluid samples, an injection test was conducted. Injection fluid, consisting of filtered Westlands Water District irrigation water, was treated with 2% potassium chloride and a chlorine biocide. All fluid was filtered to 0.5 micron using a Pall filter. The water was injected through a 4.75 cm tubing at a rate of 12 litres/s (4.4-4.5 barrels/min). A total of 175 000 litres were injected before the well was shut in for a 48 hour pressure fall-off test. At the end of the test, a temporary bridge plug was set at 2 228 m.
Results and economics
Calculations from the 48-hour pressure fall-off test using the final surface pressure revealed a permeability of 12 md, or about 12 mm/day. This value of permeability was too low to achieve the desired injection rate of 44 litres/s. Based on 12 md, the maximum injection rate would be approximately 20% of the proposed rate, or 8.8 litres/s at a pumping pressure of 6.21 MPa. The cost of injecting drainage water was estimated at over $US 810 per 1 000 m^3. An acceptable minimum permeability for recharge on this project would be from 50 to 60 md.
Abandonment procedures
The outlined procedures for plugging and abandoning an injection well can be found in Section 146.10 of 40 CFR (Plugging and Abandonment Class I-III Wells). This regulation states that 'the well shall be plugged with cement in a manner which will not allow the movement of fluids either into or between underground sources of drinking water'. In the United States, all abandonment procedures must be witnessed by an authorized representative of the Environmental Protection Agency (40 CFR, Section 146.10). The district's prototype deep well was abandoned in July 1993. Four cement plugs were used in closing the well. The cement plugs were placed at the top of the lower and uppermost injection zones, at the base of the freshwater sands, and 1.5 m below the normal ground surface.
<section>b</section>
Chapter 7 - Health issues related to drainage water management
Martin S. Fritsch
Swiss Federal Institute of Technology, Zurich, Switzerland
The interactions between drainage, water management and health
Water related diseases and their vectors
Water-borne excreta related infections
Health risks and chemical pollution
Integrated control of transmission of vector-borne diseases
Environmental management measures in drainage water management
Development of control strategies
The interactions between drainage, water management and health
Proper surface and subsurface drainage to remove excess water in a safe and timely manner plays an important role in controlling water related diseases. Careful control and appropriate reuse of drainage water can help protect the environment and optimize the use of water resources.
The health issues related to drainage water management can be grouped in three categories:
i. water related vector-borne diseases;
ii. faecal/orally transmitted diseases; and
iii. chronic health issues related to exposure to residues of agrochemicals.
In tropical and subtropical regions there is a close link between the presence of excess water (due to lack of adequate drainage) and the transmission of water related vector-borne diseases. Malaria, schistosomiasis (bilharziasis) and lymphatic filariasis are important water related vector-borne diseases. Despite control programmes, health services and available treatments, these diseases today represent a growing health problem.
Water related vector-borne diseases are caused by bacteria, viruses and parasites (protozoa and helminths) transmitted by water related disease transmitting agents, also called vectors or intermediate hosts. A vector is an animal, often an insect, that transmits an infection from one person to another person or from infected animals to humans (Cairncross and Feachem, 1983). Most infections can only be transmitted by a particular, disease-specific vector, e.g., malaria by Anopheles mosquitoes. An intermediate host has a similar role to a vector. However, such an organism does not actively transmit a pathogen, like freshwater snails in the case of schistosomiasis. Vectors and intermediate hosts represent critical elements in various disease transmission cycles of parasitic water related diseases. In general, they live in or near aquatic environments.
Direct pathogen transfer and the transmission by vectors and intermediate hosts require specific environmental and socio-economic conditions. The conditions are defined by:
i. quality and quantity of water;
ii. type and frequency of human-water contacts;
iii. number and distribution of vector or intermediate host breeding sites; and
iv. exposure of humans to vector and intermediate host populations.
Consequently, the above-mentioned diseases can also be associated either directly or indirectly with the design and management of treatment and disposal plants for the re-use, treatment or disposal of drainage water. The key criteria for such a health risk are:
i. introduction of temporary or permanent open water surfaces bodies, e.g., constructed wetland, stabilization ponds, or evaporation ponds;
ii. suitability of such water for vector breeding;
iii. accessibility for the local population;
iv. location in relation to human settlements and transport links (e.g., roads); and
v. pollution by organic or inorganic substances.
Misuse and lack of maintenance are the two main reasons why drainage structures (road drainage ditches, culverts, dam site drainage or drainage canals in irrigation schemes, and also drainage water treatment and disposal facilities) are often associated with environmental health problems. Farmers, associations or national agencies generally conduct regular maintenance on irrigation canals. Water quality and flow velocity are relatively high. However, in drainage facilities the opposite conditions are frequent. Silting, uncontrolled aquatic weed growth, slow water flow or stagnant pools associated with the resulting wetlands offer ideal breeding conditions for mosquitoes and aquatic snails. Farmers seem to concentrate on irrigation water management rather than on drainage management.
Moreover, there is often a lack of adequate domestic water supplies and sanitation facilities. Thus, drainage canals or drainage water treatment and disposal facilities are often misused for washing, drinking and uncontrolled disposal of human excreta or other waste by the poorest and, thus, most vulnerable social groups. In this way, drainage water contributes to disease transmission.
Water related diseases and their vectors
Incidence of diseases - cases and mortality
Vector-borne diseases: transmission by insects
Water-based diseases: transmission by aquatic and semi-aquatic snails
Incidence of diseases - cases and mortality
Although the eradication of diseases such as malaria has long been an objective, the problem is far from being solved. According to the most recent information (WHO, 1997) the malaria situation is serious and deteriorating. Global malaria mortality is estimated at 1.5-2.7 million and global malaria cases at 300-500 million. Malaria is one of the most serious health problems facing African countries and a major limitation on their socio-economic progress. Children under the age of five and pregnant women are most at risk. WHO (1997) reports that 90 percent of the global burden of this disease can be attributed to environmental factors, including land and water management.
Schistosomiasis is almost as widespread as malaria but rarely causes immediate death. An estimated 200 million people are infected and transmission occurs in about 74 countries. The infection is primarily common in children who play in water inhabited by the snail intermediate host. Water development projects, especially those associated with the irrigation of large areas, have often been associated with an increased incidence of schistosomiasis. Intestinal schistosomiasis was unknown or infrequent in the Nile, Senegal and Volta deltas before the construction of the Aswan, Diama and Alosombo dams.
Worldwide parasitic and infectious communicable diseases (including all water related diseases) caused 32% of all deaths in 1993, resulting in the loss of 16.5 million lives; 99% of reported cases occurred in developing countries. In these countries, parasitic and infectious communicable diseases cause 41.5% of all deaths (WHO, 1995).
Vector-borne diseases: transmission by insects
Insect vectors represent the largest group of disease transmitting agents. In most cases and for the most widespread diseases, mosquitoes are the main vectors. Among a wide range of vector-borne diseases, two diseases, namely, malaria and lymphatic filariasis stand out as serious health hazards in the context of poor drainage.
Malaria
Malaria is caused by a protozoan parasite of the genus Plasmodium. Malaria is a complex disease causing fever, anaemia and an enlargement of the spleen. This causes additional cerebral complications, especially for children. Correspondingly, child mortality rates for P. falciparum, one of the four types of malaria affecting humans, are very high with approximately one million children below the age of five dying in 1993 (WHO, 1995; WHO, 1996).
Malaria is transmitted by the bite of a mosquito of the genus Anopheles. The transmission cycle is only between man and mosquitoes. Man acts as the intermediate host or reservoir and the mosquitoes as the vector. Protozoan parasites of the genus Plasmodium have to undergo complex development and multiplication processes both in man and mosquito before they can be further transmitted. Only the female mosquitoes are of importance for transmission, as they need a blood meal for oviposition.
Malaria covers not only all developing countries, but is present on almost the entire land surface between the latitudes 40°N and 60°S. However, the distribution is not uniform and depends mainly on climate, altitude, population density and the specific environmental requirements of the mosquitoes species.
Highly endemic areas are sub-Saharan Africa, Central America and the northern part of South America, the Indian subcontinent and Southeast Asia.
In the last two decades, a growing number of malaria cases have been observed (WHO, 1996). However, this cannot only be explained by the increasing population. To a large extent, this is also due to the increasing number of WRDPs such as irrigation and drainage schemes or hydro-electric dams. With the introduction of new open water surfaces in the form of canals, ponds and artificial lakes, new mosquito breeding sites have been created.
However, the persistence of the disease is also due to the absence of effective long-lasting vaccines, and the growing resistance of malaria pathogens and mosquitoes to treatment and insecticides, respectively. By the end of 1985, 50 of the 150 potential malaria transmitting Anopheles species were already recorded to be resistant to one or more pesticides (including DDT). At least 11 of those 50 species are known to be important and dangerous malaria transmitters (WHO, 1989).
Furthermore, malaria transmission is not only related to WRDPs. Deforestation, mining, road construction and all the negative consequences of rapid and uncontrolled urbanization are also contributing to the creation of mosquito breeding sites. In this context, urban drainage plays two key roles. On the one hand, it is an essential and effective tool for reducing and eliminating mosquito breeding sites by controlling surface water and waterlogging and by eliminating unnecessary open water surfaces. However, on the other hand poorly maintained drainage canals can represent potential breeding sites for various mosquito species if they are permanently flooded and aquatic weeds are not cleared.
Finally, a fresh risk might result from climatic change. First reports suggest that global warming can change the geographical distribution of mosquito breeding and shift the malaria transmission border line to the north (WHO, 1996).
Lymphatic filariasis
Mosquito-borne filariasis includes a group of diseases which lead to inflammations and obstructive lesions of the lymphatic system. Filarial parasites are nematode worms, which also need an insect to complete their life cycle. The main disease is Wuchereria bancrofti or elephantiasis and is transmitted by mosquitoes of the genera Culex. During a bite, a mosquito might take up a number of microfilariae which then undergo a development cycle in the mosquito. When the infected mosquito bites again, the infectious larvae enter first the skin and then the lymph vessels and lymph glands. Unlike malaria, filarial infection requires repeated and lengthy exposure to the vector due to the low pathogen load per bite. For W. bancrofti, long exposure to repeated infection will finally lead to severe and disfiguring deformities of the legs, arms and genitals. Lymphatic filariasis affected about 120 million people in 1994; 80% of cases were reported either from tropical Africa or India
(WHO, 1996).
Although filariasis is more of a serious urban health problem, it can cause significant health hazard in rural settlements and farming communities where poor surface drainage, lack of sanitation facilities and environmental pollution through uncontrolled waste and excreta disposal are prevalent. The principal vector, Culex quinquefasciatus (fatigans), breeds in polluted water in ditches, drains, tanks, barrels, tins and any kind of water accumulation container.
Mosquito bionomics
There are more than 3 000 known mosquito species. However, of the 150 species that are potential vectors, only 30 are considered dangerous. The three genera Anopheles, Aedes and Culex, from three sub-families, are disease relevant.
The epidemiology and the life cycle stages demonstrate the importance of climatic factors and that water is the essential environmental component for mosquitoes. The immature stages such as eggs, larvae and pupae require an aquatic environment, whereas adult mosquitoes live in terrestrial ecosystems. The quality and quantity of water, whether it is running or standing, shallow or deep, clean or polluted, sweet or brackish, shaded or sunlit, permanent or seasonal, and finally the climate will determine which particular species can breed. A summary of the physical and biological factors is given below (WHO, 1982).
Physical factors: In general mosquitoes prefer higher air humidity and average water temperatures between 23 °C and 33°C in order to complete their aquatic stages within two weeks. Rainfall can be a limiting as well as a positive factor. Rain will fill ditches, rivers, ponds, etc., but heavy rain can have a flooding effect and flush out breeding places. Mosquitoes do not normally reproduce where excess water is quickly removed. Sunlight or shade can also be positive or negative depending on the species. All these factors also determine the resting and biting habits. Some species rest and feed indoors and/or outdoors. Peak biting activity is usually about one hour before dawn. However, Aedes is a daytime biter, and many Anopheles bite throughout the night.
Biological factors: The presence of vegetation and floating plants are important for optimal breeding conditions. First, the plants are larval food and, more importantly, they provide shelter from predators and protection against wave movement. Therefore, mosquito larvae are not found on the open surfaces of large water bodies. The abundance of a number of species is linked to the presence of specific plants.
As a group, mosquitoes breed in an almost infinite variety of sizes, types and qualities of water bodies and each species requires specific environmental breeding and living conditions. However, most of the mosquito vectors breed in a rather restricted and narrow range of habitats. It is the number of potential transmitting species and their population dynamics which makes control efforts difficult. A comprehensive classification by species, country and habitat, including potential environmental management measures, is presented in the Manual on Environmental Management for Mosquito Control (WHO, 1982).
Natural biological control of mosquitoes can be accomplished with larvivorous fish. Over 250 different species of such fish are known to consume mosquito larvae. The most common mosquito fish is Gambusia affinis which is found in about 60 countries (Gerberich and Laird, 1985). Mosquito fish can be propagated commercially and introduced into drainage water. The fish are most effective during the warmer months of the year.
Water-based diseases: transmission by aquatic and semi-aquatic snails
Schistosomiasis
There are three major types of schistosomiasis that affect man: S. haematobium, S. mansoni and 5. japonicum. The disease is caused by female and male trematode worms inhabiting the blood vessels of the urinary bladder (S. haematobium, S. japonicum) or the portal and mesenteric veins (S. mansoni). Typical visible symptoms of schistosomiasis infection are blood in the urine in the case of S. haematobium, and intermittent diarrhoea and faeces containing blood in the case of S. mansoni (Jordan and Webbe, 1982). Although effective chemotherapy is available, the costs per treatment and per caput make it too expensive for many developing countries.
The transmission of all three species is based on a complex four-phase cycle which includes the presence of freshwater snails (Figure 9). Eggs expelled in the urine or faeces of an infected person may reach water, where they hatch rapidly into miracidia, a free-swimming brief larval stage. Depending on the species, the miracidia have to find a specific aquatic or semi-aquatic freshwater snail as intermediate host within 24 hours.
S. haematobium, S. mansoni and S. japonicum require snails of the genera Bulinus, Biomphalaria and Oncomelania, respectively. Oncomelania are semi-aquatic snails that live part of the time outside water in moist soil or mud. In the snails, the parasites develop within 4 to 6 weeks into cercariae, a second free-swimming larval stage. It is at this stage that they are infectious for man. Snails shed numerous cercariae into the water. These have to find human beings in contact with water in order to penetrate their skin. Having penetrated the skin of the host, the parasites find their way through the veins, heart and lungs to the final organ, there recommencing the cycle with egg production (Jordan and Webbe, 1982).
Bionomics of snail intermediate hosts
Although individual snail species require a specific physical environment, the variety of aquatic habitats is almost infinite. Marshes and swamps, permanent or temporary ponds and pools, natural or man-made freshwater lakes or reservoirs, seasonal or permanent or slow flowing river streams, irrigation or drainage canals, rice fields and all other types of standing, slow flowing or impounded water are potential snail breeding sites. Generally, the water needs to be shallow, clean or brackish, with little turbidity. The duration of the life cycle depends on water temperatures. The ideal water temperature ranges from 26° C to 28 °C.
Snail intermediate hosts of both S. haematobium and S. mansoni show great tolerance regarding pH values (5.3-9), mean water temperatures (18°-30°C) and salinity. Snails are found in shaded water bodies but are also known to be active when exposed to direct sunlight. The shedding of cercariae correlates with day-time and thus the intensity of sunlight. Furthermore, snails have a remarkable capability to survive long periods (5-8 months) in moist sand or mud (aestivation). Their main source of food is organic matter originating from decaying submerged or emerged vegetation, different species of algae, bacteria and fungi.
However, both of these aquatic snail species are sensitive to water velocity and water table fluctuations. Tolerable average current speed ranges between 0.0-0.3 m/s (Jobin and Ippen, 1964; Jobin et al., 1984). In natural rivers, snails are generally dislodged due to turbulence and fleeting shear stresses along the transition zone between the embankment vegetation and the river bed sediments, which are their actual habitat zone (Fritsch, 1993).
FIGURE 9 Transmission cycle of schistosomiasis
Water-borne excreta related infections
Most of the aquatic vector or intermediate host habitats correspond to sites where humans access water for washing, bathing, swimming (mainly children) or fishing.
Cairncross and Feachem (1983) proposed a classification of water-borne diseases in order to more fully understand the effects of excreta disposal:
i. Faecal-oral diseases (non-bacterial): Infections transmitted from person to person due to domestic contamination and lack of personal hygiene. Examples: hepatitis A, giardiasis.
ii. Faecal-oral (bacterial): Person-to-person transmission with longer transmission cycles either through contaminated food, crops or water contaminated with faecal material. Examples: Various diarrhoeas and dysenteries like cholera, E. coli diarrhoea or salmonellosis, enteric fevers such as typhoid.
iii. Soil transmitted helminths: Eggs of parasitic worms are expelled in faeces and require a development stage in moist soils. They reach the human host either by being ingested on vegetables or by penetrating the soles of the feet. Transmission takes place in communal defecation areas or around dirty latrines without clean concrete floors. Examples: ascariasis, trichuriasis, hookworm.
iv. Beef and pork tapeworms: Transmission cycle includes an intermediate development stage in an animal and infection of man occurs when the meat is eaten without sufficient cooking. Transmission can be triggered through the application of sewage sludge as fertilizer on grazing land. Examples: taeniasis.
v. Water-based helminths: The most typical example is that presented above in the section on schistosomiasis. As indicated, eggs in faeces must reach water in order to undergo the next development stage in an aquatic snail. The guinea worm (fasciolopsiasis) follows a similar cycle.
vi. Excreta related insect vectors: Filariasis transmitting Culex mosquitoes prefer to breed in highly polluted water. Badly maintained, unventilated latrines or uncovered septic tanks offer the best breeding conditions, mainly in urban areas. However, filariasis may also reach man simply by being carried by flies or cockroaches.
An important characteristic is the persistence of a specific pathogen, i.e., its ability to survive in the environment and whether animals, either in series or parallel, are part of the transmission cycle in view of the above classification, water quality and the environmental conditions around water bodies can be seen to play an essential role in the transmission.
Thus, unlined open drains, stabilization and evaporation ponds or wetlands might become particularly dangerous transmission sites. It is at such sites that excreta disposal, animal feeding and human water contacts might occur in a concentrated and uncontrolled form, so leading to the necessary pathogen-man/animal contacts.
Health risks and chemical pollution
Health risks related to water quality, primarily due to agrochemicals, represent a growing problem. This is mainly due to the intensified production and the expansion of irrigated agriculture, often in association with food processing industries (FAO, 1993; Hespanhol, 1996). In this context, both subsurface and open drains can have an impact on human health on four levels:
i. Drainage water re-use (see Chapter 3). Leaching or surface runoff from fields treated with pesticides, e.g., herbicides, fungicides, insecticides or molluscicides (Cairncross and Feachem, 1983), represents a non-point source of toxic organic substances. If re-used as irrigation water, toxic substances can be ingested by individuals directly or indirectly in the case of drainage water being mixed with drinking water resources. Another source of direct water contamination with agrochemicals is the washing of spraying equipment in open drains or irrigation canals or the spraying of irrigation canals and reservoirs against aquatic weeds (Hespanhol, 1996). Leaching also includes the transport of inorganic substances like salts, nitrate, phosphorus or heavy metals. Such substances can lead to health problems if consumed in high enough doses (FAO, 1993). However, salt will be less of a health problem as a salty taste will prohibit individuals from drinking too much salt
polluted water. Trace elements are more of a concern in the case of subsurface drainage water.
ii. Drainage water treatment and disposal. As outlined in Chapters 5 and 6, both lead to an accumulation, a transfer or a transition of pollutants. In addition, treatment facilities such as constructed wetlands or stabilization ponds introduce new and permanent open water surfaces, and aquatic weed growth. Here, the question is whether this will change the human-water contact patterns and create new vector breeding sites. Furthermore, the quality of inadequately treated drainage water might also represent a potential health risk.
iii. The application of insecticides or molluscicides during chemical vector control campaigns. This can lead to short-term high concentrations of toxic chemicals in irrigation or drainage canals. Unprotected workers are exposed to health risks and local populations might consume polluted water without being aware of the risks. Indirect contamination might occur through the ingestion of fish.
iv. Uncontrolled disposal of industrial liquid waste in drainage canals or treatment and disposal systems. Due to the lack of environmental regulations and proper treatment plants, especially in urban areas, drains often serve as liquid waste disposal sites for local industries, thus representing dangerous point sources of toxic substances.
Although health risks arising from chemical pollution are accepted as a major environmental concern, data on medical implications are difficult to obtain and often unavailable for developing countries. Due to the lack of environmental protection and regulation standards as well as the lack of environmental monitoring data, governmental services often neglect the issue of pollution control.
Integrated control of transmission of vector-borne diseases
Components of integrated control approaches
Environmental management for vector control
Components of integrated control approaches
Control of water related diseases always requires the interruption of transmission cycles, which include a number of quite different actors: man, animals, or vectors. These all have different functions, react differently to environmental changes and have different capacities in either transmitting a disease or in resisting infection. There are basically three ways to disrupt transmission:
i. by eliminating or reducing vector densities;
ii. by protecting the susceptible host through immunization, prophylactic drugs, reduced man-vector contacts using repellents, window screening, bed nets, or by installing adequate water supplies and sanitation facilities; and
iii. by reducing the reservoir of infection, by treating infected people or animals and by eliminating the pathogen in the host body.
FIGURE 10 Elements for integrated control of schistosomiasis
Therefore, an effective and integrated disease and vector control strategy has to integrate four basic elements (adapted and extended after WHO, 1988; Hespanhol 1996): chemical vector control, biological vector control, environmental management, and medical treatment. Figure 10 presents an example of an integrated approach to controlling schistosomiasis.
Medical treatment is still considered as most important. In fact most research and financial resources are focused on medical or bio-medical issues. However, it is clear from the magnitude of the present health problems, the increasing number and spread of diseases and the close environmental interactions set forth above, that any intervention can only be successful when all four elements are integrated. This requires all parties involved to have a sound understanding of all environmental, social, economic and bio-medical implications of the epidemiological dynamics of water related disease transmission.
The core of such an approach can be reduced to the formula: transmission and morbidity control. Transmission control combines chemical, biological and environmental management control of vectors, intermediate hosts, or any activity in the field of water supply, sanitation facilities and water treatment. Morbidity control includes chemotherapy or any medical control of the pathogen and the treatment of the disease. Oomen et al. (1990) visualize the environmental management component as a vehicle having hygiene education and community participation as wheels rolling on the road of basic infrastructure.
Environmental management for vector control
In 1979, WHO, FAO and UNEP established the Panel of Experts for Environmental Management (PEEM). The aim was to better introduce and develop the environmental management component in the context of vector and disease control. Attached to the panel are 13 collaborating centres. These are research institutes working in the field of either bio-medical or engineering related topics of tropical disease control. PEEM defines environmental management as all technical and managerial interventions which modify and/or manipulate environmental factors and their interaction with man and vectors. The overall objectives are to prevent or minimize vector propagation and to reduce man-vector-pathogen contacts in order to achieve an optimal health status for a target population (WHO, 1982; Bos et al., 1993). In 1979, the WHO Expert Committee on Vector Biology and Control defined three categories of environmental management for vector control as shown in Table 8.
TABLE 8 WHO definition of environmental management
Definition
Examples
Environmental modification
Long-lasting or permanent transformation of land, water and vegetation to prevent, reduce or eliminate vector or intermediate host breeding habitats (water related, vector-borne diseases) or environmental conditions which favour water-borne and water-washed disease transmission.
Grading, filling, drainage, land levelling, housing, urban drainage.
Environmental manipulation
Changes of environmental conditions to create temporary unfavourable breeding conditions for vector breeding or transmission.
Water level fluctuations, water velocity changes, flushings, weed clearing, salinity changes.
Modification or manipulation of human habitation or behaviour
Any environmental manipulation of modification measures to reduce man-vector and/or man-pathogen contacts.
Bed nets, personal protection, house screening, safe bathing and laundry places, latrines, wastewater treatment, water supply.
Chemical vector control would include the application of repellents, attractants, insecticides, molluscicides and chemosterilants. Biological vector control can be achieved by the release of predators and microbial insecticides and by genetic manipulation. Finally, the medical interventions would focus on morbidity control, such as chemotherapy, vaccines, general treatment, improved nutrition, preventive health safeguards, prophylaxis and health education.
Environmental management measures in drainage water management
Drainage water treatment, re-use and disposal
Environmental management measures applied to drainage structures
In the context of drainage water re-use, treatment and disposal, the potential health risks can be analysed within a simplified system of drainage water flow in combination with the transfer of pathogens, toxic substances and the occurrence of potential vector breeding sites. This allows one to differentiate whether a health risk is related to water quality, vector breeding or both.
For each of the three drainage water management options, a number of questions are formulated to identify potential health risks. These are followed by a range of recommendations for possible control and preventive measures.
Drainage water treatment, re-use and disposal
Direct re-use
At first glance, re-use may appear to be a water quality problem mainly regarding high salt concentrations which can affect plant growth. However, pesticides in different forms can be washed into surface drains and trace elements can be leached out by subsurface drains. Where drainage water with high loads of pesticides and trace elements is re-used for irrigation, a health risk can occur for irrigation workers or for persons using this water for domestic purposes.
Questions:
i. What is the quality of the drainage water? Is the drainage water used directly or blended with high quality water? What are the maximum loads of toxic substances after the blending? Are there seasonal changes in concentration according to agricultural management schedules?
ii. How do individuals or social groups come into contact with irrigation water blended with re-used drainage water? Do people use this irrigation water for any other purposes such as drinking, washing or cooking? Are children in frequent contact with the water?
iii. Are there any other risks? Can polluted water enter other hydrological cycles with high quality freshwater (e.g., groundwater), which will then be used for drinking?
Control and prevention:
In the case of direct re-use, there are few opportunities for the application of environmental management measures. Once drainage water has entered an irrigation system, it is probable that individuals will come into contact with this water. Therefore, effective monitoring of the quality standards of drainage effluents is most important. It is necessary to develop safeguard strategies in case of unacceptable or dangerous contamination levels.
Agriculture-forestry system and solar evaporators
This system of drainage water management, which aims at a continuous concentration of salt in progressively smaller volumes of water, is less of a health risk, as saline water is unsuitable for drinking. Here, the question is whether this series of irrigation systems will create new open water surfaces for vector breeding.
Questions:
i. Does the system create new and permanent open water surfaces (e.g., solar evaporator)?
ii. If so, can they serve as breeding sites for mosquitoes? Do mosquitoes already breed in the area and to what extent are mosquito transmitted diseases prevalent? Are there mosquito species involved which can breed in brackish water (e.g., some Aedes, Culex and Anopheles species)? Can water snails tolerate the salt concentrations?
iii. Can aquatic weeds grow?
iv. How near are settlements, roads or larger urban centres? What diseases are prevalent there?
Control and prevention:
i. Intermittent drying out of ponds or storage tanks for at least 5-7 days.
ii. If possible, lining of all facilities to avoid seepage and minimize aquatic growth. In the case of earth lining: regular weed control. Only restricted or no applications of insecticides or molluscicides.
iii. Restricted accessibility in order to reduce man-water contacts.
iv. Settlement planning: geographical separation of settlements from ponds and tanks.
v. Monitoring of vector breeding activities and water quality.
vi. Evaluation of options.
Constructed wetlands
The various physical, chemical and biological treatment processes may require a number of water retention structures such as constructed wetlands. This could lead to new and mostly permanent open water surfaces. Here again, the question is whether vector breeding sites will be created, or whether the purification capacity will determine the effluent quality and thus the water quality for low-end consumers.
Questions:
i. How many new and permanent water bodies will be created? How large is the area?
ii. What kind of vegetation (e.g., aquatic weed) will grow?
iii. Is the local population already exposed to water related diseases? Where are the transmission foci located? What vector or intermediate host species are involved?
iv. Do their breeding requirements correspond to the environmental conditions created by a constructed wetland (e.g., for mosquitoes)?
v. How near are settlements, urban centres and roads? Are there any migration movements in the areas?
vi. What is the quality and performance of the local health service? What is the perception of the population regarding environmental health issues?
vii. Are there any data on prevalence, incidence, vector population dynamics or breeding habitats?
Control and prevention:
i. Water level fluctuations and intermittent drying out of the wetland area.
ii. Restricted accessibility.
iii. Geographical location outside and separated from human settlements.
iv. Off site: settlement planning and maintenance, housing improvement, personal protection.
v. Health education.
vi. Monitoring: vector populations, water quality, case reporting.
vii. In the case of high community vulnerability and environmental receptivity: evaluation of options.
Environmental management measures applied to drainage structures
Drainage systems need to be connected either with subsurface or surface drainage canal outlets. In the case of surface collector canals, additional open water surfaces will be introduced. In warm and hot climatic zones, drainage canals include a number of typical features which favour vector breeding, disease transmission and direct pathogen propagation, such as low and irregular flow velocities, low embankment slopes, high seepage, uncontrolled water access, uncontrolled deposition of excreta and aquatic weed growth.
Environmental modification
Modifications to the drainage system environment would include: the change from open to piped or covered drains; canal lining with concrete in order to increase flow velocities and reduce aquatic weed growth; installation of special structures for cattle crossings and drinking; and boat ramps to protect earth embankments.
Environmental manipulation
The key elements of environmental manipulation would be flow and water level management measures. Periodical flushing will help to dislodge snails and mosquito larvae if drag forces or shear stresses due to higher velocities exceed certain limits (Jobin, 1987; Oomen et al., 1990; Fritsch, 1993). Water level fluctuations can have distinct control impacts on both snail and mosquito breeding. If the drop is fast enough, snails, larvae and eggs become stranded and die (Fritsch, 1993). Intermittent flow, drying out of canals in connection with flushing, and water table fluctuations can also be effective tools to control mosquito or snail breeding. However, the approach differs for snails and mosquitoes, according to the locally specific population dynamic and vector bionomics (Oomen et al., 1990). Finally, canal maintenance means weed control and the removal of sediments. Weed control can be done either mechanically, chemically by applying herbicides, or biologically with fish.
Modification and manipulation of human habitation or behaviour
Interventions and environmental changes related to human habitation might not be sufficient if canals are continuously misused for excreta and waste disposal. In this case, health risks have to be minimized with a set of non-drainage related measures. This includes the improvement of sanitation facilities and personal hygiene (e.g., water supply systems or latrines) and the planning and maintenance of settlements.
Development of control strategies
There are no standard packages of engineering techniques available, nor should environmental management be considered as the ultimate solution for controlling water related parasitic diseases (Fritsch, 1993).
Birley (1995) has introduced a systematic approach to forecast vector-borne disease implications. In this assessment methodology he addresses three main components which contribute to the potential health hazards:
Community vulnerability: This describes the prevalence of specific diseases in social groups such as children, adults, males, females, workers or farmers. The prevalence is brought into relation with the proximity of disease foci, the immune status, previous history of exposure, general health status and the role of migrants. Vulnerability is ranked as high, moderate or low.
Environmental receptivity: This is the receptivity to transmission of the pathogen related to the abundance of the vector, to human contacts with water or vectors and to any other ecological or climatic factors favourable for transmission. The assessment is followed according to possible transmission but not to occurrence, transmission easily resumed, or to high receptivity.
Vigilance of the health services: This describes the quality and performance required of a health service to cope with an increased health hazard. The question is whether a health service is able to support and manage vaccination campaigns, continuous case detection, drug provision and delivery, hospitals, sufficient and skilled staff, health education and information or means for chemical or biological vector control. The ranking includes: very good, effective preventive measures only, effective treatment only, and none.
The assessment of control strategy effectiveness is organized in a sequence of flow charts and worksheets. It includes a comprehensive set of questions which finally lead to the ranking of the three components. The methodological approach forces one to focus on control elements and will structure data and help to set up additional survey or monitoring concepts if there is a lack of data. It will also help in assessing the quality and reliability of data. Overall the final outcome (the total assessment of potential health hazards) will provide a sound basis for identifying the required set of safeguards and preventive intervention measures, including the environmental engineering required to control water management related diseases.
<section>c</section>
Chapter 8 - Institutional arrangements
Ashok Subramanian
World Bank, Washington D.C., USA
Issues and problems
Institutional mechanisms
Issues and problems
Objectives and interest groups
Need for regulation, conservation and communication
Role of public and private agencies and water users
Objectives and interest groups
Drainage related environment management involves activities concerning the transport, treatment, re-use or disposal of lower quality water. These activities within one farm, land unit or basin can affect the availability, quality and use of water in other farms, land units or basins. Water management at the farm level is necessary to appropriately affect the level of drainage water quality and return flows. However, regulations governing water use and discharge practices, agreements among groups of users, and joint action are also necessary to:
i. address issues and problems of meeting agreed water quality standards;
ii. ensure the flow of collective benefits of water management in a basin leading to improved water quality, or minimizing water quality degradation;
iii. decide on ways of sharing joint costs such as those associated with the treatment of poor quality drainage water; and
iv. deal with externalities, for example, the effect of low volume or low quality drainage water on downstream uses, including degradation of natural plant habitats and loss of wetlands.
Several types of stakeholders are usually involved in drainage water management. These include: policy-makers at the federal and local levels who set and enforce standards; irrigation, drainage and water management agencies who participate in monitoring and complying with standards; individual farmers and farmer groups who manage water on a day-to-day basis; other downstream groups affected by the quantity and quality of drainage water; and recreational and environmental interests. Poor quality agricultural drainage water imposes costs on stakeholders due to impacts on:
i. human health, as, for example, in the case of health effects in the lower reaches of the Aral Sea watershed;
ii. agricultural productivity, as in the case of increasing salinity in the lower Indus River in Pakistan, in the tail-end of the irrigation systems in Egypt, and the lower Murray River in Australia; and
iii. ecology and aquatic resources, as in many lakes and estuaries in North America and Europe.
Need for regulation, conservation and communication
Drainage water discharge standards are needed to reduce negative impacts on human health, agriculture, fisheries and ecosystems. to meet such standards, drainage water management and environmental strategies are also needed, usually in the form of land and water conservation and pollution control measures. the objectives and interests of various stakeholders have to be taken into account in the formulation of such strategies. where these objectives and interests vary, it is necessary to clearly identify trade-offs and to agree on a package of acceptable and affordable measures that will support the positive impacts and mitigate the negative impacts. any use of water for any purpose lowers the quality of the remaining water, hence compromise between upstream and downstream users is indispensable.
Several operational difficulties may arise either in monitoring or with regard to follow-up action. It is, therefore, important to provide forums for communication between stakeholders in the planning and implementation of drainage water management strategies. The purpose of such communication is to:
i. set objectives for water quality and drainage water management in the basin;
ii. participate in a joint planning process;
iii. cooperate in monitoring the performance level in achieving standards; and
iv. negotiate action plans, where necessary.
Incentives for such communication and mutually beneficial agreements among stakeholders have to be identified. These incentives could be: the sharing of potential additional costs of mitigation; increased ability to influence policies and regulations; and early and local resolution of conflicts without recourse to litigation or governmental intervention.
Role of public and private agencies and water users
A wide range of public and private sector agencies are involved in drainage water management and they deal with issues of regulation, conservation and communication (NESPAK and Mott MacDonald, 1992). In developed countries, drainage water management is usually the responsibility of both the individual farmer and some type of collective organization, such as an irrigation or drainage district, water user group or a municipal-type authority. The collective organization may monitor the performance of on-farm drainage systems and ensure compliance with federal and local water quantity and quality regulations and standards. In much of the developing world, however, drainage development is at a low level and heavily dependent on government initiatives. Individual farmers are responsible for on-farm drainage, but enforcement of regulations is difficult due to a lack of formal water quality standards, the absence of organized user groups, or weak enforcement capacity on the part of
governmental institutions. It is, therefore, important to redefine public (both central and local) and private sector roles in water management and to strengthen technical and management capacities for setting, monitoring and enforcing workable standards, and for promoting water conservation measures. Such standards and measures are usually formulated within a framework of overall water and environmental policy at various levels in a country or region. For example, in France, drainage improvement plans are expected to be in line with the regional water management master plan proposed by the country's 1992 Water Law (Zimmer et al., 1996).
Institutional mechanisms
Laws and regulations
Corporate organizations
Participatory planning
Incentives for water quality enhancement
Monitoring
Institutional capacity
Laws and regulations
Federal, state and local laws and regulations on water quality and pollution control provide a framework for drainage water management in a basin. Regulations vary in scope, depending on the context of a specific locality, ecology, customs and traditions, country or state. Generally, they set appropriate standards for acceptable water quality levels. In Egypt, for example, water quality legislation and related regulations cover: protection of the Nile River and waterways from pollution; wastewater disposal from sewage systems; bathing and washing in streams; clearance of weeds and disposal of dead animals; industrial pollution; irrigation and drainage waters; and the licensing of groundwater wells. In some cases, regulations must also address the quantity of water used upstream, for example, in irrigation projects, to control downstream drainage flows. Regulations also specify measurement and compliance reporting requirements. Accordingly, those who manage drainage water
have to implement programmes to ensure monitoring of water quantity and quality at the project or basin level and take corrective action when warranted. In setting standards, the functional purpose for which water is used must be kept in mind so that affordable and sustainable water quality practices are adopted.
Corporate organizations
A number of organizational arrangements can be used to address water quality problems. The choice of the most appropriate arrangement for a country depends on local conditions and the prevailing laws and practices in the country. In the Netherlands, the water boards, which had their origins in the management of dikes, canals, watercourses and polders, have increasingly taken an active role in water quality management. This implies close attention to water and sediment pollution. There are over 100 water boards. The water purification boards are more specialized. A few municipalities also carry out quality management functions, but expect to turn these functions over to the boards (Schultz and de Vries, 1993). The boards play an important role in the implementation of the national Pollution of Surface Waters Act. To effectively discharge their responsibilities, they enjoy powers relating to water issues that are similar to those of municipalities. Local interest groups are
represented in their governing bodies. There is a tradition of stakeholder participation. Technical and administrative staff are recruited by the board depending on local needs. Decision making in the boards is open to public scrutiny. Activities related to public information and water related education are an important part of the boards' functions (Ministry of Transport and Public Works, 1991).
In the United States, federal, state, and sometimes, local governments set water quality standards. Generally, water districts may assume on behalf of the water users the responsibility for grass-roots implementation. For example, the California State Water Resources Control Board has determined that a 30% reduction in the volume of drainage water discharge in a particular region of the Central Valley of the state would significantly contribute to reducing the salt and Se load in drainage water flowing out of the farms in that region. Consequently, the water districts in the region have initiated an active programme of pricing and technology adoption to induce farm-level reductions in drainage discharge volumes (National Research Council, 1989; Dinar et al., 1994). Water districts are user based organizations with formal powers to frame operational rules, manage irrigation and drainage systems, and levy appropriate user fees. The districts' functions cover irrigation,
drainage and resource conservation. They provide for the representation of a number of interests in their governing bodies. As in the case of the water boards, technical and administrative staff are hired to carry out operation and maintenance (O&M), drainage and other functions.
The costs of setting up and operating water organizations with stakeholder participation can be high. The key rule is to allow for the demand-based evolution of self-financed user organizations that have substantial water management functions. An enabling policy climate is necessary in which such organizations and federations of such organizations can emerge and negotiate directly with one another and with governmental organizations. In the developing world, water management and drainage is handled by a combination of public sector irrigation and drainage agencies, pollution control boards and other public sector monitoring and research organizations. Their functions and roles will need to be restructured and adjusted as they come to play a greater role in planning and regulation and turn local water management functions over to user organizations.
Participatory planning
Participatory planning is an important process in solving some of the difficult problems (Le Moigne et al., 1994). Direct water users such as farmers, municipalities and hydropower companies are relatively easy to identify and include in the planning process. Specific provisions must be made to identify and invoke 'indirect' users, for example, fishermen interested in instream flows, recreational users and others that represent environmental interests. Groups of farmers at the tail-end of irrigation systems who may currently receive poor quality water are often left out of planning exercises. Special efforts to incorporate their needs are necessary. The absence of formal, registered water rights may often make it difficult to identify and include all stakeholders.
Opportunities for stakeholder participation are usually available through the environmental 'clearance' procedure. This procedure involves the preparation of environmental assessments. These assessments provide a basis for a relevant planning body to approve specific projects that represent all interests. Public hearings on plans and projects are frequently used to elicit public participation and promote the involvement of user groups, other stakeholders and interested neutral research institutions in the planning process. Public participation procedures in planning development projects are now formally prescribed in some countries.
Innovative approaches ('good practice' cases) that stress responsible and negotiated agreements between participants should be reviewed and adapted to local circumstances. The Murray Darling Basin Commission in Australia launched a Salinity and Drainage Strategy in 1988. This included an initial programme of salinity mitigation works to arrest Murray River salinity, and drainage works in the three participating states to protect agricultural land. Jointly funded by the three states, the objective was to reduce mean river salinity by 0.8 dS/m. Subsequently, each state was given a 'salinity credit'. This salinity credit permits them to undertake irrigation or drainage projects that could in turn increase river salinity according to the respective contribution of the states to the development works. The Commission oversees the implementation of the programme and maintains a register of all projects and their salinity effects. Its Salinity and Drainage Assessment Working Group
has the operational responsibility for programme monitoring. A salinity effect and cost model has been developed using a representative period (1975-1985) and this model provides the basis for studying impacts of new projects proposed in the basin (Department of Water Resources, 1990; Murray Darling Basin Ministerial Council, 1989).
Incentives for water quality enhancement
Incentives for water quality protection and enhancement relate to:
i. Adoption of water conserving crops, technologies and agricultural management practices. To reduce irrigation and drainage flows, water conserving crops and irrigation and drainage technologies could be promoted through support for field-based research and development (R&D). Water that is not 'saved' cannot be diverted to other consumptive uses with complementary downstream water quality problems. To foster a more rational use of chemicals, integrated pest management practices could be supported. The adoption of 'best' technology and management practices by farmers requires a careful assessment of private costs and benefits, and an efficient extension system.
ii. Water pricing. It is generally preferable to control drainage flows through incentives for conserving irrigation water rather than through restrictions on drainage discharge. In this regard, it is important to consider a mix of pricing and irrigation water delivery strategies that meet the requirements of farmers and which at the same time lead to a reduction in water use and drainage volumes (Wichelns and Cone, 1992). Flat rates per hectare do not favour water conservation, whereas block rates and volumetric charges promote more efficient water use. However, there could be political difficulties in the implementation of water pricing policies, especially in cases where substantial public subsidies are currently used to finance services. Thus, a careful assessment of costs, and public information and education on costs of water services and the consequences of subsidies are important elements of rational pricing strategies. The progressive participation of water users
in management functions is also a way to improve cost sharing. Mexico and Mali have recently shown that such user participation is possible and can be achieved within a reasonable time frame. An emerging area for study is the potential for using tradable pollution permits and the conditions necessary for their effective operation.
Monitoring
Water quality management information systems should be an integral part of drainage water management. These systems usually cover two aspects:
i. systematic collection and analysis of data based on a network of observation stations where samples are collected and analysed regularly; and
ii. investigations and research to address specific issues, including basin-wide modelling studies, that take an integrated view of economic, technical and institutional issues in water management.
The costs of monitoring and reporting must be built into project plans and budgets. In Egypt, for example, the Ministry of Public Works and Water Resources undertakes a detailed monitoring effort through the institutes affiliated with the National Water Research Centre. The River Nile Research Institute monitors water quality in the Nile River and its branches. The Groundwater Research Institute reviews groundwater potential and chemical pollution. The Drainage Research Institute (DRI) is responsible for monitoring drainage water quality in the Nile Delta. The DRI focuses on chemical composition, bacteriology, heavy metals, organic matters and nutrients. It collects periodic samples from open drains and pump stations in four regions of the country. Yearbooks on drainage water quality are published (DRI, 1995).
Institutional capacity
The functions of drainage organizations have expanded over time from building satisfactory drainage and flood control measures to include meeting established water quality standards and ensuring environmental protection. This transition from an agency exclusively involved in infrastructural development to one also engaged in water quality protection and negotiations with stakeholders is not an easy one. Communication between related governmental and nongovernmental organizations is necessary. For example, several organizations such as ministries of water, power, industry, agriculture, urban development, health, environment and education, in addition to the many private, community based and non-governmental organizations, are usually involved in drainage and environmental management issues. New skills, staff retraining, support for R&D and innovation in planning techniques are important.
Drainage and water quality management involves a host of new functions in most countries:
i. formulating an acceptable and affordable plan for sustainable drainage water management within the framework of the country/region's overall water resources and environmental policy and plans. Close coordination with resource management plans and environmental action plans is expected;
ii. mobilizing financial resources for preventive and mitigative actions, including R&D;
iii. ensuring public information and participation; and
iv. monitoring and reviewing environmental quality indicators.
These new functions place further burdens on already stretched budgets and an overextended water management bureaucracy. However, they are needed to address future challenges.
Water management agencies will need to divest themselves of some of their existing functions (e.g., management of lower levels of irrigation systems to water users in the developing countries) and begin to build partnerships with other resource institutions such as non-governmental organizations, commercial entities, R&D institutions and local governments and communities. The Comisión Nacional del Agua (CNA) in Mexico has been implementing a programme of redefining its functions since the mid-1980s. Previously, the CNA had been involved in all activities, from new source development to management of water down to the tertiary irrigation system level. However, over the past decade, it has transferred the function of management of distributaries and below to water user organizations. It has steered the formulation of a national water law and has initiated a major programme of registering tradable water rights (Gorriz et al., 1995). There has also been progress in other
countries. In Egypt, rising soil salinity has led to the increasing installation of subsurface drains, together with the recovery of capital costs of field drainage systems from farmers. The cost recovery programme is backed by appropriate legislation. The Egyptian Public Authority for Drainage has been managing a systematic programme of drainage expansion and rehabilitation in the country. To meet future challenges, the ministry has initiated a programme of restructuring water management by improving services and promoting user participation in management of the lower levels of the irrigation system.
Existing staff skills need to be upgraded and new expertise introduced in irrigation and drainage organizations. The upgrading needs to start by identifying the training needs of existing engineering staff in the light of the new challenges facing environmental management. Centres and field sites for formal and non-formal training need to be located. Some prerequisites which have proven useful are:
i. careful identification of 'best practice' sites to host the event; and
ii. formulation of a seminar design that combines classroom training and field visits.
New capabilities may have to be obtained through recruitment. Where budgetary pressures make such recruitment difficult, part-time consultancy inputs and secondments from academic and private sector organizations may be viable options. Human resources need to be developed now in order to effectively address the drainage and environmental issues of the twenty-first century.
Institutions, laws and regulations will require restructuring to meet water quality and environmental needs as scientific and engineering changes are made in the way water supplies are managed.
<section>d</section>
References
American Society of Agricultural Engineers. 1994. Design, installation and operation of water table management systems for sub-irrigation/controlled drainage in humid regions. ASAE Standards, 779-782, St. Joseph, MI: ASAE.
American Society of Civil Engineers. 1990. Agricultural salinity assessment and management. ASCE Manuals and Reports on Engineering Practice, 71, New York: ASCE, 619p.
Ayers, R.S. and Westcot, D.W. 1985. Water quality for agriculture. FAO Irrigation and Drainage Paper No. 29 (Rev. 1). Rome, Italy. 176 pp.
Bengtson, R.L., Carter, C.E., Morris, H.F. and Kowalczuk, J.G. 1984. Reducing water pollution with subsurface drainage. Trans. Am. Soc. Agric. Eng. 27: 80-83.
Bielorai, H.S., Dasberg, Y., Erner, Y. and Brum, M. 1988. The effect of saline irrigation water on Shamouti orange production. Proc. Intl. Citrus Cong. 6: 707-715.
Birley, M.H. 1995. The health impact of development projects. London: HMSO, 256pp.
Bos, M.G., Murray-Rust, D.H., Merrey, D.J., Johnson, H.G. and Sneller, W.B. 1993. Methodologies for assessing performance of irrigation and drainage management. Irrig. Drain. Syst. 7(4): 231-261.
Bowen, H.J.M. 1979. Chapter 3. The elemental geochemistry of rocks. Chapter 4. The elemental geochemistry of soils. p. 31-62. In: Environmental chemistry of the elements. London: Academic Press.
Cairncross, S. and Feachem, R.G. 1983. Environmental health engineering in the tropics: an introductory text. Chichester, U.K.: John Wiley and Sons Ltd.
California Department of Pesticide Regulation. 1994. Sampling for pesticide residues in California well water. Sacramento. CA: California Environmental Protection Agency. 151pp.
California Regional Water Quality Control Board. 1995. Staff Report. Water quality of the Lower San Joaquin River: Lander Avenue to Vernalis, October 1992 to September 1994. Sacramento, California: California Regional Water Quality Control Board.
Chilcott, J.E., Westcot, D.W., Werner, K. and Belden, K. 1988. Water quality survey of tile drainage discharges in the San Joaquin river basin. Sacramento, CA: Central Valley Regional Water Quality Control Board. 65pp.
Chilcott, J.E., Westcot, D.W., Toto, A.L. and Enos, C.A. 1990. Water quality in evaporation basins used for the disposal of agricultural subsurface drainage water in the San Joaquin Valley, California, 1988 and 1989. Sacramento, California: Central Valley Regional Water Quality Control Board. 48 pp.
Clark, R.B. 1982. Plant response to mineral element toxicity and deficiency. p. 71-142 In: Breeding plants for less favorable environments (Christiansen, M.N. and Lewis, C.F., eds.) New York: John Wiley and Sons, Inc.
Connor, V., Foe, C. and Dennovk, L. 1993. Sacramento river basin biotoxicity survey results 1988-1990, Sacramento, California: Central Valley Regional Water Quality Control Board.
Department of Water Resources, New South Wales. 1990. A guide to the salinity and drainage strategy of the Murray Darling Ministerial Council. Paramatta. Australia: Dept. of Water Resources, NSW.
Deverel, S.I. and Fujii, R.J. 1990. Chemistry of trace elements in soils and ground water. p. 64-90. In; Agricultural salinity assessment and management (K.K. Tanji, ed.). ASCE Manual No 71. New York: ASCE.
Deverel, S.J., Gilliom, R.J., Fujii, R., Izbicki, J.A. and Fields, J.C. 1984. Areal distribution of selenium and other inorganic constituents in shallow groundwater of the San Luis service area, San Joaquin Valley, California: a preliminary study. United States Geological Survey Water Resources Investigations Report 84-4319. USGS, Washington, D.C. 67pp.
Dhaliwal, J.S.I 992. Selenium removal by slow sand filtration and chemical adsorption. M.S. Project Report, Department of Civil Engineering, California State University, Fresno.
Di Giorgio, C., Bailey, H.C. and Hinton, D. 1995. Colorado river basin toxicity report. Davis, CA: University of California. 100 pp.
Dinar, A., Howitt, R.E. and Zilberman, D. 1994. Irrigated agriculture and environmental pollution: lessons from the westside San Joaquin Valley, California. Resources and Technology Division, Economic Research Service, USDA. Staff Report No. AGES 9427. Washington, D.C. USDA-ERS.
DRI (Drainage Research Institute). 1995. Re-use of drainage water in the Nile Delta: monitoring, modeling and analysis. Re-use Report No. 50. Cairo, Egypt: DRI.
EPOC AG. 1987. Removal of selenium from subsurface agricultural drainage by an anaerobic bacterial process - a final report on continued operation of the Murrieta Pilot Plant, report submitted to the California Department of Water Resources, November, 1987. Fresno, CA: EPOC AG.
Erie, L.J., French, O.F., Bucks, D.A. and Harris, K. 1982. Consumptive use of water by major crops in the southwestern United Sates. Conserv. Res. Rept. 29; USDAARS, 40 p. Washington, D.C.: USDA-ARS.
Evans, R.O., Gilliam, J.W. and Skaggs, R.W. 1991. Controlled drainage management guidelines for improving drainage water quality. Cooperative Extension Service, North Carolina State University Publication No. AG-443: 16pp.
Evans, R.O., Parsons, J.E., Stone, K. and Wells, W.B. 1992. Water table management on a watershed scale. J. Soil Water Conserv. 47: 58-64.
Evans, R.O., Skaggs, R.W. and Gilliam, J.W. 1995. Controlled versus conventional drainage effects on water quality. J. Irrig. Drain. Eng., 121: 1-6.
Evans, R.O. and Skaggs, R.W. 1989. Design guidelines for water table management systems on coastal plain soils. Applied Eng. Agric., 5(4): 539-548.
Evans, R.S. 1989. Saline water disposal. BMR Journal of Australian Geology and Geophysics 77:167-185.
FAO. 1993. Prevention of water pollution by agriculture and related activities. Water Reports No. 1. FAO, Rome.
FAO. 1995. Environmental impact assessment of irrigation and drainage projects. FAO Irrigation and Drainage Paper No. 53. FAO, Rome.
FAO. 1996. Control of water pollution from agriculture. FAO Irrigation and Drainage Paper No. 55. FAO, Rome.
Fausey, N.R., Doering, E.J. and Palmer, M.L. 1987. Purposes and benefits of drainage. p. 4851. In; Farm drainage in the United States: history, status and prospects. (G.A. Pavelis, ed.), ERS, USDA, Publ. No. 1455. Washington, D.C.: ERS, USDA.
Foe, C. and Connor, V. 1991. San Joaquin watershed bioassay results, 1988-1990. Sacramento, CA: Central Valley Regional Water Quality Control Board.
Ford, S.A. 1988. Agricultural drainage evaporation ponds in the San Joaquin Valley -progress of the investigation. Memorandum Report, California Department of Water Resources. Sacramento, California: California Department of Water Resources.
Fouss, J.L., Skaggs, R.W., Ayars, J.E. and Belcher, H.W. 1990. water table control and shallow groundwater utilization. p. 783-824. In; Management of farm irrigation systems (Hoffman, G.J., Howel, T.A. and Solomon, K.H., eds.). St. Joseph, MI: ASAE.
Francois, L.E. and dark, R.A. 1979a. Accumulation of sodium and chloride in leaves of sprinkler-irrigated grapes. J. Am. Soc. Hort. Sci. 104: 11-13.
Francois, L.E. and dark, R.A. 1979b. Boron tolerance of twenty-five ornamental shrub species. J. Am. Soc. Hort. Sci. 104: 319-322.
Francois, L.E. and dark, R.A. 1980. Salinity effects on yield and fruit quality of 'Valencia' orange. J. Am. Soc. Hort. Sci. 105: 199-202.
Francois, L.E. and Maas, E.V. 1978. Plant responses to salinity: an indexed bibliography. USDA, ARM-W-6, Washington, D.C.: USDA.
Francois, L.E. and Maas, E.V. 1985. Plant responses to salinity: a supplement to an indexed bibliography. USDA, ARS-24, Washington, D.C.: USDA
Francois, L.E. and Maas, E.V. 1994. Crop response and management on salt-affected soils. p. 149-181. In; Handbook of plant and crop stress (M. Pessarakli, ed.). New York: Marcel Dekker, Inc.
Fritsch, M.S. 1993. Environmental management for schistosomiasis control, river flushing. A case study in Namwawala, Kilombero District, Tanzania. Zurich: Verlag der Fachvereine.
Frost, B.1990. Atriplex tested as feed option - selenium transfer. Fresno, CA: California State University.
Gerberich, J.B. and Laird, M. 1985. Larvivorous fish in the biocontrol of mosquitoes, with a selected bibliography of recent literature. p. 47-76, In: Integrated mosquito control methodologies, Vol. 2. New York, NY: Academic Press.
Gilliam, J.W. 1987. Drainage water quality and the environment. p. 19-28. In: Drainage design and management, proceedings of the 5th National Drainage Symposium, Dec. 14-15, Chicago, IL. St. Joseph, MI: ASAE.
Gilliam, J.W. and Skaggs, R.W. 1986. Controlled agricultural drainage to maintain water quality. J. Irrig. Drain. Eng. 112: 254-263.
Gilliam, J.W., Skaggs, R.W. and Weed, S.B. 1979. Drainage control to reduce nitrate loss from agricultural fields. Journal of Environmental Quality 8:137-142.
Gorriz, C., Subramanian, A. and Simas, J. 1995. Irrigation management transfer in Mexico: process and progress. Technical Paper 292. Washington, D.C.: World Bank.
Gowing, J.W. and Wyseure, G.C.L. 1992. Dry drainage: a sustainable and cost-effective solution to water logging and salinization? p. 6.26-6.34. In; Proc. 5th International Drainage Workshop, Vol. III. Lahore, Pakistan: IWASRUICID.
Grattan, S.R. and Grieve, C.M. 1992. Mineral element acquisition and growth response of plants grown in saline environments. Agric. Ecosys. Environ. 38: 275-300.
Grismer, M.E., Karajeh, F. and Bouwer, H. 1993. Evaporation pond hydrology. p. 580-586, In; Proc., 1993 National Conference on Irrigation and Drainage Engineering (Alien, R.G. and Neale, C.M.U., eds.), American Society of Civil Engineers, Park City, Utah, July 21-23, 1993. New York: ASCE.
Harding, R.B., Miller, M.P. and Fireman, M. 1958. Absorption of salts by citrus leaves during sprinkling with water suitable for surface irrigation. Proc. Am. Soc. Hort. Sci. 71: 248-256.
Hespanhol, I. 1990. Guidelines and integrated measures for public health protection in agricultural reuse systems. J Water SRT- Aqua 39(4): 237-249.
Hespanhol, I. 1996. Health impacts of agricultural development. In: Sustainability of irrigated agriculture (Pereira, L.S. et al., eds.). Kluwer Academic Publishers.
Hillel, D. 1990. Ecological aspects of land drainage for salinity control in arid and semi-arid regions. p. 125-135. In; Symposium on land drainage for salinity control in arid and semi-arid regions, Vol. 1 (H.M. Amer, ed.). Cairo, Egypt: Nubar Printing House.
Hoffman, G.J., Schrale, G. and Ayars, J.E. 1990. Irrigated land requiring drainage at risk in California. p. 186-194. In; Symposium on land drainage for salinity control in arid and semi-arid regions. Vol. 3 (H.M. Amer, ed.). Cairo, Egypt: Nubar Printing House.
ICID. 1993. The ICID environmental checklist to identify environmental effects of irrigation, drainage, and flood control projects. Oxfordshire, U.K.: HR Wallingford.
ILRI. 1994. Drainage principles and practices, Chapter 25. In: Publication 16, 2nd eat., International Institute for Land Reclamation and Improvement, Wageningen, The Netherlands: ILRI.
Jacangelo, J., Laine, J., Cams, K., Cummings, E. and Mallevialle, J. 1991. Low-pressure membrane filtration for removing giardia and microbial indicators. Journal American Water Works Association 83(9): 97-106.
Jobin, W.R. 1987. Environmental management of disease vectors; Case studies on disease vector control through environmental management in water resource development project. Interregional travelling seminar on environmental management measures for disease vector control in water resource development project, USSRI India, November/December 1987, WHO Doc. No. VBC/TRV/SEM/ENV/VCT/87.9/1, Geneva: WHO.
Jobin, W.R. and Ippen, A.T. 1964. Ecological design of irrigation canals for snail control. Science, 145: 1324-1326.
Jobin, W.R., Laracuente, A., Mercado, R. and Negron-Aponte, H. 1984. Critical water velocities for snail habitats in canals. J. Env. Eng. 110: 279-282.
Johnston, W.R., Ittihadieh, F., Daum, R.M. and Pillsbury, A.F. 1965. Nitrogen and phosphorus in tile drainage effluent. Soil Sci. Soc. Am. Proc. 29: 287-289.
Jordan, P. and Webbe, G. 1982. Schistosomiasis, epidemiology, treatment and control. London: Heinemann Medical Books.
Jorgensen, G.S., Solomon, K.H. and Cervinka, V. 1993. Agroforestry systems for on-farm drain water management. Fresno, CA: California State University, Center for Irrigation Technology.
Jurinak, J.J. and Suarez, D.L. 1990. The chemistry of salt-affected soils and waters. p. 42-63. In; Agricultural salinity assessment and management (Tanji, K.K., ed). ASCE Manual No 71. New York: ASCE.
Kalita, P.K., Kanwar, R.S. and Melvin, S.W. 1992. Sub-irrigation and control drainage management tools for reducing environmental impacts of non-point source pollution. p. 129-136. In; Proc. 6th International Drainage Symposium. St. Joseph, MI: ASAE.
Khouri, N., Kalbermattern, J.M. and Bartone, C.R. 1994. Re-use of wastewater in agriculture: a guide for planners. Water and Sanitation Report 6, UNDP-World Bank Water and Sanitation Program. Washington, D.C.: World Bank, 49 pp.
Konyha, K.D., Skaggs, R.W. and Gilliam, J.W. 1992. Effects of drainage and water-management practices on hydrology. Journal of Irrigation and Drainage Engineering 118: 807-819.
Lalonde, V., Madramootoo, C.A., Trenholm, L. and Broughton, R.S. 1995. Effects of controlled drainage on nitrate concentrations in subsurface drain discharge. Agriculture Water Management, 29(2):187-199.
Lee, E.W. 1993. Treatment re-use and disposal of drainwaters. Journal of Irrigation and Drainage Engineering 119(3):501 -513.
Le Moigne, G., Subramanian, A., Xie, M. and Giltner, S. (eds.). 1994. A guide to the formulation of water resources strategy. Technical Paper 263. Washington, D.C.: World Bank.
Letey, J. 1993. Relationship between salinity and efficient water use. Irrig. Sci. 14: 7584.
Letey, J. and Dinar, A. 1986. Simulated crop-water production functions for several crops when irrigated with saline waters. Hilgardia 54: 1-32.
Luthin, J.N. (ed.). 1957. Drainage of agricultural lands. Madison, WI: Amer. Soc. of Agronomy, 620 pp.
Luttge, U. and Smith, J.A.C. 1984. Structural, biophysical, and biochemical aspects of the role of leaves in plant adaptation to salinity and water stress. p. 125-150. In; Salinity tolerance in plants: strategies for crop improvement (Staples, R.C. and Toenniessen, G.H., eds.). New York: John Wiley and Sons, Inc.
Maas, E.V. 1985. Crop tolerance to saline sprinkling waters. Plant Soil 89: 273-284. Maas, E.V. 1986. Salt tolerance of plants. Appl. Agric Res. 1: 12-26.
Maas, E.V. 1990. Crop salt tolerance. p. 262-304. In: Agricultural salinity and assessment management (K.K. Tanji, ed.). Amer. Soc. Civil Eng., Manuals and Reports on Engineering No. 71. New York: ASCE.
Maas, E.V. and Hoffman, G.J. 1977. Crop salt tolerance - current assessment. J. Irrig. Drain. Div. ASCE 103:115-134.
MacKenzie, A.J. and Viets, F.G., Jr. 1974. Nutrients and other chemicals in agricultural drainage waters. p. 489-508. In. Drainage for agriculture (J. van Schilfgaarde, ed.). Agronomy Monograph No 17. Madison, WI: Am. Soc. of Agron.
Macy, J.M., Lawson, S. and DeMoll-Decker, H. 1993. Bioremediation of selenium oxyanions in San Joaquin drainage water using Thauera selenatis in a biological reactor system. Applied Microbiology and Biotechnology, 40: 588-594.
Madramootoo, C.A. 1994. Controlled drainage systems for reducing nitrate pollution. Paper presented at the 22nd annual convention of the Corrugated Plastic Pipe Association, Ponte Vedra Beach, Florida, 9 p.
Madramootoo, C.A., Broughton, S.R. and Dodds, G.T. 1995. Watertable management for soybean production on a sandy loam soil. Can. Agric. Eng. 37(1): 1-7.
Madramootoo, C.A., Kaluli, W.J., Zhou, X., MacKenzie, A.F. and Smith, D.L. 1995. Water table management and cropping systems for environmental sustainability. ASAE Paper No. 95-2360, St. Joseph, MI: ASAE.
Madramootoo, C.A., Papadopoulos, A. and Dodds, G.T. 1993. Agronomic and environmental benefits of water table management. J. Irr. and Drain. Engineering, ASCE, 119(6): 1052-1065.
Madramootoo, C.A., Wiyo, K.A. and Enright, P. 1992. Nutrient loses through the tile drains from two potato fields. Applied engineering in agriculture 8(5):639-646.
Marshack, J.B. 1993. A compilation of water quality goals. Sacramento, California: Central Valley Regional Water Quality Control Board.
Metcalf and Eddy, Inc. 1991. Wastewater engineering: treatment, disposal, and re-use. 3rd Edition, New York: McGraw-Hill, Inc.
Michaelsen, J. 1995. Monitoring preferential leaching of herbicides at a tile drained field plot. In: Annual meeting on ground-water contamination research (Vingstedcentret 7.-8 March 1995). Copenhagen: Danish Academy of Sciences, 424 pp.
Micklin, P.P. 1991. The water management crisis in Soviet Central Asia. The Carl Beck Papers No. 905, Pittsburgh, PA: Center for Russian and East European Studies, University of Pittsburgh, PA.
Ministry of Transport and Public Works (The Netherlands) 1991. Water in the Netherlands: a time for action. National policy document on water management, September.
Montgomery, J.M., Consulting Engineers, Inc. 1985. Water treatment principles and design. New York: John Wiley and Sons, Inc.
Munster, C.L., Skaggs, R.W., Parsons, J.E., Evans, R.O., Gilliam, J.W. and Harmsen, E.W. 1995. Aldicarb transport in drained coastal plain soil. J. Irrig. Drain. Eng. 121(6): 378-384.
Murray-Darling Basin Ministerial Council. 1989. Salinity and drainage strategy.
National Research Council. 1989. Irrigation-induced water quality problems: what can be learned from the San Joaquin Valley experience. Washington, D.C.: National Academy Press.
NESPAK and Mott MacDonald. 1992. Pakistan: drainage sector environmental assessment. Vol.3, Supplementary reports.
Ochs, W.J. and Bishay, B.G. 1992. Drainage guidelines. World Bank Technical Paper No. 195. Washington, D.C.: World Bank.
Ohlendorf, H.M., Skorupa, J.P., Saiki, M.K. and Barnum, D.A. 1993. Food-chain transfer of trace elements to wildlife. p. 596-693. In. Proc., 1993 national conference on irrigation and drainage engineering (Alien, R.G. and Neale, C.M.U., eds.), American Society of Civil Engineers, Park City, Utah, July 21-23, 1993. New York, NY: ASCE.
Oomen, J.M.V., de Wolf, J. and Jobin, W.R. 1990. Health and irrigation. ILRI publication 45, Volume 1 and 2, Wageningen, The Netherlands: ILRI.
Owens, L.P., Kovac, K.C., Kipps, J.A.L. and Hayes, D.W.J. 1995. Biological reduction of soluble selenium in subsurface agricultural drainage water. p. 89-94. In: Bioremediation of inorganics, (R.E. Hinchee, ed.), Columbus, OH: Battelle Press.
Page, A.L., Chang, A.C. and Adriano, D.C. 1990. Deficiencies and toxicities of trace elements p. 138-160. In: Agricultural salinity assessment and management (K.K. Tanji, ed). ASCE Manual No 71. New York: ASCE.
Post, J.C. and Ochs, W.J. 1995. Flow through wetlands for controlling water quality in discharges from irrigated area. p. 83-91 In: The inter-relationship between irrigation, drainage and the environment in the Aral Sea Basin (M.S. Bos, ea.) NATO ASI Series 9: Environment - Vol. 22, Dordrecht, The Netherlands: Kluwer Academic Publishers.
Reeve, R.C., Pillsbury, A. F. and Wilcox, L. V. 1955. Reclamation of saline and high boron soils in the Coachella Valley of California. Hilgardia 24: 69-91.
Rhoades, J.D. 1974. Drainage for salinity control p. 433-462. In: Drainage for agriculture (J. van Schilfgaarde, ed.), Amer. Soc. Agron. Monograph 17. Madison, WI: Amer. Soc. Agron.
Rhoades, J.D. 1976. Measuring, mapping and monitoring field salinity and water table depths with soil resistance measurements. FAD-UN Soils Bulletin 31: 159-186.
Rhoades, J.D. 1987. Use of saline water for irrigation. Water Qual. Bull. 12: 14-20.
Rhoades, J.D. 1988. Evidence of the potential to use saline water for irrigation. p. 16-21. In: Int. seminar on the re-use of low quality water for irrigation (R. Bouchet, ed.), Bari, Italy/Cairo Egypt: CIHEAM - Mediterranean Agronomic Institute of Bari and Water Research Centre, Cairo, Egypt
Rhoades, J.D. 1989. Intercepting, isolating and re-using drainage waters for irrigation to conserve water and protect water quality. Agric. Water Manage. 16:37-52.
Rhoades, J.D. 1993. Electrical conductivity methods for measuring and mapping soil salinity. Adv. Agron. 49: 201-251.
Rhoades, J.D., Kandiah, A. and Mashali, A.M. 1992. The use of saline waters for crop production. FAO Irrigation and Drainage Paper No 48. FAO, Rome. 133 pp.
Salamor, M., Rex, J.E. and Owens, L.P. 1996. Adams Avenue Agricultural Drainage Research Center: Final report on reactor operations for the period January 1, 1995 November 21, 1995. Submitted to California Department of water Resources under Contract B-80502. Fresno, CA:
Adams Avenue Agricultural Drainage Research Center.
Schlumberger Ltd. 1989. Log interpretation principle/application. Section 146.10 of 40 Code of Federal Regulations, Plugging and Abandoning Class I-III Wells.
Schultz, B. and de Vries, W. 1993. The Netherlands experience. Paper presented at the 15th ICID Congress, The Hague.
Shacklette, H.T., and Boerngen, J.G. 1984. Element concentrations in soils and other surficial materials of the conterminous United States. United States Geological Service Professional Paper no. 1270, 105 pp. USGS, Washington, D.C.
Shalhevet, J. 1994. Using water of marginal quality for crop production: major issues. Agric. Water Manage. 25: 233-269.
Shannon, M.C. and Francois, L.E. 1978. Salt tolerance of three muskmelon cultivars. J. Amer. Soc. Hort. Sci. 103: 127-130.
Shannon, M.C., Grieve, C.M. and Francois, L.E. 1994. Whole-plant response to salinity. p. 199-244. In: Plant-Environment Interactions (R.E. Wilkinson, ed.). New York: Marcel Dekker, Inc.
Shirmohammadi, A., Camp, C.R. and Thoma, D.L. 1992. Water table management for field-sized areas in the Atlantic Coastal Plain. J. Soil Water Conserv. 47: 52-57.
Shuval, H.I., Adin, A., Fattal, B., Rawitz, E. and Yekutiel, P. 1986. Wastewater irrigation in developing countries. Health effects and technical solutions. World Bank Technical Paper No. 51. Washington, D.C.: World Bank, 324 pp.
Skaggs, R.W., Breve, M.A. and Gilliam, J.W. 1994. Hydrologic and water quality impacts of agricultural drainage. Critical Reviews in Environmental Science and Technology 24: 1-32.
Skaggs, R.W. and Gilliam, J.W. 1981. Effect of drainage system design and operation on nitrate transport. Trans. ASAE, 24(4): 929-934.
Smedema, L.K. 1990. Natural salinity hazards of irrigation development in (semi)arid regions. p. 22-35. In: Symposium on land drainage for salinity control in arid and semi-arid regions (H.R. Amer, ed.), Vol. 1, Cairo, Egypt: Nubar Printing House.
Smith, G.R., Tanji, K.K., Jurinak, J.J. and Burau, R.G. 1995. Applications of the Pitzer equations-based model to hypersaline solutions. Chapter 7. p. 113-141. In: Chemical equilibria and reaction models (R.H. Loeppert, A.D. Schwab and S. Goldberg, eds.). Soil Science Society of America Special Publication 42. Madison, WI: SSSA.
Soil Conservation Service, United States Department of Agriculture. 1992. Wetland restoration, enhancement or creation. Chapter 13. p. 79. In: Engineering field handbook. Washington, D.C.: USDA-SCS.
Summers Engineering, Inc. 1995. Personal communication.
Suri, R.P.S, Liu, J., Hand, D.W., Crittendon, J.C., Perram, D.L. and Mullins, M.E. 1993. Heterogeneous photocatalytic oxidation of hazardous organic contaminants in water. Water Environment Research 65(5): 665-673.
Tanji, K.K., Ford, S., Toto, A., Summers, J. and Willardson, L. 1993. Evaporation ponds: what are they; why some concerns, p. 573-579. In: Proc., 1993 National Conference on Irrigation and Drainage Engineering (R.G. Alien and C.M.U. Neale, eds.), American Society of Civil Engineers, Park City, Utah, July 21-23, 1993. New York, NY: ASCE.
Tanji, K.K., and Grismer, M. 1989. Physicochemical efficacy of agricultural evaporation ponds: an interim literature review and synthesis. Report to Department of Water Resources. Sacramento, CA.
Tanji, K.K. and Karajeh, F. 1992'. Saline drainwater re-use in agroforestry systems. J. Irrig. and Drain. Engrg., 119 (1): 170-180.
Tanji, K.K. and Karajeh, F. 1993. Agroforestry demonstration project: water and salt balance modeling. A final report to California Department of Food and Agriculture, Sacramento, CA.
Tanji, K.K., Ong, C.G.H., Dahlgren, R.A. and Herbel, M.J. 1992. Salt deposits in evaporation ponds: an environmental hazard? California Agriculture 46(6): 18-21.
Tanji, K.K., Lauchli, A. and Meyer, J. 1986. Selenium in the San Joaquin Valley. Environment 28(6): 6-11, 34-39.
Trewhella, N.W., and Badruddin, M. 1991. Evaporation ponds for disposal of saline drainage effluent in Pakistan. (Guidelines for monitoring, evaluation and research). Lahore, Pakistan: Netherlands Research Assistance Project.
United States Environmental Protection Agency. 1982. Handbook for sampling and sample preservation water and wastewater. EPA-600/4-82-029, Environment Monitoring Support Manual. Cincinnati, OH: USEPA.
United States Environmental Protection Agency. 1994. Short-term methods for estimating the chronic toxicity of effluents and receiving water to freshwater organisms, 3rd Edition. United States Environmental Protection Agency Report #EPA-600-4-91-002 (July 1994). Cincinnati, OH: USEPA, 341 pp.
URS Corporation. 1987. Draft environmental impact report prepared for Westlands water district, September 1987, prototype deep well injection for agricultural drainage water. Sacramento, CA: URS Corp.
Westcot, D.W., Chilcott, J.E. and Smith, G. 1993. Pond water, sediment and crystal chemistry, p. 587-594. In: Proc., 1993 National Conference on Irrigation and Drainage Engineering (R.G. Alien and C.M.U. Neale, eds.), American Society of Civil Engineers, Park City, Utah, July 21-23, 1993. New York, NY: ASCE.
Westcot, D.W., Enos, C.A., Chilcott, J.E. and Belden, K.K. 1990a. Water and sediment quality survey of selected inland saline lakes. Sacramento, CA: Central Valley Regional Water Quality Control Board, 16pp.
Westcot, D.W., Grewell, B.J. and Chilcott, J.E. 1990b. Trace element concentrations in selected streams in California: a synoptic survey. Sacramento, CA: Central Valley Regional Water Quality Control Board, 75 pp.
Westcot, D.W., Rosenbaum, S. and Bradford, G. 1989. Trace element buildup in drainage water evaporation basins, p. 123-135. In: Proc., 2nd Pan-American Regional Conference on Toxic Substances in Agricultural Water Supply and Drainage (J. Summers and S.S. Anderson, eds.), International Commission on Irrigation and Drainage, Ottawa, Canada, June 8-9, 1989.
Willardson, L.S., Meek, B.D., Grass, L.B., Dickey, G.L. and Bailey, J.W. 1972. Nitrate reduction with submerged drains. Trans. Am. Soc. Agric. Eng., 15: 84-85.
World Bank. 1991. Environmental assessment sourcebook. Vol. II. Sectoral Guidelines. World Bank Technical Paper No. 140, Washington, D.C.: World Bank.
WHO. 1982. Manual on environmental management for mosquito control. WHO Offset publication No. 66. Geneva: WHO.
WHO. 1988. Environmental management for vector control - training and information materials. Slide set series. By: Pozzi, A. Geneva: WHO.
WHO. 1989. Geographical distribution of arthropod-borne diseases and their principal vectors. Geneva: WHO, Vector Biology and Control Division.
WHO. 1995. Rapport sur la santé dans le monde 1995, réduire les écarts. Geneva: WHO.
WHO. 1996. The World Health Report 1996: fighting the disease - fostering development. Geneva: WHO.
WHO. 1997. Health and environment in sustainable development - five years after the Earth Summit. WHO, Geneva.
Wichelns, D. and Cone, D. 1992. Farm-level and district efforts to improve water management during drought. Irrig. Drain. Syst. 6(3): 189-199.
Zimmer, D., Arlot, M.P. and Nedelec, Y. 1996. Drainage and environment: the need for an integrated approach, pp. 448-454. In: Proc. 6th International Drainage Workshop, ICID, Ljubljana, Slovenia, April 21-29, 1996.
FAO TECHNICAL PAPERS
WATER REPORTS
Prevention of water pollution by agriculture and related activities, 1993 (E S)
Irrigation water delivery models, 1994 (E)
Water harvesting for improved agricultural production, 1994 (E)
Use of remote sensing techniques in irrigation and drainage, 1995 (E)
Irrigation management transfer, 1995 (E)
Methodology for water policy review and reform, 1995 (E)
Irrigation in Africa in figures/L'irrigation en Afrique en chiffres, 1995 (E/F)
Irrigation scheduling: from theory to practice, 1996 (E)
Irrigation in the Near East Region in figures, 1997 (E)
Quality control of wastewater for irrigated crop production, 1997 (E)
Seawater intrusion in coastal aquifers - Guidelines for study, monitoring and control, 1997 (E)
Modernization of irrigation schemes: past experiences and future options, 1997 (E)
Management of agricultural drainage water quality, 1997 (E)
Availability: December 1997
Ar - Arabic
C - Chinese
E - English
F - French
P - Portuguese
S - Spanish
Multil - Multilingual
* Out of print
** In preparation
The FAO Technical Papers are available through the authorized FAO Sales Agents or directly from Sales and Marketing Group, FAO, Viale delle Terme di Caracalla. 00100 Rome, Italy.
