pursuer:0.033009475094
learning:0.0264087747505
game:0.0251996108837
ga:0.0242027137031
games:0.0223720822914
nn:0.0223355609922
lazy:0.0170720962068
evasion:0.0130975332318
reinforcement:0.0116591431896
evasive:0.0105329488263
pursuers:0.00983075223789
genetic:0.00958803526776
maneuvers:0.00888716637146
evader:0.00888716637146
grefenstette:0.00738922856239
gll:0.00698277357758
pursuit:0.00667399423582
players:0.00628589240661
payoff:0.00581740783164
differential:0.00561734646563
editing:0.00531516129736
teacher:0.00497422722408
actions:0.00482753656713
nearest:0.00466760723854
reward:0.00440504948056
pedestrian:0.00439384554833
action:0.00439049696282
robot:0.00432692411313
evades:0.00421317953053
fitness:0.00383274121309
evade:0.00380878558777
agent:0.00359331886854
plan:0.00349081786049
training:0.00342737587673
car:0.0034109232043
neighbor:0.00324893720165
chauffeur:0.00317398798981
success:0.00299337784399
player:0.0028584998431
tesauro:0.00283663995016
atkeson:0.00283663995016
learner:0.00281292789325
homicidal:0.00280878635368
smoke:0.00272791214068
watkins:0.00272791214068
learn:0.00270492497196
000:0.00267189403306
barto:0.00263901020085
salzberg:0.00263901020085
p1:0.00263345913434
1992:0.00252405097112
play:0.00246910038957
escape:0.00240536810685
backgammon:0.00238136967851
eager:0.00232696313265
samuel:0.00230367474431
1990:0.00227516511748
gle:0.00226931196013
playing:0.0022450761187
teaching:0.00223873987837
train:0.00210172872011
angle:0.00207324874479
database:0.0020664950122
population:0.00198158059467
strength:0.00197277380067
bootstrapping:0.00191160016545
plateau:0.00191160016545
markov:0.00190684974492
sheppard:0.00190439279388
widrow:0.00190439279388
clouse:0.00190439279388
1989:0.00189331074611
delayed:0.00189331074611
curvature:0.00184345471341
sutton:0.00180880989966
agents:0.00179440517798
isaacs:0.00178602725888
rules:0.00177785224202
task:0.00173538961251
p2:0.00173407125104
chess:0.00172728047253
strategies:0.00165972476425
edited:0.00163943961347
fired:0.00162019697967
utgoff:0.00158340612051
classifier:0.00157366955389
neighbors:0.00157309899135
planning:0.00155177532415
tasks:0.00154706001516
ramsey:0.00153827389705
plans:0.0015272490997
aha:0.00149915005341
temporal:0.00149887037437
learned:0.00149214751924
radius:0.00148852763133
wilson:0.00146600549848
markovian:0.00141769838972
outperformed:0.00141769838972
ritter:0.00140571361413
teach:0.00140571361413
othello:0.00140439317684
metagamer:0.00140439317684
devijver:0.00140439317684
colombetti:0.00140439317684
simulator:0.0013860756584
rewards:0.00135660742475
1963:0.00131448067213
bad:0.00130305661815
1993:0.00130305661815
stored:0.00128113270897
td:0.00127758040436
prioritized:0.00127758040436
decision:0.00127416021593
torras:0.00126959519592
ase:0.00126959519592
dorigo:0.00126959519592
tomek:0.00126959519592
millan:0.00126959519592
averaging:0.00126500102269
wins:0.00124474094905
speeds:0.00123819580909
sweeping:0.0012295797101
bearing:0.0012295797101
rule:0.0012290456712
90:0.00122822174411
difficulty:0.00122384013965
ace:0.00119068483925
gammon:0.00119068483925
mutation:0.00118820906573
neural:0.00118809704403
stores:0.00115651250763
parking:0.00113465598006
maze:0.00113465598006
maneuvering:0.00113465598006
competition:0.00112981129085
achieving:0.00111946553571
generations:0.00110927513058
sharply:0.00110927513058
chromosome:0.00109116485627
opponents:0.00109116485627
poor:0.00109078261013
attributes:0.00108362367377
percent:0.00107105496148
1991:0.00106713189769
holland:0.00105560408034
abilities:0.00105484795997
successful:0.00102888940437
fuel:0.00102551593137
mccallum:0.00102551593137
littman:0.00102551593137
alone:0.00101907784476
matcher:0.000999433368938
moore:0.00097475659596
remained:0.00097475659596
began:0.000968402867467
striking:0.000956069319802
credit:0.000956069319802
escaping:0.00093714240942
threshold:0.000927305390528
learns:0.000921727356706
700:0.000916349459819
storing:0.00090000864752
connectionist:0.000889865898109
cart:0.000889865898109
speed:0.000889724672387
1983:0.000885860216227
perfect:0.000879302292615
randomly:0.000876766470161
consisted:0.000876310250039
goals:0.000862045823658
1994:0.00085638934765
shaping:0.000829827299366
trials:0.000828302594133
classifying:0.000816349866745
pruning:0.000816349866745
comparator:0.000810098489835
heading:0.000810098489835
turns:0.000787050935025
rl:0.000783728147286
goldberg:0.000783728147286
navigate:0.000783728147286
strategy:0.000780000857693
1987:0.000775887662076
climbing:0.000775654377551
1975:0.000767891581438
aircraft:0.000760416272398
pole:0.000753207527232
1988:0.000749259589746
gordon:0.000746246626124
tolerating:0.000733002749242
strategic:0.000726690896294
maximally:0.000720568745011
facing:0.000714624960775
sensing:0.000708849194861
gas:0.000703231973313
sequential:0.000703026410587
olsder:0.000702196588421
noncooperative:0.000702196588421
imado:0.000702196588421
basar:0.000702196588421
k nn:0.0424218293988
the ga:0.029599898403
q learning:0.0220732905702
lazy learning:0.0178967092776
the game:0.0131212682601
two pursuer:0.0130837964602
one pursuer:0.0115445262884
evasive maneuvers:0.0107748912025
differential games:0.0105339720388
reinforcement learning:0.00927580227903
000 games:0.00912944243358
a lazy:0.00890371774794
differential game:0.008427177631
genetic algorithm:0.00807380316287
the evasive:0.00769635085896
two pursuers:0.00692671577307
pursuer task:0.00692671577307
state action:0.00634859264719
learning to:0.00633773062973
pursuer game:0.00615708068717
nearest neighbor:0.00590974749021
the evader:0.00561811842067
the pursuer:0.00561811842067
k nearest:0.00556677168071
the players:0.00543224554262
lazy learner:0.00538744560127
grefenstette et:0.00538744560127
turn angle:0.00538744560127
pursuit games:0.00538744560127
pursuit game:0.00538744560127
the pedestrian:0.00530272867484
a game:0.00525741113115
temporal difference:0.00507887411775
learning algorithms:0.00504224323002
action pairs:0.00491585361808
the car:0.00482707158911
genetic algorithms:0.00475048273536
of examples:0.00469076623662
maneuvers task:0.00461781051538
delayed reinforcement:0.00461781051538
pursuer problem:0.00461781051538
games in:0.00453906482019
a ga:0.00429202872198
good examples:0.00429202872198
control tasks:0.0042135888155
nn and:0.00416780177486
games the:0.00397168171766
a genetic:0.00395072403099
the genetic:0.00389134119731
games one:0.00384817542948
single pursuer:0.00384817542948
lazy q:0.00384817542948
lazy approach:0.00384817542948
lazy methods:0.00384817542948
markov decision:0.00381944799725
game we:0.00380915558831
difference learning:0.00380915558831
p1 and:0.00374893378861
game theory:0.00369497319068
to play:0.00364579733707
control problems:0.00361797122927
nn s:0.00351132401292
nn on:0.00351132401292
the database:0.00350592805262
a plan:0.00338075323683
of differential:0.00336222088892
ga to:0.00331420542178
5 000:0.00321804772607
bad examples:0.00317429632359
of curvature:0.00316711987773
nearest neighbors:0.00313981234112
game is:0.0031202537373
the homicidal:0.00307854034359
percent success:0.00307854034359
the pursuers:0.00307854034359
that fired:0.00307854034359
random games:0.00307854034359
nn for:0.00307854034359
chauffeur game:0.00307854034359
ga was:0.00307854034359
traditional lazy:0.00307854034359
90 success:0.00307854034359
pursuer two:0.00307854034359
homicidal chauffeur:0.00307854034359
game the:0.00307684666216
based learning:0.00307684666216
the payoff:0.00306573480141
either method:0.00306573480141
optimal strategies:0.00297700126776
the state:0.00289292989008
sequential decision:0.00283691551262
to evade:0.00280905921033
learning s:0.00280905921033
learning algorithm:0.0027578821621
to train:0.00266985702964
examples in:0.00265783881661
car the:0.00265136433742
nn to:0.00265136433742
games and:0.00260021144775
the lazy:0.00260021144775
to learn:0.00259108796969
learning for:0.00255540902718
perfect performance:0.00253943705887
ga we:0.00253943705887
ga and:0.00253943705887
learning approach:0.00253035076396
the simulator:0.00251175726772
state space:0.00251038630932
strategies for:0.00250358270495
continuous state:0.00245258784113
of lazy:0.00244126243551
and p2:0.00244126243551
the task:0.0024277003271
example set:0.00238160101421
robot to:0.00238160101421
an agent:0.00233917964915
of actions:0.00232718918308
train a:0.00232156065906
examples stored:0.00230890525769
stored state:0.00230890525769
barto et:0.00230890525769
generated games:0.00230890525769
isaacs 1963:0.00230890525769
maneuvers game:0.00230890525769
salzberg 1993:0.00230890525769
successful evasion:0.00230890525769
evader e:0.00230890525769
player pursuit:0.00230890525769
plan fitness:0.00230890525769
rule strength:0.00230890525769
eager learning:0.00230890525769
complete game:0.00230890525769
pursuers figure:0.00230890525769
20 state:0.00230890525769
game e:0.00230890525769
learning after:0.00230890525769
reinforcement problems:0.00230890525769
it reached:0.00230890525769
for evasive:0.00230890525769
prioritized sweeping:0.00230890525769
pursuer evader:0.00230890525769
e evades:0.00230890525769
multi agent:0.0023004655374
of learning:0.00228147042675
for lazy:0.00226953241009
the editing:0.00222362354132
radius of:0.00218967977273
decision problems:0.00217490932399
success rate:0.00214536515072
to escape:0.00214536515072
game in:0.00211141325182
storing examples:0.00210679440775
to reinforcement:0.00210679440775
optimal play:0.00210679440775
payoff function:0.00210679440775
20 time:0.00210679440775
action pair:0.00210679440775
utgoff 1992:0.00210679440775
correct action:0.00210679440775
helpful teacher:0.00210679440775
s performance:0.002093497154
the learning:0.00208545632023
instance based:0.00205123110811
algorithms to:0.00205094799271
performance of:0.00202875409858
the actions:0.00202464237559
in differential:0.00199906091572
watkins 1989:0.00198852325307
classifier systems:0.00198852325307
000 examples:0.00198852325307
two player:0.00198852325307
the pursuit:0.00198852325307
game that:0.00198852325307
credit assignment:0.00198852325307
for k:0.00194839290053
of q:0.00193634992881
learn to:0.00191178586587
learning methods:0.00191178586587
with lazy:0.00190457779416
in instance:0.00190457779416
game as:0.00190457779416
a pursuit:0.00190457779416
a helpful:0.00190457779416
game playing:0.00190457779416
a reinforcement:0.00190457779416
tesauro 1992:0.00190457779416
al 1990:0.00189268534352
a robot:0.00189268534352
better than:0.00186484202531
each example:0.00185705206649
reward for:0.00183944088085
these games:0.00183944088085
a teacher:0.00183944088085
near perfect:0.00183944088085
each plan:0.00183944088085
optimal strategy:0.0018243679386
a differential:0.00180898561463
states and:0.00180675505295
learning and:0.0017867359557
car and:0.00178620076065
the two pursuer:0.0121367086735
the evasive maneuvers:0.00809113911566
for k nn:0.00647291129253
of the game:0.00607656368446
two pursuer task:0.00566379738096
grefenstette et al:0.00566379738096
for the ga:0.00561866424
of q learning:0.00561866424
k nn and:0.00519224178998
state action pairs:0.00519224178998
the genetic algorithm:0.00491633121
evasive maneuvers task:0.0048546834694
5 000 games:0.0048546834694
of k nn:0.0048546834694
the one pursuer:0.0048546834694
performance of k:0.00445049296284
a genetic algorithm:0.00441931904325
temporal difference learning:0.00404616150106
differential game theory:0.00404556955783
lazy learning approach:0.00404556955783
two pursuer game:0.00404556955783
a lazy learner:0.00404556955783
to k nn:0.00404556955783
k nn s:0.00404556955783
k nn on:0.00404556955783
lazy q learning:0.00404556955783
than the ga:0.00404556955783
radius of curvature:0.00391594561062
set of examples:0.00365839746092
k nearest neighbors:0.00357274859228
number of examples:0.00332348531534
markov decision problems:0.00326328800885
one pursuer two:0.00323645564626
a lazy learning:0.00323645564626
homicidal chauffeur game:0.00323645564626
for lazy learning:0.00323645564626
pursuer two pursuers:0.00323645564626
k nn for:0.00323645564626
lazy learning to:0.00323645564626
that k nn:0.00323645564626
the homicidal chauffeur:0.00323645564626
the ga was:0.00323645564626
k nn to:0.00323645564626
q learning s:0.00323645564626
in the game:0.00309960588043
k nearest neighbor:0.00297729049357
of the players:0.00297729049357
a differential game:0.00296699530856
with k nn:0.00296699530856
q learning to:0.00296699530856
optimal strategies for:0.00296699530856
of lazy learning:0.00280933212
on the two:0.00280375798925
p1 and p2:0.00272844219259
to train a:0.00269744100071
of the car:0.00269744100071
of the ga:0.00261063040708
instance based learning:0.00247968470435
in the database:0.00246957731944
for evasive maneuvers:0.0024273417347
the evader e:0.0024273417347
two pursuers figure:0.0024273417347
lazy learning methods:0.0024273417347
pursuer game we:0.0024273417347
two pursuer problem:0.0024273417347
in instance based:0.0024273417347
and k nn:0.0024273417347
the ga to:0.0024273417347
000 games and:0.0024273417347
and the evader:0.0024273417347
the ga we:0.0024273417347
the single pursuer:0.0024273417347
20 time steps:0.0024273417347
20 state action:0.0024273417347
delayed reinforcement problems:0.0024273417347
state action pair:0.0024273417347
examples to k:0.0024273417347
learning s performance:0.0024273417347
of examples stored:0.0024273417347
randomly generated games:0.0024273417347
player pursuit games:0.0024273417347
and the pedestrian:0.0024273417347
barto et al:0.0024273417347
evasive maneuvers game:0.0024273417347
a lazy approach:0.0024273417347
with lazy learning:0.0024273417347
and two player:0.0024273417347
a pursuit game:0.0024273417347
approaches to reinforcement:0.0024273417347
000 games the:0.0024273417347
games one pursuer:0.0024273417347
near perfect performance:0.0024273417347
game where e:0.0024273417347
p1 and p:0.0024273417347
two player pursuit:0.0024273417347
nn for the:0.0024273417347
storing examples in:0.0024273417347
k nn is:0.0024273417347
examples in the:0.00238996450298
the state space:0.00233538321247
the car and:0.00222524648142
a helpful teacher:0.00222524648142
the correct action:0.00222524648142
the car the:0.00222524648142
car and the:0.00222524648142
in the ga:0.00222524648142
the optimal strategies:0.00222524648142
and q learning:0.00222524648142
in differential games:0.00222524648142
which an agent:0.00222524648142
to reinforcement learning:0.00222524648142
a robot to:0.00210699909
the example set:0.00210699909
perform well on:0.0020704852497
et al 1990:0.00205143039219
on one and:0.00202308075053
one and two:0.0019992551313
sequential decision making:0.00195797280531
the k nearest:0.00195797280531
our experiments demonstrate:0.00190476283037
of a game:0.00190476283037
learning for the:0.00190476283037
threshold was set:0.00190476283037
on its own:0.00188670190158
better than the:0.00183305502552
the ability of:0.00182754747666
work together to:0.00182077391098
performance on the:0.00179922774646
performed better than:0.00178637429614
genetic algorithms in:0.00175559533458
in the early:0.00175298144623
the results of:0.00172585773304
to determine the:0.001709830997
set of actions:0.0016789147981
strategies for the:0.0016789147981
the best performance:0.00164638487963
nn on its:0.00161822782313
jump start a:0.00161822782313
a lazy version:0.00161822782313
the ga learned:0.00161822782313
standard q learning:0.00161822782313
to perform sequential:0.00161822782313
learning the task:0.00161822782313
cart and pole:0.00161822782313
lazy learning algorithm:0.00161822782313
see sheppard salzberg:0.00161822782313
that lazy methods:0.00161822782313
q learning after:0.00161822782313
some state action:0.00161822782313
two pursuer evasion:0.00161822782313
two lazy methods:0.00161822782313
the pedestrian the:0.00161822782313
reinforcement learning can:0.00161822782313
the actual reward:0.00161822782313
of perceptual aliasing:0.00161822782313
the ga and:0.00161822782313
class of differential:0.00161822782313
many bad examples:0.00161822782313
e is facing:0.00161822782313
either method could:0.00161822782313
that genetic algorithms:0.00161822782313
success stored games:0.00161822782313
genetic algorithm can:0.00161822782313
maze like environments:0.00161822782313
classifier systems and:0.00161822782313
training agents to:0.00161822782313
evader pursuer pursuer:0.00161822782313
one pursuer task:0.00161822782313
to robot path:0.00161822782313
pursuers p1 and:0.00161822782313
either method alone:0.00161822782313
using simulation models:0.00161822782313
continued to improve:0.00161822782313
to play differential:0.00161822782313
game is a:0.00161822782313
on the evader:0.00161822782313
pursuer task is:0.00161822782313
by grefenstette et:0.00161822782313
by the players:0.00161822782313
a sample game:0.00161822782313
problems widrow 1987:0.00161822782313
3 000 games:0.00161822782313
large continuous state:0.00161822782313
ramsey and grefenstette:0.00161822782313
success rate on:0.00161822782313
nn and q:0.00161822782313
models and competition:0.00161822782313
games as the:0.00161822782313
neighbor k nn:0.00161822782313
after 50 000:0.00161822782313
approach stores complete:0.00161822782313
to teach a:0.00161822782313
finding in non:0.00161822782313
exceeding the ga:0.00161822782313
turn angle at:0.00161822782313
the game then:0.00161822782313
learning approach had:0.00161822782313
k nn alone:0.00161822782313
of differential game:0.00161822782313
with one pursuer:0.00161822782313
