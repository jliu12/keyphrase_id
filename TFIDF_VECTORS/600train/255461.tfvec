pruning:0.0523095765964
rep:0.0417159498708
tdp:0.0264802706146
fossil:0.0193405843595
learning:0.0138587303158
theories:0.0116519557325
overfitting:0.0112091301828
cutoff:0.0110844584906
training:0.00985125598893
post:0.00915797337417
foil:0.00873287608912
grow:0.00830237074488
clause:0.00823460665268
conquer:0.00796483239951
learned:0.00777712850531
krk:0.0071933685002
growing:0.00627789463535
literals:0.00604199647426
noise:0.00544529743574
clauses:0.00540313666429
stnd:0.0052548174251
noisy:0.0050537480116
accuracy:0.00480792910653
quinlan:0.00457950793191
furnkranz:0.00452102181225
pruned:0.00439217690911
cn2:0.0041744959581
literal:0.0038881886937
predictive:0.00378169984806
rule:0.00370533411712
dev:0.00368312895512
bratko:0.00350321161673
stopping:0.00340218880292
pre:0.00339672048551
cohen:0.00337132768296
separateandconquer:0.00322930129447
krkpa7:0.00322930129447
pruningset:0.00322930129447
1993:0.00308189329949
lymphography:0.00291934301394
endgame:0.00291934301394
domains:0.00291390061906
1994:0.00267811992636
covered:0.00257695072478
rules:0.00256962740932
brunk:0.00250905525671
mushroom:0.00250905525671
niblett:0.00233547441115
dzeroski:0.00233547441115
cameron:0.00229812528661
correlation:0.00228829759609
pazzani:0.00224518498268
holte:0.00219779367722
secs:0.00214027387212
datasets:0.00212009010698
phase:0.0020660159625
97:0.00205262663837
criterion:0.00203763699873
chess:0.00198587872606
dolsak:0.00193758077668
splitratio:0.00193758077668
muggleton:0.00193758077668
growingset:0.00193758077668
pagallo:0.00193758077668
king:0.0019326086522
89:0.0018599924268
99:0.00184519370722
98:0.00183337818567
climbing:0.0017835615601
frnkranz:0.00175160580837
classification:0.00167428227618
1990:0.00167410523609
750:0.00165689610245
rook:0.00164273658806
negative:0.00163694587204
heuristic:0.001636579752
concept:0.00160669515088
prolog:0.00155727074019
prune:0.0015353271962
learn:0.00153102296553
relational:0.00148784939809
decision:0.00146491997487
votes:0.00145717410664
austrian:0.00145637148313
1000:0.00143190457637
cover:0.00142860645512
separate:0.00141416837786
learns:0.00141296392645
haussler:0.00137887517197
illegal:0.00137275243741
propositional:0.00136546785045
overly:0.00135060708859
84:0.00134447926106
87:0.00132448337728
hill:0.00132216575732
79:0.00131124219684
1989:0.00130605964428
learner:0.00129362515237
euthyroid:0.00129172051779
elc:0.00129172051779
c4:0.00128356768833
jones:0.00128234575924
clark:0.00126422200917
explanations:0.00125490768832
faster:0.001250369883
exit:0.00124117168664
cpu:0.00122766121474
95:0.00121339640734
log:0.00120948210433
prunes:0.00119152723564
1991:0.00117782073296
500:0.00117060799056
88:0.00116924376092
luaces:0.00116773720558
ranilla:0.00116773720558
overfits:0.00116773720558
bahamonde:0.00116773720558
incremental:0.00116280026281
1992:0.00116077480335
johannes:0.00115956519132
regularities:0.00114487698298
instances:0.00113749492322
avoidance:0.00109845628108
hepatitis:0.00109515772537
250:0.00108402450804
bias:0.00107970131404
76:0.00107395102369
66:0.00106026163345
sick:0.00104362398952
sparcstations:0.00104362398952
stop:0.00103974217495
93:0.00103974217495
white:0.00103430973886
asymptotic:0.0010272977665
setup:0.000996093048958
background:0.000977951116317
olshen:0.000970914322086
michie:0.000970914322086
growth:0.000967312336122
down:0.000967103195727
deliberately:0.00096267576625
b2:0.00096267576625
artificial:0.000960924757567
accuracies:0.000955328372502
top:0.000943209308712
generalize:0.000939026606601
relations:0.000930677664453
67:0.000929831710205
merits:0.000927703463523
induction:0.00092747861138
michalski:0.000919250114644
covers:0.000910623278786
42:0.000902120311901
78:0.000901174445206
widmer:0.000898073993072
breast:0.000898073993072
deleting:0.00089595088529
mesh:0.000888809158967
tries:0.000888809158967
incompatibility:0.000879117470889
commonly:0.000869812080228
75:0.000854391135655
greedy:0.000850199656054
breiman:0.000846278647082
cart:0.000818473101137
domain:0.000811903613497
significance:0.000811895329619
sizes:0.000810296415695
confirms:0.000801709047713
77:0.000800770631305
94:0.000800770631305
tree:0.000795568356633
cancer:0.000794351490425
id3:0.000794351490425
intermediate:0.000793524757733
oscar:0.00078338784734
aq:0.00078338784734
differences:0.000780931579388
inductive:0.000773849868898
uci:0.00077304346088
inefficiency:0.00077304346088
heuristics:0.000772325958383
prematurely:0.000763251321984
learnable:0.000763251321984
disjuncts:0.000763251321984
unseen:0.000763251321984
glass:0.000763251321984
costs:0.000761824539911
tested:0.000760565433823
caught:0.000736661458905
96:0.000730704556755
undone:0.00072858705332
fastest:0.000726654028091
entirely:0.000723626021755
slower:0.000723445640846
deletion:0.000720077692271
fires:0.000713424624041
greedily:0.00070628462708
wherever:0.000699409052473
mere:0.000699409052473
i rep:0.0417650813866
post pruning:0.0372207377952
pre pruning:0.0148561132228
separate and:0.0131430669265
the pruning:0.0117915548086
pruning set:0.0116265233917
pruning phase:0.0116265233917
and conquer:0.00983680405517
rule learning:0.00920344114587
than rep:0.00920247555976
reduced error:0.00875881156766
training set:0.00858046932625
error pruning:0.00853520422739
rep s:0.0083969335607
specific theory:0.00778671008903
rep grow:0.00778671008903
of rep:0.00710509762828
rep and:0.00710509762828
the krk:0.00707882735366
krk domain:0.00707882735366
final theory:0.00707882735366
cohen 1993:0.00707882735366
the cutoff:0.00702598243096
positive examples:0.00673615676674
pruning algorithm:0.00670623189295
pruning algorithms:0.00670623189295
pruning methods:0.00657153346325
and pruning:0.00652412654304
conquer rule:0.00645917966207
concept description:0.0064058645705
dev range:0.00637094461829
accuracy stnd:0.00637094461829
starting theory:0.00637094461829
stnd dev:0.00637094461829
range time:0.00637094461829
the training:0.00625223322886
predictive accuracy:0.0062181110858
and post:0.00618490025563
top down:0.00616964356731
pre and:0.00613909673497
negative examples:0.00605104717539
noisy domains:0.00581326169587
learning algorithms:0.0057079069456
cutoff parameter:0.00566306188293
initial rule:0.00548691700332
theory that:0.00523956161407
set sizes:0.00517220216493
time fossil:0.00516734372966
growing phase:0.00516734372966
and grow:0.00507555515389
decision tree:0.00502015151174
down search:0.00495517914756
pruning i:0.00495517914756
incremental reduced:0.00495517914756
in cohen:0.00495517914756
rep in:0.00495517914756
run times:0.00495066295917
accuracy on:0.00454216775097
conquer learning:0.00452142576345
pruning and:0.00438102230884
of pruning:0.00430484613632
rep is:0.0042676021137
down pruning:0.0042472964122
pruning approaches:0.0042472964122
by fossil:0.0042472964122
growing and:0.00417486894737
training examples:0.0041596001423
stopping criterion:0.0039860873678
tree learning:0.00394765400858
the learned:0.0038791516237
most specific:0.00377329753875
cpu secs:0.00365794466888
run time:0.00359251650533
empty theory:0.00353941367683
cameron jones:0.00353941367683
rule growth:0.00353941367683
overfitting theory:0.00353941367683
tdp is:0.00353941367683
fossil s:0.00353941367683
quinlan 1990:0.00353941367683
furnkranz 1994:0.00353941367683
jones 1994:0.00353941367683
algorithm foil:0.00353941367683
and bratko:0.00353941367683
that tdp:0.00353941367683
return theory:0.00353941367683
tdp s:0.00353941367683
positive 99:0.00353941367683
brunk and:0.00353941367683
and tdp:0.00353941367683
clause is:0.00352556029206
the growing:0.00332768011384
theory is:0.00330032029434
pazzani 1991:0.00322958983104
grow i:0.00322958983104
overly specific:0.00322958983104
theories that:0.00313115171053
a theory:0.00308170446526
theory figure:0.00304828722407
growing set:0.00304828722407
10 noise:0.00304828722407
background knowledge:0.00295984482597
in noisy:0.00295984482597
theory will:0.00291960385589
learning algorithm:0.00289897263113
faster than:0.00288223114908
of post:0.00286989742421
this theory:0.00284273508224
pruning tdp:0.00283153094146
bratko 1992:0.00283153094146
initial top:0.00283153094146
in cameron:0.00283153094146
foil quinlan:0.00283153094146
initial theory:0.00283153094146
fossil with:0.00283153094146
simpler theory:0.00283153094146
subsequent clauses:0.00283153094146
dzeroski and:0.00283153094146
pruning as:0.00283153094146
integrating pre:0.00283153094146
the grow:0.00283153094146
foil 6:0.00283153094146
pruning with:0.00283153094146
intermediate theory:0.00283153094146
rep 97:0.00283153094146
overfitting phase:0.00283153094146
pruning heuristics:0.00283153094146
rule growing:0.00283153094146
and pazzani:0.00281975286327
the clause:0.00276100328356
of theories:0.00273813894302
pruning is:0.00260929309211
overfitting avoidance:0.00258367186483
maximum correlation:0.00258367186483
noise handling:0.00258367186483
the learning:0.00255750385519
learning decision:0.00250927943963
asymptotic complexity:0.00250927943963
training instances:0.00250927943963
is learned:0.00246653735497
and consistent:0.00245563869399
rep has:0.00243862977926
an overfitting:0.00243862977926
holte 1993:0.00243862977926
the overfitting:0.00243862977926
the run:0.00243792250334
pruning in:0.00242750268207
positive and:0.00233780191165
of overfitting:0.00233568308471
the noise:0.00232964291406
a correlation:0.00227108387548
machine learning:0.00226762493778
data sets:0.0022609442786
clark and:0.00225580229062
noisy examples:0.00225580229062
been learned:0.00225580229062
the post:0.00224863154642
pruning the:0.00224538558891
hill climbing:0.00224538558891
learned by:0.00222106822603
clause will:0.00219051115442
pruning a:0.00219051115442
examples that:0.00216274515984
a little:0.00216116442354
conquer strategy:0.00213528819017
search heuristic:0.00213528819017
in holte:0.0021236482061
and niblett:0.0021236482061
grow fossil:0.0021236482061
67 negative:0.0021236482061
initial overfitting:0.0021236482061
grow is:0.0021236482061
pruning sets:0.0021236482061
that rep:0.0021236482061
42 correct:0.0021236482061
pagallo and:0.0021236482061
artificial noise:0.0021236482061
growth rep:0.0021236482061
pruning rep:0.0021236482061
domain initial:0.0021236482061
further deletion:0.0021236482061
quinlan 1994:0.0021236482061
rep on:0.0021236482061
fossil foil:0.0021236482061
subsequent post:0.0021236482061
haussler 1990:0.0021236482061
of tdp:0.0021236482061
krk endgame:0.0021236482061
1994 i:0.0021236482061
pruning decisions:0.0021236482061
examples growingset:0.0021236482061
pruning time:0.0021236482061
growingset pruningset:0.0021236482061
mesh design:0.0021236482061
examples splitratio:0.0021236482061
all theories:0.0021236482061
99 67:0.0021236482061
negative cover:0.0021236482061
grow algorithm:0.0021236482061
pruning criterion:0.0021236482061
separate and conquer:0.0178605153965
and post pruning:0.0119070102643
the pruning set:0.0104186339813
reduced error pruning:0.00955120091935
post pruning algorithms:0.0074418814152
post pruning phase:0.0074418814152
the final theory:0.0074418814152
on the pruning:0.0074418814152
pre and post:0.00707556508361
and conquer rule:0.00682228637096
accuracy stnd dev:0.00669769327368
conquer rule learning:0.00669769327368
stnd dev range:0.00669769327368
dev range time:0.00669769327368
training set sizes:0.00660314999958
range time fossil:0.00595350513216
the krk domain:0.00595350513216
faster than rep:0.00595350513216
in the krk:0.00595350513216
rep and grow:0.00520931699064
most specific theory:0.00520931699064
top down search:0.00520931699064
the pruning phase:0.00520931699064
and conquer learning:0.00477560045967
a theory that:0.00467178359111
theory that is:0.00467178359111
of post pruning:0.00446512884912
i rep s:0.00446512884912
the cutoff parameter:0.00446512884912
in cohen 1993:0.00446512884912
top down pruning:0.00446512884912
pruning i rep:0.00446512884912
of the cutoff:0.00446512884912
incremental reduced error:0.00446512884912
the most specific:0.0042376161223
decision tree learning:0.00420200454519
the post pruning:0.00409337182258
rule learning algorithms:0.00409337182258
in noisy domains:0.00409337182258
complete and consistent:0.00387585410301
accuracy on the:0.00383373706286
return theory figure:0.0037209407076
in the pruning:0.0037209407076
post pruning methods:0.0037209407076
and pazzani 1991:0.0037209407076
a pre pruning:0.0037209407076
post pruning algorithm:0.0037209407076
brunk and pazzani:0.0037209407076
grow i rep:0.0037209407076
initial rule growth:0.0037209407076
cameron jones 1994:0.0037209407076
i rep is:0.0037209407076
the training set:0.00357522053842
the negative examples:0.00350383769333
rule learning algorithm:0.00341114318548
the separate and:0.00341114318548
the background knowledge:0.00322987841917
pruning methods for:0.00310123762625
general to specific:0.00310123762625
in the data:0.00307120316451
foil quinlan 1990:0.00297675256608
for separate and:0.00297675256608
initial rule growing:0.00297675256608
down search for:0.00297675256608
integrating pre and:0.00297675256608
post pruning is:0.00297675256608
and i rep:0.00297675256608
domain with 10:0.00297675256608
rule growing phase:0.00297675256608
than rep s:0.00297675256608
initial top down:0.00297675256608
post pruning approaches:0.00297675256608
good starting theory:0.00297675256608
dzeroski and bratko:0.00297675256608
the intermediate theory:0.00297675256608
to specific order:0.00297675256608
down pruning tdp:0.00297675256608
in cameron jones:0.00297675256608
and bratko 1992:0.00297675256608
than rep in:0.00297675256608
the initial rule:0.00272891454839
post pruning in:0.00272891454839
concept description and:0.00272891454839
the growing set:0.00272891454839
the concept description:0.00258390273534
predictive accuracy on:0.00258390273534
growing and pruning:0.00258390273534
of finite elements:0.00258390273534
number of finite:0.00258390273534
terms of accuracy:0.002480990101
of the negative:0.00242771959769
run time of:0.00238042899151
positive and negative:0.00230957581381
the asymptotic complexity:0.00228070723455
and conquer strategy:0.00228070723455
i rep grow:0.00223256442456
generate all theories:0.00223256442456
rep grow i:0.00223256442456
growth rep grow:0.00223256442456
post pruning with:0.00223256442456
of pre pruning:0.00223256442456
positive 99 67:0.00223256442456
that post pruning:0.00223256442456
in holte 1993:0.00223256442456
while negative cover:0.00223256442456
the pre pruning:0.00223256442456
clark and niblett:0.00223256442456
and niblett 1989:0.00223256442456
positive examples are:0.00223256442456
loop exit loop:0.00223256442456
error pruning i:0.00223256442456
literals has to:0.00223256442456
of pruning and:0.00223256442456
pre pruning algorithm:0.00223256442456
learned by fossil:0.00223256442456
pagallo and haussler:0.00223256442456
error pruning rep:0.00223256442456
a search heuristic:0.00223256442456
a simpler theory:0.00223256442456
that i rep:0.00223256442456
conquer learning strategy:0.00223256442456
overfitting the noise:0.00223256442456
krk domain with:0.00223256442456
the initial overfitting:0.00223256442456
learning and pruning:0.00223256442456
until any further:0.00223256442456
domain initial rule:0.00223256442456
of rep grow:0.00223256442456
rep grow fossil:0.00223256442456
pruning and learning:0.00223256442456
the grow algorithm:0.00223256442456
a subsequent post:0.00223256442456
an overfitting theory:0.00223256442456
pre pruning heuristics:0.00223256442456
in i rep:0.00223256442456
any further deletion:0.00223256442456
foil 6 1:0.00223256442456
a starting theory:0.00223256442456
and pruning sets:0.00223256442456
with 10 noise:0.00223256442456
examples growingset pruningset:0.00223256442456
grow fossil foil:0.00223256442456
and haussler 1990:0.00223256442456
99 67 negative:0.00223256442456
rule growth rep:0.00223256442456
integration of pruning:0.00223256442456
fossil foil 6:0.00223256442456
subsequent post pruning:0.00223256442456
search for a:0.0022271405474
examples in the:0.002198186456
and negative examples:0.00219070689306
to the final:0.00211607472451
as a search:0.00208762100165
a good starting:0.00208762100165
the run time:0.00208526054262
of the training:0.00206856527503
combining pre and:0.00204668591129
noisy domains the:0.00204668591129
pruning set and:0.00204668591129
examples that have:0.00204668591129
overfitting avoidance as:0.00204668591129
the best theory:0.00204668591129
is learned from:0.00204668591129
with increasing training:0.00204668591129
theory will be:0.00204668591129
rule learning systems:0.00204668591129
average run time:0.00204668591129
increasing training set:0.00204668591129
will be learned:0.00204668591129
avoidance as bias:0.00204668591129
rules and conditions:0.00204668591129
accuracy of the:0.00199818871511
the costs of:0.0019500345873
a cutoff of:0.0019379270515
a tight integration:0.0019379270515
the training instances:0.0019379270515
tight integration of:0.0019379270515
cn2 induction algorithm:0.0019379270515
in this domain:0.00188681735563
the positive examples:0.00186074257575
in decision tree:0.00186074257575
for a good:0.00182415093852
p n p:0.00180085909079
asymptotic complexity of:0.00180085909079
description length principle:0.00180085909079
the learning of:0.00180085909079
inductive logic programming:0.00180085909079
needed to encode:0.00180085909079
of pre and:0.00180085909079
most commonly used:0.00177075355443
of the run:0.0017585491648
that is complete:0.00175191884667
the clause is:0.00175191884667
to the concept:0.00174674211935
is a little:0.00174674211935
seems to be:0.00171962189908
the fastest algorithm:0.00171053042591
the predictive accuracy:0.00171053042591
noise in the:0.00170301598084
is faster than:0.00168297945842
