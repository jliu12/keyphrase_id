face:0.041243282278
recognition:0.0209211667013
eigenface:0.0157559131364
facial:0.014729442687
pcc:0.0145439198182
persons:0.0128327201764
training:0.0122104925132
views:0.0116427360186
classifier:0.0102517242996
confidence:0.010217401504
image:0.00942482751035
orl:0.00848395322729
cmf:0.00808473237129
classification:0.00655641920413
dimensionality:0.0065064150736
achermann:0.00646157926737
illumination:0.00644235302425
faces:0.00556912022333
template:0.00551333381209
databases:0.00537797563227
database:0.00532880834303
neural:0.00532118053199
rotations:0.0050945368729
bern:0.00508659607139
hmm:0.00468281436464
pixels:0.00448049412897
person:0.00447477817721
images:0.00424620122432
lattice:0.00393352223377
lines:0.00382888911795
brunelli:0.00363597995455
pose:0.00358981828149
imaging:0.00349350086514
neighbour:0.00346785057357
glasses:0.00346488530198
sampling:0.00291208472105
plane:0.00291117203614
elastic:0.00290965551559
pixel:0.00287921368048
normalises:0.00285905235428
nnc:0.00285905235428
rectilinear:0.00271737277666
connectionist:0.00271737277666
frontal:0.00260088793018
accessories:0.00258463170695
afr:0.00258463170695
classified:0.00254807413092
testing:0.00250711744057
ub:0.00250316850218
intensity:0.00229352455052
human:0.00228641604386
lighting:0.00227880433536
biometric:0.00222138465401
poses:0.00219489884279
segments:0.00216958014868
eigenfaces:0.00214899031187
fronto:0.00214899031187
radial:0.0020986410345
flattens:0.00208773709975
resampled:0.00208773709975
nearest:0.00206571246575
recognise:0.00203463842856
accuracy:0.002026993248
classifiers:0.00199760480324
rates:0.00198895768348
variance:0.00196050830189
robust:0.00195980483287
seconds:0.00191893623252
knee:0.00187312574586
poggio:0.00187312574586
matching:0.00186909226573
lawrence:0.00184414091
misclassification:0.00184118033588
head:0.00179601958246
templates:0.00176510416546
video:0.00168828125984
sphere:0.00165400014363
settings:0.00164094088518
firstly:0.00163079013833
decision:0.00162120321126
workers:0.0015955074701
undertaken:0.0015790709545
distinctive:0.00154804934236
varied:0.0015313498739
geometric:0.0014915087936
profile:0.00148730150174
experiment:0.00147298453395
classifications:0.00146692806602
identification:0.00145604236053
zhang:0.00144159076757
viewing:0.00143960684024
100:0.00143713746515
zygomatic:0.00142952617714
aeberhard:0.00142952617714
beards:0.00142952617714
maching:0.00142952617714
inten:0.00142952617714
illuminant:0.00142952617714
eyebrow:0.00142952617714
mensionality:0.00142952617714
sigma30:0.00142952617714
sigma20:0.00142952617714
abstractmuch:0.00142952617714
samaria:0.00142952617714
wavelet:0.00142050357282
doubled:0.00142050357282
authentication:0.00142050357282
versus:0.00139321254348
expressions:0.00138750841357
benchmark:0.00137654406296
400:0.00136834711149
auto:0.00135929739427
descriptors:0.00134072536328
amongst:0.00134072536328
max:0.00130806497779
secondly:0.00130347512992
markov:0.00129398313387
utilises:0.00129231585347
forensics:0.00129231585347
dimensionalities:0.00129231585347
centage:0.00129231585347
grey:0.0012894429211
recognize:0.00128928217792
varying:0.00126228959908
1d:0.00122942727333
score:0.00122248070135
expense:0.00122208233488
ranganath:0.00121199331818
baron:0.00121199331818
wear:0.00121199331818
exacting:0.00121199331818
randomised:0.00121199331818
olivetti:0.00121199331818
valentin:0.00121199331818
fourthly:0.00121199331818
combined:0.00120849522263
correctly:0.00119584564957
assignments:0.00119264023661
200:0.00118934912005
row:0.0011696461869
2d:0.00116030371413
uncluttered:0.00115496176733
bunke:0.00115496176733
fractionally:0.00115496176733
ten:0.00113689380071
hidden:0.00113305827455
var:0.00112416784742
females:0.00111069232701
parametrisation:0.00111069232701
cluded:0.00111069232701
classifying:0.00110794566978
psi:0.00110794566978
inherent:0.00109203177039
male:0.00107449515593
edelman:0.00107449515593
bidimensional:0.00107449515593
80:0.0010473023945
convolutional:0.00104386854987
chellappa:0.00104386854987
sity:0.00104386854987
quasi:0.00103914167073
nose:0.00101731921428
flatter:0.00101731921428
compounded:0.00101731921428
dmax:0.00101731921428
necessitating:0.000993883943489
comparative:0.000983380558442
120:0.000983380558442
hmms:0.000972905067397
cessing:0.000972905067397
turk:0.000972905067397
caption:0.000972905067397
computationally:0.00096099023866
assess:0.000940902190054
associative:0.00093722302497
optimised:0.000936562872928
pentland:0.000936562872928
resampling:0.000936562872928
prototypical:0.000920590167938
cluttered:0.000920590167938
boundaries:0.000918384351511
feature:0.000912556560258
curve:0.000909509424187
90:0.000909237805786
equi:0.000905790925552
misclassified:0.000905790925552
matched:0.000905322866695
networks:0.000884861673773
classify:0.000881888036147
exploits:0.000878630029844
boundary:0.000868983419947
gabor:0.000866962643393
summarised:0.000866962643393
percentage:0.000866364840779
controlled:0.000863557623684
outdoor:0.000855514678429
neighbours:0.000855514678429
superior:0.000853312138474
network:0.000849672466341
measure:0.000848768792864
face recognition:0.0354985503814
test lines:0.0219376443906
training lines:0.0172367205926
confidence measure:0.0162287485204
testing lines:0.0156697459933
measure factor:0.0156697459933
minimum confidence:0.014298089157
line dimensionality:0.014102771394
face database:0.0125357967946
of training:0.0111126455141
face databases:0.0109688221953
lines n:0.0109688221953
the face:0.0107819762042
face image:0.010635052849
image views:0.0100086624099
the eigenface:0.0100086624099
view recognition:0.0100086624099
combined face:0.00940184759599
per view:0.00904800017743
recognition algorithm:0.00873591331545
lines was:0.00857885349422
image based:0.00815373069514
training views:0.00783487299665
lattice lines:0.00783487299665
classification accuracy:0.00778477994593
line based:0.00775542872351
based face:0.00775542872351
the line:0.00761193630625
was set:0.00727557750314
facial expressions:0.00727339819232
recognition rates:0.00727339819232
robust to:0.0072477606179
in facial:0.00646285726959
of face:0.00644824604852
the combined:0.00629318542956
dimensionality l:0.00626789839732
neighbour classifier:0.00626789839732
eigenface classifier:0.00626789839732
test views:0.00626789839732
face class:0.00626789839732
achermann et:0.00626789839732
pcc versus:0.00626789839732
correctly classified:0.0061051225474
image representation:0.00590836269392
of testing:0.00587744296847
l j:0.00584022961671
the image:0.00583190816174
of test:0.00582184807548
the orl:0.00571923566281
correct classification:0.00565911311668
the classification:0.00557673621166
a face:0.00557161102451
neural network:0.00555632275706
the confidence:0.00552463348014
two persons:0.0053981694006
c p:0.00534285283803
line segments:0.00516796951615
class f:0.00508760212283
zhang et:0.00499346108319
in illumination:0.00484893212821
versus time:0.00484893212821
nearest neighbour:0.00472669015513
recognition rate:0.00472669015513
classification pcc:0.00470092379799
classified if:0.00470092379799
dimensionality was:0.00470092379799
confidence factor:0.00470092379799
cmf min:0.00470092379799
rotations out:0.00470092379799
intermediate changes:0.00470092379799
template approach:0.00470092379799
set to:0.00463363137752
face images:0.00462076080488
et al:0.00456028663337
feature based:0.00452985038619
human face:0.00444364802031
to changes:0.00443905228086
the template:0.00442810850101
template based:0.00436795665773
initial testing:0.00428942674711
model face:0.00428942674711
benchmark algorithms:0.00428942674711
test view:0.00428942674711
the pcc:0.00428942674711
image view:0.00428942674711
orl database:0.00428942674711
face view:0.00428942674711
changes in:0.00418559677897
is robust:0.0041237954672
total the:0.00412142891839
or image:0.00412142891839
training and:0.00407706275194
geometric or:0.00404862705045
of persons:0.00404862705045
lawrence et:0.00404862705045
and view:0.00402183081241
was varied:0.00397632207455
based approach:0.0039744095167
view for:0.0039332587952
the plane:0.00390585928106
imaging plane:0.00387771436176
viewing sphere:0.00387771436176
individual databases:0.00387771436176
elastic matching:0.00387771436176
al 20:0.00385350136989
views in:0.00368308898676
100 0:0.0036529417088
lines is:0.0036529417088
experiment the:0.00363778875157
facial features:0.00363669909616
and elastic:0.00363669909616
of facial:0.00354501761635
varied from:0.00351707920583
views of:0.00349842685636
view based:0.00346557060366
test time:0.00346557060366
the training:0.00345999774018
lines l:0.00333273601523
al 1:0.00328012934631
face in:0.00322412302426
seconds per:0.00322412302426
test image:0.00322412302426
initial number:0.00322412302426
the algorithm:0.00318370188752
the minimum:0.00318322371921
and out:0.0031428079188
classified persons:0.00313394919866
selected sampling:0.00313394919866
were resampled:0.00313394919866
classifier performed:0.00313394919866
face boundaries:0.00313394919866
pcc of:0.00313394919866
person for:0.00313394919866
curve flattens:0.00313394919866
face rotations:0.00313394919866
imaging constraints:0.00313394919866
per face:0.00313394919866
k face:0.00313394919866
network 96:0.00313394919866
17 values:0.00313394919866
distinctive knee:0.00313394919866
test times:0.00313394919866
both face:0.00313394919866
for explanation:0.00313394919866
lattice line:0.00313394919866
accuracy increase:0.00313394919866
head pose:0.00313394919866
max selected:0.00313394919866
face classes:0.00313394919866
strict imaging:0.00313394919866
person database:0.00313394919866
8 values:0.00313394919866
lines set:0.00313394919866
the ub:0.00313394919866
flexible template:0.00313394919866
combined database:0.00313394919866
facial structures:0.00313394919866
superior compared:0.00313394919866
controlled illumination:0.00313394919866
threshold cmf:0.00313394919866
reduced recognition:0.00313394919866
quasi real:0.00313394919866
persons from:0.00313394919866
ten frontal:0.00313394919866
rotations both:0.00313394919866
example misclassification:0.00313394919866
database using:0.00313222503365
decision is:0.00311179143676
compared with:0.00311025897635
radial basis:0.00309107168879
segments of:0.00299823002282
the imaging:0.00298224155591
of correctly:0.0029499440964
random sampling:0.0029499440964
neural networks:0.00292224562771
of faces:0.00291929247972
accuracy for:0.00291023100126
in total:0.00291023100126
recognition of:0.00289838737827
face and:0.00289012602742
to 0:0.00288236286852
rectilinear line:0.00285961783141
k faces:0.00285961783141
achieved 100:0.00285961783141
1d line:0.00285961783141
illumination intensity:0.00285961783141
9 values:0.00285961783141
near 100:0.00285961783141
classification rate:0.00285961783141
auto associative:0.00285961783141
of bern:0.00285961783141
first increases:0.00285961783141
8 max:0.00285961783141
method achieved:0.00285961783141
selected 100:0.00285961783141
l 32:0.00285961783141
factor equal:0.00285961783141
persons the:0.00285961783141
current face:0.00285961783141
face boundary:0.00285961783141
the minimum confidence:0.0164751918729
confidence measure factor:0.0164751918729
of training lines:0.0164751918729
the line dimensionality:0.0131801534983
of test lines:0.0131801534983
number of training:0.0121151769454
of testing lines:0.011532634311
face recognition algorithm:0.0100106423629
combined face database:0.00988511512374
the combined face:0.00988511512374
number of testing:0.00906210169465
based face recognition:0.00858055059679
the face recognition:0.00858055059679
was set to:0.00855853160992
minimum confidence measure:0.00823759593645
number of test:0.00724521524028
training lines n:0.00659007674916
nearest neighbour classifier:0.00659007674916
pcc versus time:0.00659007674916
the eigenface classifier:0.00659007674916
achermann et al:0.00659007674916
lines n k:0.00659007674916
line dimensionality l:0.00659007674916
total the number:0.00659007674916
and view recognition:0.00659007674916
values in total:0.00659007674916
the classification accuracy:0.00606236420382
versus time for:0.00604140112977
in facial expressions:0.00604140112977
l r s:0.00604140112977
changes in facial:0.00604140112977
is robust to:0.00595791057662
or image based:0.00572036706452
line segments of:0.00572036706452
c p 1:0.0054925341682
changes in illumination:0.00531577028291
zhang et al:0.00531577028291
in total the:0.00517130862197
to changes in:0.00507007243144
was varied from:0.00504913840986
geometric or image:0.00494255756187
correct classification pcc:0.00494255756187
correctly classified if:0.00494255756187
lines was set:0.00494255756187
seconds per view:0.00494255756187
training lines was:0.00494255756187
line dimensionality was:0.00494255756187
facial expressions and:0.00494255756187
rotations out of:0.00494255756187
measure factor is:0.00494255756187
initial testing lines:0.00494255756187
line based face:0.00494255756187
minimum confidence factor:0.00494255756187
face and view:0.00494255756187
of two persons:0.00494255756187
r s l:0.00494255756187
of the plane:0.00486644997609
out of the:0.0047506802081
for the combined:0.00462167490198
line based algorithm:0.00453105084733
recognition rates for:0.00453105084733
the confidence measure:0.00453105084733
image based approaches:0.00453105084733
the line based:0.00453105084733
the feature based:0.00453105084733
in the viewing:0.00453105084733
the orl database:0.00453105084733
images for each:0.00453105084733
the number of:0.00438330416766
c p 2:0.00429027529839
percentage of correctly:0.00429027529839
lawrence et al:0.00429027529839
robust to changes:0.00429027529839
of correct classification:0.00429027529839
et al 20:0.00417712621698
the imaging plane:0.00411940062615
probability of correct:0.00411940062615
on the combined:0.00411940062615
of correctly classified:0.00411940062615
the viewing sphere:0.00411940062615
in and out:0.00403839231513
that the face:0.00398682771218
the algorithm is:0.00398423717951
and out of:0.00397710868623
face recognition using:0.00387848146648
feature based approach:0.00378685380739
32 and the:0.00378685380739
the initial number:0.00363741852229
and the minimum:0.00362459719922
et al 1:0.00360578449998
initial number of:0.00351803872587
of the imaging:0.00351803872587
set of random:0.00337448448903
of the face:0.00333339531019
superior compared with:0.00329503837458
views in quasi:0.00329503837458
for both face:0.00329503837458
in quasi real:0.00329503837458
is superior compared:0.00329503837458
class f g:0.00329503837458
robust to rotations:0.00329503837458
current face recognition:0.00329503837458
first increases rapidly:0.00329503837458
each face class:0.00329503837458
8 max 0:0.00329503837458
factor is smaller:0.00329503837458
classification accuracy increase:0.00329503837458
accuracy first increases:0.00329503837458
5 training views:0.00329503837458
face image views:0.00329503837458
the curve flattens:0.00329503837458
face class f:0.00329503837458
set to 300:0.00329503837458
and elastic matching:0.00329503837458
class f k:0.00329503837458
test image views:0.00329503837458
two neural networks:0.00329503837458
achieved 100 correct:0.00329503837458
face databases and:0.00329503837458
rotations in and:0.00329503837458
quasi real time:0.00329503837458
curve flattens out:0.00329503837458
testing lines is:0.00329503837458
benchmark algorithms and:0.00329503837458
threshold cmf min:0.00329503837458
to class f:0.00329503837458
of views on:0.00329503837458
the individual databases:0.00329503837458
individual lines we:0.00329503837458
the combined database:0.00329503837458
robust to variations:0.00329503837458
test views in:0.00329503837458
from the orl:0.00329503837458
lines set to:0.00329503837458
and test lines:0.00329503837458
the geometric or:0.00329503837458
per view sec:0.00329503837458
text for explanation:0.00329503837458
example misclassification of:0.00329503837458
0 8 max:0.00329503837458
the image representation:0.00329503837458
dimensionality was set:0.00329503837458
ten frontal face:0.00329503837458
rotations both in:0.00329503837458
correctly classified persons:0.00329503837458
lattice lines l:0.00329503837458
k face classes:0.00329503837458
view for 5:0.00329503837458
recognition of views:0.00329503837458
300 the initial:0.00329503837458
2d face image:0.00329503837458
test view for:0.00329503837458
testing lines set:0.00329503837458
a face view:0.00329503837458
of initial testing:0.00329503837458
database see text:0.00329503837458
test lines n:0.00329503837458
persons from the:0.00329503837458
the classification rate:0.00329503837458
confidence factor was:0.00329503837458
testing lines was:0.00329503837458
training views of:0.00329503837458
misclassification of a:0.00329503837458
recognition performance results:0.00329503837458
max selected 100:0.00329503837458
selected 100 0:0.00329503837458
200 the line:0.00329503837458
lines is doubled:0.00329503837458
flexible template approach:0.00329503837458
for 5 training:0.00329503837458
of current face:0.00329503837458
in head pose:0.00329503837458
two persons from:0.00329503837458
of facial structures:0.00329503837458
class c p:0.00329503837458
neural network 96:0.00329503837458
lattice lines for:0.00329503837458
l 32 and:0.00329503837458
hidden markov models:0.00319252946716
accuracy for the:0.00316194431382
training and test:0.00305316514339
of a test:0.00302879423635
factor was set:0.00302070056488
template based approach:0.00302070056488
face recognition and:0.00302070056488
line l j:0.00302070056488
rectilinear line segments:0.00302070056488
factor equal to:0.00302070056488
the nearest neighbour:0.00302070056488
expected both the:0.00302070056488
image based face:0.00302070056488
and is robust:0.00302070056488
frontal face images:0.00302070056488
algorithm is robust:0.00302070056488
lines l j:0.00302070056488
our decision is:0.00302070056488
0 7 seconds:0.00302070056488
to 300 the:0.00302070056488
experiment the number:0.00302070056488
a test view:0.00302070056488
