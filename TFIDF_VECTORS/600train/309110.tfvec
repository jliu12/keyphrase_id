maximizer:0.0235011342999
evader:0.0210339274896
players:0.0189904308308
minimizer:0.0170285219964
equilibrium:0.015741159086
pursuer:0.0147237492427
policies:0.0143667920613
games:0.0139594652806
game:0.0137729566585
policy:0.012735069661
stationary:0.0113149640288
player:0.0100635731823
patek:0.0093068733541
stochastic:0.00846944421394
shapley:0.00736187462135
primal:0.00734632936994
bertsekas:0.00699545800264
shortest:0.00643110275596
proper:0.00525421443162
terminal:0.00511894914018
contraction:0.0050313131797
monotonicity:0.00457108130789
ssp:0.00451944890382
improper:0.00442283686242
sup:0.00426579095015
prolonging:0.00420678549792
kushner:0.00420678549792
undiscounted:0.00420678549792
bellman:0.003810909598
dual:0.0036647576038
transition:0.00355782816367
opposing:0.00323532095412
chamberlain:0.00316702988467
mixed:0.00300216549612
evasion:0.00295898787097
stay:0.00288412216967
prolong:0.00271166934229
minimax:0.00268424974344
caught:0.00265383061137
lim:0.00255925374038
actions:0.00249937082388
clockwise:0.00242879411118
terminating:0.00233310944078
transitioning:0.00232890596355
action:0.00222258801163
probability:0.0022099533297
convergence:0.00219299290791
randomized:0.00218541781124
markov:0.00210610643964
shiau:0.00210339274896
move:0.00200392483081
continuity:0.00197635290389
discounted:0.00196855686059
seeks:0.00194623046574
toward:0.00192860006372
played:0.00191527341803
indefinitely:0.00182159558339
iteration:0.00179313762779
proposition:0.00176663341098
3g:0.00174761180365
catch:0.00172079022746
semicontinuous:0.00169901617532
compact:0.00164232326995
lemma:0.00162901793599
location:0.00162309392553
termination:0.00156841559517
costs:0.00156827478509
delta:0.00156164475826
nonnegativity:0.0015526039757
infimum:0.0015243638392
monotonically:0.00150774540615
pursuit:0.00147427895414
virginia:0.00144408261523
probabilities:0.00142349291316
inevitable:0.00139244857702
drive:0.00130293478924
stage:0.00130235070702
absorbing:0.00129843599554
adjacent:0.00128690602784
spaces:0.00124968541194
supremum:0.00124787117684
horizon:0.00124787117684
strategies:0.00120300998889
decision:0.0011874126449
maximizing:0.00117627338256
charlottesville:0.00116335916926
9300494:0.00116335916926
dimitrib:0.00116335916926
prolongs:0.00116335916926
sdp5f:0.00116335916926
22903:0.00116335916926
additive:0.0011576578401
converges:0.00115391383022
terminates:0.0011463224921
inequality:0.00114081102202
assumptions:0.00114031482427
corner:0.00112645191142
infinite:0.00111877491334
bilinear:0.00109856637276
decisions:0.00107718241044
decides:0.00106860269757
operator:0.00106149690974
verifies:0.00105597399929
happily:0.00105169637448
kakutani:0.00105169637448
wal:0.00105169637448
mizer:0.00105169637448
minimizing:0.00104792396977
thanks:0.00103045200326
inf:0.00103023434959
preceding:0.00102598433445
existence:0.00101340156079
ft:0.00100626263594
corners:0.000994864001725
stagewise:0.000986329290322
prevails:0.000986329290322
infima:0.000986329290322
suprema:0.000986329290322
duality:0.00097311523287
matrix:0.000968057793241
faced:0.000957636709016
formulation:0.000944198863642
v2v:0.000939916584708
randomize:0.000939916584708
weierstrass:0.000939916584708
tsitsiklis:0.000939916584708
interfere:0.000937991274177
shall:0.000924930717706
appendix:0.00092493070995
opponents:0.000903889780764
groundwork:0.000903889780764
olsson:0.000903889780764
dmi:0.000903889780764
path:0.000894794966711
kumar:0.000884766197786
admit:0.000884766197786
operators:0.000883657775926
characterized:0.000883657775926
tracts:0.000874432250331
induc:0.000874432250331
02139:0.000874432250331
ij:0.000871177258612
ultimately:0.000868623192828
achievable:0.00085315819003
inflexible:0.000849508087659
his:0.000838193076977
convex:0.000837432115572
theorists:0.000827902038398
optimally:0.000820548815876
designation:0.00080883023853
uniqueness:0.000806918109032
proceeding:0.000803585808637
mathematical:0.000802083677469
payoffs:0.000791757471166
catches:0.000791757471166
regularity:0.000790541161558
underlying:0.000782550979558
1x:0.000776301987848
play:0.00076699957286
feasible:0.000765744491068
commensurate:0.0007621819196
discounting:0.0007621819196
relating:0.000759747937835
rewards:0.000749183211983
arranging:0.000749183211983
nonstationary:0.000749183211983
norm:0.000721238345017
distributions:0.000721238345017
nonterminal:0.000715414888343
stages:0.000707345523675
controls:0.000707185251798
fied:0.000705540728619
kt:0.000705540728619
characterization:0.000700491865121
incentive:0.000696224288512
establishing:0.000694431844909
nonnegative:0.000694431844909
weighted:0.000693698032463
decreasing:0.000684730162554
collect:0.000677223738098
wind:0.000671062435859
establish:0.000667974943605
pur:0.000663457652842
mini:0.000663457652842
jb:0.000656185620197
persistence:0.000642529917677
land:0.000642529917677
397:0.000642529917677
gammap:0.000629907106882
satis:0.000629907106882
continuous:0.000627524355343
232:0.000623935588418
suit:0.000623935588418
covered:0.000618899462771
min:0.000613520891533
edu:0.000598428587139
componentwise:0.000596898554062
subject:0.000592087710198
deterministic:0.000590091222474
fixes:0.00058719041367
the maximizer:0.0267615675502
stochastic shortest:0.02550337542
the minimizer:0.0236134708025
the evader:0.0221073818893
equilibrium cost:0.020402700336
the pursuer:0.0162896498132
stationary policies:0.015302025252
stationary policy:0.0151261033979
cost vector:0.0139625569827
players are:0.0118378565139
shortest path:0.0116990977097
the game:0.010869977102
the players:0.0106368459536
terminal state:0.010471917737
p bertsekas:0.010201350168
d patek:0.010201350168
all stationary:0.010201350168
patek and:0.010201350168
have that:0.00936547421082
the equilibrium:0.00923056203402
matrix game:0.00892618139699
unique fixed:0.00887839238541
2 x:0.00856709671574
two players:0.00846061080257
fixed point:0.00841648063493
policies for:0.00819011039887
value iteration:0.00789190434259
policies m:0.00765101262599
path games:0.00765101262599
probability one:0.00736397027554
policy iteration:0.00736305953599
of t:0.00727300717225
a stationary:0.00714254740378
current location:0.00710909024995
that x:0.00703429970021
2 m:0.00663482145635
t x:0.00659895814996
stochastic games:0.00658936371063
policy for:0.00656291235137
u i:0.00646857058311
is proper:0.00640159551539
proper policy:0.006375843855
monotonicity of:0.00636201821254
an equilibrium:0.00627178590895
the primal:0.00609843675956
other s:0.00607597399024
are proper:0.00591892825694
bellman s:0.00581773207613
expected cost:0.00579624704108
the unique:0.00576917275465
2 u:0.00569388256422
the monotonicity:0.00564296089722
minimizer is:0.00564040720171
move toward:0.00525932823999
one player:0.00525932823999
main results:0.00518292789967
assumption ssp:0.005100675084
stay where:0.005100675084
location action:0.005100675084
shapley s:0.005100675084
player stochastic:0.005100675084
and chamberlain:0.005100675084
kushner and:0.005100675084
across from:0.005100675084
path game:0.005100675084
assumption r:0.0050794582401
d p:0.00494615098403
lemma 4:0.00478064479135
at state:0.00470383943171
control constraint:0.0046541856609
transition costs:0.0046541856609
terminating with:0.0046541856609
unique equilibrium:0.0046541856609
s d:0.00460704702193
with probability:0.0045750175474
v i:0.00455227760291
of policies:0.00452017632196
q e:0.0044958227709
x 2:0.00444511557484
path problems:0.00444318140622
is improper:0.00439290914042
two player:0.00439290914042
a contraction:0.00437286496343
the minimizing:0.00437286496343
path problem:0.00430815654968
s current:0.00426310943639
games where:0.00420746259199
and policy:0.00406356659208
functions c:0.00404480211027
contraction mapping:0.00394595217129
the convergence:0.00389175741651
t t:0.00387426753421
constraint sets:0.00384647439632
t we:0.00384427230581
dynamic games:0.003825506313
game will:0.003825506313
maximizer to:0.003825506313
primal cost:0.003825506313
weighted sup:0.003825506313
players implement:0.003825506313
evader is:0.003825506313
randomized policies:0.003825506313
minimizer are:0.003825506313
of kushner:0.003825506313
thus 3g:0.003825506313
game indefinitely:0.003825506313
dual value:0.003825506313
policy m:0.003825506313
mixed decision:0.003825506313
equilibrium strategy:0.003825506313
actions simultaneously:0.003825506313
is prolonging:0.003825506313
transition cost:0.003825506313
maximizer s:0.003825506313
minimizing player:0.003825506313
2 s:0.00381420959059
primal dual:0.00381152297473
convergence of:0.00378122142763
x delta:0.00377837670207
zero sum:0.00376027146781
toward the:0.00370961323802
m and:0.00365799962242
u 2:0.00363336595818
s equation:0.00363017826052
policy 2:0.00361614105757
lim t:0.00361614105757
or move:0.00361614105757
transition probabilities:0.00360349534542
x we:0.00356523768018
the terminal:0.00355259119699
the transition:0.00349893890005
to prolong:0.00349063924568
action spaces:0.00349063924568
are across:0.00349063924568
cost x:0.00349063924568
prolong the:0.00349063924568
mixed strategies:0.00349063924568
sup norm:0.00349063924568
the opposing:0.00349063924568
additive cost:0.00349063924568
2 n:0.00346802317266
u v:0.00345154619092
operator t:0.00344652523974
iteration and:0.00340366710359
true 1:0.0033985792736
state 3:0.0033985792736
3 x:0.00329944048173
control actions:0.00329468185531
players may:0.00329468185531
e d:0.00326277126305
of value:0.00322972199532
terminates with:0.00320079775769
game we:0.003155596944
both players:0.003155596944
his current:0.003155596944
terminal states:0.003155596944
underlying control:0.003155596944
t 1:0.0031539196466
following are:0.0031500424611
state 2:0.00313589295448
one another:0.00310054806342
state i:0.00309134436502
finite state:0.00307275186208
minimizer and:0.00304767494406
to termination:0.00304767494406
s policy:0.00304767494406
transitioning from:0.00304767494406
proper the:0.00304767494406
are compact:0.00304767494406
that t:0.00299282002668
lemma a:0.0029726820022
cost to:0.00297257240739
is terminating:0.00295946412847
continuous as:0.00295946412847
mathematical formulation:0.00295946412847
of transitioning:0.00295946412847
are adjacent:0.00294830327228
proposition 4:0.00292471010722
all policies:0.00288485579724
at his:0.00288485579724
and characterization:0.00288485579724
a policy:0.00288279627634
the functions:0.0028602918064
in x:0.00281300301819
g 3:0.00280549354498
the policy:0.00276746922905
distributions over:0.00276315556909
the dual:0.00275635650852
x there:0.00273310355233
optimal cost:0.00271210579318
the minimax:0.00266590884373
seeks to:0.00265425775294
inequality follows:0.00265425775294
vector in:0.00264875053987
policy is:0.00262516494055
policies of:0.00262371897806
state space:0.00259958118744
the contraction:0.00258489392981
to assumption:0.00258489392981
over underlying:0.002550337542
stochastic shortest path:0.025472630448
for the minimizer:0.0196647023252
for the maximizer:0.0120659828438
the equilibrium cost:0.0120659828438
equilibrium cost vector:0.0120659828438
policies for the:0.0116357648286
a stationary policy:0.0110613950579
s d patek:0.0107253180834
all stationary policies:0.0107253180834
patek and d:0.0107253180834
d p bertsekas:0.0107253180834
d patek and:0.0107253180834
monotonicity of t:0.0104736034933
we have that:0.0103472739312
policy for the:0.00986646846436
unique fixed point:0.00973281427161
two players are:0.00938465332296
x 2 x:0.00913383392996
and d p:0.00865139046365
with probability one:0.00834565519166
the unique fixed:0.00814613605031
to the minimizer:0.00814613605031
the two players:0.00814613605031
shortest path games:0.00804398856254
stationary policies for:0.00804398856254
other s current:0.00804398856254
value iteration and:0.00804398856254
the monotonicity of:0.00694536327377
the matrix game:0.00670332380212
convergence of value:0.00670332380212
bellman s equation:0.00670332380212
the terminal state:0.00670332380212
the minimizer is:0.00648854284774
of t we:0.00633609906739
2 u i:0.0063122098561
i 2 s:0.0061876788346
of value iteration:0.00614521947661
one for all:0.00603387825682
u 2 u:0.00594847115212
q e d:0.00581788241432
g 2 x:0.00540711903978
subject to g:0.00540711903978
control constraint sets:0.00536265904169
one player stochastic:0.00536265904169
g 3 x:0.00536265904169
across from one:0.00536265904169
players are adjacent:0.00536265904169
for the pursuer:0.00536265904169
policies m and:0.00536265904169
shortest path game:0.00536265904169
toward the other:0.00536265904169
stay where they:0.00536265904169
stationary policy for:0.00536265904169
iteration and policy:0.00536265904169
kushner and chamberlain:0.00536265904169
player stochastic shortest:0.00536265904169
terminating with probability:0.00536265904169
the other s:0.00536251460679
shortest path problems:0.00526017488008
fixed point of:0.00519582628993
shortest path problem:0.0051359052362
m and n:0.00508306769891
i and v:0.0050103533218
u i and:0.00496901152602
s current location:0.00491617558129
move toward the:0.00491617558129
and policy iteration:0.00491617558129
t we have:0.00478505385407
have that x:0.00477132586372
2 m is:0.00470109598951
from the monotonicity:0.00470109598951
terminates with probability:0.00465493488589
is a contraction:0.00465493488589
point of t:0.00465493488589
the convergence of:0.0046136849743
the players are:0.00446953641666
following are true:0.00446953641666
2 x there:0.00432569523182
probability one for:0.00432569523182
that the equilibrium:0.00432569523182
are true 1:0.00420813990407
and v i:0.00414084293835
lim t 1:0.00410872418896
the operator t:0.00410872418896
cost to the:0.00402258550455
at his current:0.00402199428127
stationary policy 2:0.00402199428127
the transition costs:0.00402199428127
pair of policies:0.00402199428127
the minimizer and:0.00402199428127
the minimizer are:0.00402199428127
unique equilibrium cost:0.00402199428127
policy iteration to:0.00402199428127
maximizer to prolong:0.00402199428127
existence and characterization:0.00402199428127
for stochastic shortest:0.00402199428127
the primal cost:0.00402199428127
if 2 m:0.00402199428127
the minimizing player:0.00402199428127
let 2 m:0.00402199428127
s location action:0.00402199428127
of kushner and:0.00402199428127
equilibrium strategy for:0.00402199428127
prolong the game:0.00402199428127
for the evader:0.00402199428127
fixed point x:0.00402199428127
m is proper:0.00402199428127
continuous as functions:0.00402199428127
policy 2 m:0.00402199428127
when the players:0.00402199428127
assumption r is:0.00402199428127
minimizer are proper:0.00402199428127
an equilibrium strategy:0.00402199428127
3 thus 3g:0.00402199428127
the evader is:0.00402199428127
games where the:0.00402199428127
are across from:0.00402199428127
his current location:0.00402199428127
the game will:0.00402199428127
delta k w:0.00402199428127
weighted sup norm:0.00402199428127
the maximizer to:0.00402199428127
c i u:0.00402199428127
players are across:0.00402199428127
a proper policy:0.00402199428127
other s location:0.00402199428127
the game indefinitely:0.00402199428127
the dual value:0.00402199428127
of t x:0.00394658738574
our main results:0.00393347373181
is the unique:0.00389656211026
x we have:0.00372654764857
element of t:0.00370918008518
from one another:0.00370918008518
that is proper:0.00368713168597
functions c i:0.00368713168597
is terminating with:0.00368713168597
for the game:0.00368713168597
to prolong the:0.00368713168597
non terminal states:0.00368713168597
point in x:0.00368713168597
k w 1:0.00368713168597
the players may:0.00368713168597
continuity of t:0.00368713168597
the following are:0.00357822610767
each i 2:0.00353581647692
to the equilibrium:0.00349120116442
of transitioning from:0.00349120116442
true 1 the:0.00349120116442
the transition probabilities:0.00346388419329
such that x:0.00346026373072
1 t t:0.00335215231249
from some initial:0.00335215231249
the functions c:0.00335215231249
fixed point in:0.00335215231249
and characterization of:0.00335215231249
and 2 n:0.00335215231249
some initial state:0.00324427142387
probability distributions over:0.00324427142387
policies of the:0.00324427142387
where they are:0.00319003590271
2 x we:0.00315610492805
every x 2:0.00312617879475
the case where:0.00304577617695
to g 2:0.00301693912841
i u v:0.00301693912841
the unique solution:0.00301458066897
lemma 4 2:0.00299382438771
be such that:0.00298545365015
iteration to the:0.00290894120716
x there exists:0.00286279551823
characterization of a:0.00286279551823
for all t:0.002823071531
for every x:0.002823071531
u v are:0.00282065759371
case where the:0.00272246800926
markov chain terminates:0.00268132952085
where the minimizing:0.00268132952085
convergence of policy:0.00268132952085
the maximizer in:0.00268132952085
minimizer is denoted:0.00268132952085
path the maximizer:0.00268132952085
are policies for:0.00268132952085
along a least:0.00268132952085
constraint sets for:0.00268132952085
transition cost functions:0.00268132952085
u v take:0.00268132952085
system but without:0.00268132952085
t within x:0.00268132952085
the unique equilibrium:0.00268132952085
most one fixed:0.00268132952085
the maximizer and:0.00268132952085
a special terminal:0.00268132952085
x gamma delta:0.00268132952085
forms an equilibrium:0.00268132952085
players implement actions:0.00268132952085
over underlying control:0.00268132952085
always move toward:0.00268132952085
location action 3:0.00268132952085
cost path the:0.00268132952085
