ele:0.0302029509926
game:0.0295330994568
player:0.0273291645117
games:0.0268659822287
agent:0.0235607671232
payo:0.0226968899969
equilibrium:0.0214426817269
learning:0.0188176587982
payos:0.0165861888439
agents:0.0146026151862
nash:0.0139318855054
players:0.0135606927371
pareto:0.0114702208925
reward:0.0109214997824
prole:0.0107955020165
monitoring:0.00961269153102
adversary:0.00922993897275
policy:0.00900175851813
reinforcement:0.00862941456932
payments:0.00785661576814
irrational:0.00785661576814
imperfect:0.00757809332145
maximin:0.00698365846057
rewards:0.00678025430446
economically:0.00644377188906
stochastic:0.00598222032514
action:0.00574787777448
normative:0.00572779723472
punishment:0.00523774384543
cient:0.00505045432015
actions:0.0043650782962
rationality:0.00429584792604
joint:0.00398814688342
played:0.00383740257074
play:0.00381952481129
g2:0.00338194687654
g1:0.00338194687654
deviate:0.00334642830755
perfect:0.00333385690785
attain:0.00325277918684
repeated:0.00323517006638
rational:0.0031651414276
folk:0.00309597455675
multiagent:0.00309508192398
deviates:0.00308193377476
mix:0.00298464644391
ai:0.00277958583123
equilibria:0.00269887550412
mixing:0.00264273560113
denition:0.00261945899903
punish:0.00261887192271
theorists:0.00261887192271
bowling:0.00261887192271
cooperative:0.00255545765759
converge:0.00253757660401
descriptive:0.00246636486109
convergence:0.00238132636275
defect:0.00237653701674
histories:0.00236815416295
plays:0.00235829810193
rg:0.00232341370489
veloso:0.00232198091756
economics:0.00229926311208
bayesian:0.00213615367356
stick:0.00202415662809
cooperate:0.00193254107231
policies:0.0018950686998
deviation:0.00187611912068
resp:0.00182034781798
prescribed:0.00179284417357
shneidman:0.00174591461514
parkes:0.00174591461514
xed:0.00171639142957
polynomial:0.00166987197151
ciently:0.00164081905896
whom:0.00158564136068
ciency:0.0015603985334
probabilistic:0.00155186802315
social:0.00147981891665
stipulates:0.00143194930868
monetary:0.00143194930868
settings:0.00140976478001
spirit:0.00138979291562
surplus:0.00134943775206
playing:0.00129996688203
payoff:0.00128529617946
history:0.0012750921358
strategies:0.00125639955552
payment:0.0012327735099
adopt:0.00122599387827
articial:0.00118826850837
rst:0.00115412891243
knows:0.00113947053812
adopted:0.00113431093503
modied:0.00112387740633
maximizes:0.00110932933642
dened:0.00110187177658
shot:0.00108485541897
justied:0.00108485541897
strategic:0.00108485541897
quickly:0.00107696944335
dierent:0.00104984475316
stationary:0.00102995187682
behave:0.00102007370864
economic:0.00100827598357
online:0.00100603153711
outcome:0.00100603153711
response:0.00100351850957
matrix:0.000986545944435
individually:0.000972105381823
initially:0.000964709298842
max:0.000955561500626
jeffrey:0.00091245454157
mixed:0.000900512529428
dierences:0.000896422086786
people:0.000891019462755
motivation:0.000881497421266
attained:0.000881177799181
unknown:0.000879691209265
covergent:0.000872957307571
deviators:0.000872957307571
conitzer:0.000872957307571
rby:0.000872957307571
thuc:0.000872957307571
unplayed:0.000872957307571
irrationality:0.000872957307571
deviator:0.000872957307571
enforceable:0.000872957307571
knowng:0.000872957307571
noncooperative:0.000872957307571
vorobeychik:0.000872957307571
fassumed:0.000872957307571
awesome:0.000872957307571
deviating:0.000872957307571
tuomas:0.000872957307571
polynominal:0.000872957307571
satinder:0.000872957307571
punishing:0.000872957307571
ramications:0.000872957307571
osprings:0.000872957307571
groves:0.000872957307571
undiscounted:0.000872957307571
iterations:0.000870890571892
paid:0.000866644588023
complements:0.000866644588023
interactions:0.000863485537079
technically:0.000839454937986
67:0.000837248419843
dynamics:0.000837248419843
themselves:0.000827747516229
associates:0.000826690885291
learns:0.000826690885291
visit:0.000826690885291
learn:0.000820409529479
adopting:0.000814419974446
existence:0.000811490778973
appealing:0.000802603388935
yevgeniy:0.000773993639186
boella:0.000773993639186
conver:0.000773993639186
vickrey:0.000773993639186
devoid:0.000773993639186
leendert:0.000773993639186
torre:0.000773993639186
pretend:0.000773993639186
prisoner:0.000773993639186
decreased:0.000769553628812
setting:0.000760352219929
termed:0.000739552890944
rigorous:0.000730130759312
strict:0.000728449053349
dene:0.00072138369774
designer:0.000720968594069
equated:0.00071597465434
opponents:0.00071597465434
wellman:0.00071597465434
shoham:0.00071597465434
faithfulness:0.00071597465434
psychology:0.00071597465434
instructed:0.00071597465434
sandholm:0.00071597465434
nv:0.00071597465434
nding:0.00071205122452
exploration:0.000703364761689
average:0.000701911672733
accomplish:0.000694896457808
autonomous:0.000694896457808
uncertainty:0.000678568327047
yoav:0.000674718876031
paramount:0.000674718876031
foe:0.000674718876031
ages:0.000674718876031
outset:0.000674718876031
vu:0.000674718876031
pv:0.000674718876031
gence:0.000674718876031
recommending:0.000674718876031
guido:0.000674718876031
incentive:0.000674718876031
utility:0.000662983423563
the game:0.0188704936988
stochastic games:0.0174942813126
repeated games:0.0165223767952
player 1:0.0145785677605
the agents:0.0138383620458
imperfect monitoring:0.0136066632431
policy prole:0.0136066632431
average reward:0.0136066632431
player 2:0.0136066632431
pareto ele:0.0136066632431
an ele:0.0136066632431
perfect monitoring:0.0126347587258
nash equilibrium:0.0122231420074
for player:0.0116628542084
learning in:0.0111010695232
learning equilibrium:0.010690949691
e cient:0.0103605838642
the players:0.0100633496108
joint action:0.00971904517367
reinforcement learning:0.00964840238135
the agent:0.00962388538121
a nash:0.00960389729153
a game:0.0092892457946
learning algorithms:0.0091018365789
a policy:0.00899835397843
game is:0.00890621758278
economically e:0.0087471406563
the adversary:0.00868007877429
cient learning:0.00785773414761
probabilistic maximin:0.00777523613894
learning algorithm:0.00759506892751
in games:0.00733688679406
of repeated:0.00723682482097
equilibrium of:0.00698465257565
a pareto:0.00698465257565
both players:0.00696693434595
repeated game:0.00680333162157
game g:0.00680333162157
monitoring setting:0.00680333162157
side payments:0.00680333162157
the learning:0.0065547268176
games with:0.00652167715028
r max:0.0061115710037
equilibrium in:0.0061115710037
multi agent:0.00604924347302
t mix:0.0058314271042
game matrix:0.0058314271042
adversary s:0.00572861257952
game in:0.00570646750649
an agent:0.00566110904777
each agent:0.00526314532434
other player:0.00523848943174
other agent:0.00519529358996
game theory:0.00501253600708
a learning:0.00494967454531
a deviation:0.00489125786271
ele in:0.00485952258683
payos in:0.00485952258683
of rewards:0.00485952258683
games is:0.00485952258683
the payo:0.00485952258683
s payo:0.00485952258683
ele for:0.00485952258683
normative approach:0.00485952258683
in equilibrium:0.00485952258683
reward of:0.00485952258683
of games:0.00485952258683
return mixing:0.00485952258683
best response:0.00485952258683
agent i:0.00472370444996
agents will:0.0046446228973
all agents:0.00445310879139
average sum:0.00436540785978
to deviate:0.00436540785978
mixing time:0.00436540785978
the player:0.00407604821892
possible histories:0.00407604821892
in stochastic:0.00404888952854
of learning:0.0040336780416
of actions:0.00394674551209
common interest:0.00388761806947
expected payo:0.00388761806947
maximin value:0.00388761806947
its payo:0.00388761806947
monitoring we:0.00388761806947
algorithms themselves:0.00388761806947
payo obtained:0.00388761806947
the ele:0.00388761806947
ele algorithm:0.00388761806947
agent initially:0.00388761806947
xed sum:0.00388761806947
ele does:0.00388761806947
initially plays:0.00388761806947
of ele:0.00388761806947
game m:0.00388761806947
the payos:0.00388761806947
always play:0.00388761806947
converge to:0.0037895860268
agent s:0.00377409873976
sum game:0.00349232628783
in repeated:0.00349232628783
agent 1:0.00349232628783
the rewards:0.00349232628783
agent reinforcement:0.00349232628783
equilibrium and:0.00349232628783
of pareto:0.00349232628783
this policy:0.00328946582771
player s:0.00326083857514
non cooperative:0.00326083857514
the denition:0.00318787195821
to learning:0.00314508228313
games in:0.00309641526487
on learning:0.00308237248419
machine learning:0.00298507130461
the policy:0.00297109414227
desired value:0.00296873919426
a repeated:0.00296873919426
of possible:0.00296679850101
rg m:0.0029157135521
sum stochastic:0.0029157135521
payos obtained:0.0029157135521
individually rational:0.0029157135521
and player:0.0029157135521
payo it:0.0029157135521
game theorists:0.0029157135521
players play:0.0029157135521
strict imperfect:0.0029157135521
ele where:0.0029157135521
equilibrium a:0.0029157135521
nash ele:0.0029157135521
prole that:0.0029157135521
self play:0.0029157135521
equilibrium ele:0.0029157135521
a normative:0.0029157135521
folk theorems:0.0029157135521
both agents:0.0029157135521
in economics:0.0029157135521
game associated:0.0029157135521
ele is:0.0029157135521
monitoring case:0.0029157135521
bowling and:0.0029157135521
ele exists:0.0029157135521
equilibrium had:0.0029157135521
and veloso:0.0029157135521
its agent:0.0029157135521
deviate from:0.00286430628976
adversary will:0.00286430628976
is played:0.00286430628976
in ai:0.00277591306659
known the:0.00271252461697
games and:0.00269925968569
agent to:0.00263157266217
player can:0.00261924471587
an economically:0.00261924471587
if player:0.00261924471587
general sum:0.00261924471587
the imperfect:0.00261924471587
games the:0.00261924471587
stick to:0.00261924471587
by agent:0.00261924471587
initially unknown:0.00261924471587
in game:0.00261924471587
adopted by:0.00257095827955
of convergence:0.00254975602107
t 0:0.00249253404444
g1 and:0.00244562893135
a player:0.00244562893135
the perfect:0.00241969738921
polynomial in:0.00240815291558
e ciently:0.00239188567715
the return:0.00233696506367
no agent:0.00232231144865
ai and:0.00232231144865
s actions:0.00232231144865
of equilibrium:0.00232231144865
deviates from:0.00232231144865
behave according:0.00232231144865
the probabilistic:0.00228146107084
the spirit:0.00226444361911
policy for:0.00226444361911
spirit of:0.0022312697783
times if:0.0022265543957
always exist:0.0022265543957
the expected:0.00221035942197
actions and:0.00219985535347
the action:0.00214936477597
its action:0.00214822971732
e ciency:0.002114489433
work on:0.00210200904256
1 resp:0.00208193479994
bayesian approach:0.00208193479994
in machine:0.00208193479994
for agent:0.00208193479994
for learning:0.00203980481686
denition in:0.00202444476427
polynomial number:0.00202444476427
with perfect:0.00202444476427
agent will:0.00202444476427
learning v:0.00202444476427
agent can:0.00197367949663
a nash equilibrium:0.00931034446984
of repeated games:0.00926837966509
economically e cient:0.00926837966509
e cient learning:0.00837931002286
the learning algorithms:0.00837931002286
cient learning equilibrium:0.0082385597023
learning in games:0.0082385597023
for player 2:0.00720873973951
a pareto ele:0.00720873973951
the other agent:0.00617891977673
a policy prole:0.00617891977673
in the game:0.00611244110208
the game is:0.00582502114215
the other player:0.00558620668191
return mixing time:0.00514909981394
normative approach to:0.00514909981394
the probabilistic maximin:0.00514909981394
sum of rewards:0.00514909981394
average reward of:0.00514909981394
of possible histories:0.00514909981394
in stochastic games:0.00514909981394
perfect monitoring setting:0.00514909981394
reinforcement learning in:0.00514909981394
in a nash:0.00514909981394
average sum of:0.00465517223492
on learning in:0.00465517223492
equilibrium of the:0.00465517223492
approach to learning:0.00436602935863
the agents will:0.00411927985115
in repeated games:0.00411927985115
of an ele:0.00411927985115
the average reward:0.00411927985115
ele does not:0.00411927985115
learning algorithms themselves:0.00411927985115
adversary will always:0.00411927985115
of the players:0.00411927985115
probabilistic maximin value:0.00411927985115
the ele algorithm:0.00411927985115
the policy prole:0.00411927985115
a repeated game:0.00411927985115
the agent initially:0.00411927985115
the return mixing:0.00411927985115
agent initially plays:0.00411927985115
agent reinforcement learning:0.00411927985115
in a game:0.00411927985115
will always play:0.00411927985115
for player 1:0.00411927985115
the expected payo:0.00411927985115
be in equilibrium:0.00411927985115
repeated games with:0.00411927985115
nash equilibrium of:0.00411927985115
the adversary s:0.00387107012309
to learning in:0.00372413778794
if the agent:0.0034928234869
work on learning:0.0034928234869
a learning algorithm:0.00343701148726
case of repeated:0.0033285835098
on its own:0.00331705869042
the adversary will:0.00320109637869
learning equilibrium ele:0.00308945988836
games with perfect:0.00308945988836
ele in the:0.00308945988836
stochastic games is:0.00308945988836
player can observe:0.00308945988836
which an ele:0.00308945988836
a game in:0.00308945988836
bowling and veloso:0.00308945988836
to deviate from:0.00308945988836
the game matrix:0.00308945988836
not always exist:0.00308945988836
known the game:0.00308945988836
always exist in:0.00308945988836
xed sum game:0.00308945988836
the perfect monitoring:0.00308945988836
nash equilibrium had:0.00308945988836
if player 1:0.00308945988836
an ele does:0.00308945988836
sum stochastic games:0.00308945988836
learning v 67:0.00308945988836
adversary s payo:0.00308945988836
stick to their:0.00308945988836
game associated with:0.00308945988836
in non cooperative:0.00308945988836
strict imperfect monitoring:0.00308945988836
learning research in:0.00308945988836
perfect monitoring we:0.00308945988836
the joint action:0.00308945988836
the game g:0.00308945988836
payo obtained by:0.00308945988836
a normative approach:0.00308945988836
an ele exists:0.00308945988836
an economically e:0.00308945988836
if the game:0.00308945988836
repeated game m:0.00308945988836
have obtained in:0.00308945988836
the imperfect monitoring:0.00308945988836
with perfect monitoring:0.00308945988836
of pareto ele:0.00308945988836
converge to a:0.0029322021127
of a policy:0.00279310334095
in the imperfect:0.00279310334095
a policy for:0.00279310334095
that the game:0.00279310334095
a deviation from:0.00279310334095
mixing time of:0.00279310334095
multi agent reinforcement:0.00279310334095
in game theory:0.00279310334095
game in which:0.00279310334095
the learning algorithm:0.00265364695234
in machine learning:0.00261961761518
that the agents:0.00261961761518
the agent will:0.00261961761518
set of possible:0.00260054974122
the spirit of:0.00249921553601
behave according to:0.00249643763235
of the game:0.00249643763235
of multi agent:0.00249643763235
67 n 1:0.00249643763235
an e cient:0.00246624905483
existence of an:0.00246624905483
of actions a:0.00240082228402
of a learning:0.00240082228402
t t and:0.00232264207386
in multi agent:0.00232264207386
obtained in a:0.00225649361991
a polynomial number:0.00225649361991
deviation from the:0.00225649361991
polynomial number of:0.00219915158452
machine learning v:0.00219915158452
number of steps:0.00219005457281
of the adversary:0.00214853555868
to the value:0.00213273493774
multi agent systems:0.0021032250536
the agent s:0.0021032250536
v 67 n:0.0021032250536
the case of:0.00209578054719
all joint actions:0.00205963992558
with k actions:0.00205963992558
be economically e:0.00205963992558
both players play:0.00205963992558
only equilibrium of:0.00205963992558
player 2 is:0.00205963992558
pareto ele is:0.00205963992558
to their algorithms:0.00205963992558
the second action:0.00205963992558
its aim is:0.00205963992558
near optimal reinforcement:0.00205963992558
game theory literature:0.00205963992558
an ele in:0.00205963992558
player 1 plays:0.00205963992558
agents stick to:0.00205963992558
multi agent interaction:0.00205963992558
to r max:0.00205963992558
rewards are based:0.00205963992558
of games in:0.00205963992558
of strict imperfect:0.00205963992558
in an imperfect:0.00205963992558
parallel to that:0.00205963992558
every t t:0.00205963992558
learning in general:0.00205963992558
the actual game:0.00205963992558
discuss the extension:0.00205963992558
stochastic games stochastic:0.00205963992558
agent i in:0.00205963992558
imperfect monitoring settings:0.00205963992558
to general sum:0.00205963992558
algorithms in computer:0.00205963992558
each agent can:0.00205963992558
called folk theorems:0.00205963992558
1 deviates from:0.00205963992558
learning in cooperative:0.00205963992558
of reinforcement learning:0.00205963992558
joint actions have:0.00205963992558
had they known:0.00205963992558
theorems in economics:0.00205963992558
other player the:0.00205963992558
any perfect monitoring:0.00205963992558
equilibrium if for:0.00205963992558
will attain a:0.00205963992558
a perfect monitoring:0.00205963992558
game is g2:0.00205963992558
a game we:0.00205963992558
corresponding rg m:0.00205963992558
player 1 deviates:0.00205963992558
imperfect monitoring the:0.00205963992558
s average reward:0.00205963992558
by the players:0.00205963992558
t and game:0.00205963992558
1 denoted cooperate:0.00205963992558
long term average:0.00205963992558
failure of at:0.00205963992558
denition for player:0.00205963992558
in that equilibrium:0.00205963992558
and its payo:0.00205963992558
action leading to:0.00205963992558
folk theorems in:0.00205963992558
punishment can be:0.00205963992558
general sum stochastic:0.00205963992558
and player 2:0.00205963992558
speed of convergence:0.00205963992558
