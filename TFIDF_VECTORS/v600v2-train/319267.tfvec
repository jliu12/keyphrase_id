face:0.041243282278
recognition:0.0209211667013
eigenface:0.0157559131364
facial:0.014729442687
pcc:0.0145439198182
persons:0.0128327201764
training:0.0122104925132
views:0.0116427360186
classifier:0.0102517242996
confidence:0.010217401504
image:0.00942482751035
orl:0.00848395322729
cmf:0.00808473237129
classification:0.00655641920413
dimensionality:0.0065064150736
achermann:0.00646157926737
illumination:0.00644235302425
faces:0.00556912022333
template:0.00551333381209
databases:0.00537797563227
database:0.00532880834303
neural:0.00532118053199
rotations:0.0050945368729
bern:0.00508659607139
hmm:0.00468281436464
pixels:0.00448049412897
person:0.00447477817721
images:0.00424620122432
lattice:0.00393352223377
lines:0.00382888911795
brunelli:0.00363597995455
pose:0.00358981828149
imaging:0.00349350086514
neighbour:0.00346785057357
glasses:0.00346488530198
sampling:0.00291208472105
plane:0.00291117203614
elastic:0.00290965551559
pixel:0.00287921368048
normalises:0.00285905235428
nnc:0.00285905235428
rectilinear:0.00271737277666
connectionist:0.00271737277666
frontal:0.00260088793018
accessories:0.00258463170695
afr:0.00258463170695
classified:0.00254807413092
testing:0.00250711744057
ub:0.00250316850218
intensity:0.00229352455052
human:0.00228641604386
lighting:0.00227880433536
biometric:0.00222138465401
poses:0.00219489884279
segments:0.00216958014868
eigenfaces:0.00214899031187
fronto:0.00214899031187
radial:0.0020986410345
flattens:0.00208773709975
resampled:0.00208773709975
nearest:0.00206571246575
recognise:0.00203463842856
accuracy:0.002026993248
classifiers:0.00199760480324
rates:0.00198895768348
variance:0.00196050830189
robust:0.00195980483287
seconds:0.00191893623252
knee:0.00187312574586
poggio:0.00187312574586
matching:0.00186909226573
lawrence:0.00184414091
misclassification:0.00184118033588
head:0.00179601958246
templates:0.00176510416546
video:0.00168828125984
sphere:0.00165400014363
settings:0.00164094088518
firstly:0.00163079013833
decision:0.00162120321126
workers:0.0015955074701
undertaken:0.0015790709545
distinctive:0.00154804934236
varied:0.0015313498739
geometric:0.0014915087936
profile:0.00148730150174
experiment:0.00147298453395
classifications:0.00146692806602
identification:0.00145604236053
zhang:0.00144159076757
viewing:0.00143960684024
zygomatic:0.00142952617714
aeberhard:0.00142952617714
beards:0.00142952617714
maching:0.00142952617714
inten:0.00142952617714
illuminant:0.00142952617714
eyebrow:0.00142952617714
mensionality:0.00142952617714
abstractmuch:0.00142952617714
samaria:0.00142952617714
wavelet:0.00142050357282
doubled:0.00142050357282
authentication:0.00142050357282
versus:0.00139321254348
expressions:0.00138750841357
benchmark:0.00137654406296
auto:0.00135929739427
descriptors:0.00134072536328
amongst:0.00134072536328
max:0.00130806497779
secondly:0.00130347512992
markov:0.00129398313387
utilises:0.00129231585347
forensics:0.00129231585347
dimensionalities:0.00129231585347
centage:0.00129231585347
grey:0.0012894429211
recognize:0.00128928217792
varying:0.00126228959908
1d:0.00122942727333
score:0.00122248070135
expense:0.00122208233488
ranganath:0.00121199331818
baron:0.00121199331818
wear:0.00121199331818
exacting:0.00121199331818
randomised:0.00121199331818
olivetti:0.00121199331818
valentin:0.00121199331818
fourthly:0.00121199331818
combined:0.00120849522263
correctly:0.00119584564957
assignments:0.00119264023661
row:0.0011696461869
2d:0.00116030371413
uncluttered:0.00115496176733
bunke:0.00115496176733
fractionally:0.00115496176733
ten:0.00113689380071
hidden:0.00113305827455
var:0.00112416784742
females:0.00111069232701
parametrisation:0.00111069232701
cluded:0.00111069232701
classifying:0.00110794566978
psi:0.00110794566978
inherent:0.00109203177039
male:0.00107449515593
edelman:0.00107449515593
bidimensional:0.00107449515593
convolutional:0.00104386854987
chellappa:0.00104386854987
sity:0.00104386854987
quasi:0.00103914167073
nose:0.00101731921428
flatter:0.00101731921428
compounded:0.00101731921428
dmax:0.00101731921428
necessitating:0.000993883943489
comparative:0.000983380558442
hmms:0.000972905067397
cessing:0.000972905067397
turk:0.000972905067397
caption:0.000972905067397
computationally:0.00096099023866
assess:0.000940902190054
associative:0.00093722302497
optimised:0.000936562872928
pentland:0.000936562872928
resampling:0.000936562872928
prototypical:0.000920590167938
cluttered:0.000920590167938
boundaries:0.000918384351511
feature:0.000912556560258
curve:0.000909509424187
equi:0.000905790925552
misclassified:0.000905790925552
matched:0.000905322866695
networks:0.000884861673773
classify:0.000881888036147
exploits:0.000878630029844
boundary:0.000868983419947
gabor:0.000866962643393
summarised:0.000866962643393
percentage:0.000866364840779
controlled:0.000863557623684
outdoor:0.000855514678429
neighbours:0.000855514678429
superior:0.000853312138474
network:0.000849672466341
measure:0.000848768792864
face recognition:0.0354985503814
test lines:0.0219376443906
training lines:0.0172367205926
confidence measure:0.0162287485204
testing lines:0.0156697459933
measure factor:0.0156697459933
minimum confidence:0.014298089157
line dimensionality:0.014102771394
face database:0.0125357967946
face databases:0.0109688221953
lines n:0.0109688221953
face image:0.010635052849
image views:0.0100086624099
view recognition:0.0100086624099
combined face:0.00940184759599
per view:0.00904800017743
recognition algorithm:0.00873591331545
image based:0.00815373069514
training views:0.00783487299665
lattice lines:0.00783487299665
classification accuracy:0.00778477994593
line based:0.00775542872351
based face:0.00775542872351
facial expressions:0.00727339819232
recognition rates:0.00727339819232
dimensionality l:0.00626789839732
neighbour classifier:0.00626789839732
eigenface classifier:0.00626789839732
test views:0.00626789839732
face class:0.00626789839732
achermann et:0.00626789839732
pcc versus:0.00626789839732
correctly classified:0.0061051225474
image representation:0.00590836269392
l j:0.00584022961671
correct classification:0.00565911311668
neural network:0.00555632275706
two persons:0.0053981694006
c p:0.00534285283803
line segments:0.00516796951615
class f:0.00508760212283
zhang et:0.00499346108319
versus time:0.00484893212821
nearest neighbour:0.00472669015513
recognition rate:0.00472669015513
classification pcc:0.00470092379799
confidence factor:0.00470092379799
cmf min:0.00470092379799
intermediate changes:0.00470092379799
template approach:0.00470092379799
face images:0.00462076080488
et al:0.00456028663337
feature based:0.00452985038619
human face:0.00444364802031
template based:0.00436795665773
initial testing:0.00428942674711
model face:0.00428942674711
benchmark algorithms:0.00428942674711
test view:0.00428942674711
image view:0.00428942674711
orl database:0.00428942674711
face view:0.00428942674711
lawrence et:0.00404862705045
based approach:0.0039744095167
imaging plane:0.00387771436176
viewing sphere:0.00387771436176
individual databases:0.00387771436176
elastic matching:0.00387771436176
al 20:0.00385350136989
facial features:0.00363669909616
view based:0.00346557060366
test time:0.00346557060366
lines l:0.00333273601523
al 1:0.00328012934631
seconds per:0.00322412302426
test image:0.00322412302426
initial number:0.00322412302426
classified persons:0.00313394919866
selected sampling:0.00313394919866
classifier performed:0.00313394919866
face boundaries:0.00313394919866
curve flattens:0.00313394919866
face rotations:0.00313394919866
imaging constraints:0.00313394919866
per face:0.00313394919866
k face:0.00313394919866
network 96:0.00313394919866
distinctive knee:0.00313394919866
test times:0.00313394919866
lattice line:0.00313394919866
accuracy increase:0.00313394919866
head pose:0.00313394919866
max selected:0.00313394919866
face classes:0.00313394919866
strict imaging:0.00313394919866
person database:0.00313394919866
lines set:0.00313394919866
flexible template:0.00313394919866
combined database:0.00313394919866
facial structures:0.00313394919866
superior compared:0.00313394919866
controlled illumination:0.00313394919866
threshold cmf:0.00313394919866
reduced recognition:0.00313394919866
quasi real:0.00313394919866
ten frontal:0.00313394919866
example misclassification:0.00313394919866
database using:0.00313222503365
radial basis:0.00309107168879
random sampling:0.0029499440964
neural networks:0.00292224562771
rectilinear line:0.00285961783141
k faces:0.00285961783141
achieved 100:0.00285961783141
1d line:0.00285961783141
illumination intensity:0.00285961783141
near 100:0.00285961783141
classification rate:0.00285961783141
auto associative:0.00285961783141
first increases:0.00285961783141
method achieved:0.00285961783141
selected 100:0.00285961783141
l 32:0.00285961783141
factor equal:0.00285961783141
current face:0.00285961783141
face boundary:0.00285961783141
confidence measure factor:0.0164751918729
number of training:0.0121151769454
face recognition algorithm:0.0100106423629
combined face database:0.00988511512374
number of testing:0.00906210169465
based face recognition:0.00858055059679
minimum confidence measure:0.00823759593645
number of test:0.00724521524028
training lines n:0.00659007674916
nearest neighbour classifier:0.00659007674916
pcc versus time:0.00659007674916
achermann et al:0.00659007674916
lines n k:0.00659007674916
line dimensionality l:0.00659007674916
total the number:0.00659007674916
values in total:0.00659007674916
changes in facial:0.00604140112977
c p 1:0.0054925341682
changes in illumination:0.00531577028291
zhang et al:0.00531577028291
geometric or image:0.00494255756187
correct classification pcc:0.00494255756187
lines was set:0.00494255756187
seconds per view:0.00494255756187
initial testing lines:0.00494255756187
line based face:0.00494255756187
minimum confidence factor:0.00494255756187
face and view:0.00494255756187
r s l:0.00494255756187
line based algorithm:0.00453105084733
image based approaches:0.00453105084733
c p 2:0.00429027529839
percentage of correctly:0.00429027529839
lawrence et al:0.00429027529839
robust to changes:0.00429027529839
et al 20:0.00417712621698
probability of correct:0.00411940062615
face recognition using:0.00387848146648
feature based approach:0.00378685380739
et al 1:0.00360578449998
set of random:0.00337448448903
views in quasi:0.00329503837458
class f g:0.00329503837458
robust to rotations:0.00329503837458
current face recognition:0.00329503837458
first increases rapidly:0.00329503837458
factor is smaller:0.00329503837458
classification accuracy increase:0.00329503837458
accuracy first increases:0.00329503837458
face image views:0.00329503837458
face class f:0.00329503837458
set to 300:0.00329503837458
class f k:0.00329503837458
test image views:0.00329503837458
two neural networks:0.00329503837458
achieved 100 correct:0.00329503837458
quasi real time:0.00329503837458
threshold cmf min:0.00329503837458
robust to variations:0.00329503837458
per view sec:0.00329503837458
text for explanation:0.00329503837458
dimensionality was set:0.00329503837458
ten frontal face:0.00329503837458
correctly classified persons:0.00329503837458
lattice lines l:0.00329503837458
k face classes:0.00329503837458
view for 5:0.00329503837458
recognition of views:0.00329503837458
2d face image:0.00329503837458
testing lines set:0.00329503837458
database see text:0.00329503837458
test lines n:0.00329503837458
recognition performance results:0.00329503837458
max selected 100:0.00329503837458
selected 100 0:0.00329503837458
lines is doubled:0.00329503837458
flexible template approach:0.00329503837458
class c p:0.00329503837458
neural network 96:0.00329503837458
hidden markov models:0.00319252946716
training and test:0.00305316514339
factor was set:0.00302070056488
template based approach:0.00302070056488
line l j:0.00302070056488
rectilinear line segments:0.00302070056488
image based face:0.00302070056488
frontal face images:0.00302070056488
algorithm is robust:0.00302070056488
lines l j:0.00302070056488
experiment the number:0.00302070056488
