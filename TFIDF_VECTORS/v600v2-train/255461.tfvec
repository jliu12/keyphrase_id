pruning:0.0523095765964
rep:0.0417159498708
tdp:0.0264802706146
fossil:0.0193405843595
learning:0.0138587303158
theories:0.0116519557325
overfitting:0.0112091301828
cutoff:0.0110844584906
training:0.00985125598893
post:0.00915797337417
foil:0.00873287608912
grow:0.00830237074488
clause:0.00823460665268
conquer:0.00796483239951
learned:0.00777712850531
krk:0.0071933685002
growing:0.00627789463535
literals:0.00604199647426
noise:0.00544529743574
clauses:0.00540313666429
stnd:0.0052548174251
noisy:0.0050537480116
accuracy:0.00480792910653
quinlan:0.00457950793191
furnkranz:0.00452102181225
pruned:0.00439217690911
literal:0.0038881886937
predictive:0.00378169984806
rule:0.00370533411712
dev:0.00368312895512
bratko:0.00350321161673
stopping:0.00340218880292
pre:0.00339672048551
cohen:0.00337132768296
separateandconquer:0.00322930129447
pruningset:0.00322930129447
lymphography:0.00291934301394
endgame:0.00291934301394
domains:0.00291390061906
covered:0.00257695072478
rules:0.00256962740932
brunk:0.00250905525671
mushroom:0.00250905525671
niblett:0.00233547441115
dzeroski:0.00233547441115
cameron:0.00229812528661
correlation:0.00228829759609
pazzani:0.00224518498268
holte:0.00219779367722
secs:0.00214027387212
datasets:0.00212009010698
phase:0.0020660159625
criterion:0.00203763699873
chess:0.00198587872606
dolsak:0.00193758077668
splitratio:0.00193758077668
muggleton:0.00193758077668
growingset:0.00193758077668
pagallo:0.00193758077668
king:0.0019326086522
climbing:0.0017835615601
frnkranz:0.00175160580837
classification:0.00167428227618
rook:0.00164273658806
negative:0.00163694587204
heuristic:0.001636579752
concept:0.00160669515088
prolog:0.00155727074019
prune:0.0015353271962
learn:0.00153102296553
relational:0.00148784939809
decision:0.00146491997487
votes:0.00145717410664
austrian:0.00145637148313
cover:0.00142860645512
separate:0.00141416837786
learns:0.00141296392645
haussler:0.00137887517197
illegal:0.00137275243741
propositional:0.00136546785045
overly:0.00135060708859
hill:0.00132216575732
learner:0.00129362515237
euthyroid:0.00129172051779
elc:0.00129172051779
jones:0.00128234575924
clark:0.00126422200917
explanations:0.00125490768832
faster:0.001250369883
exit:0.00124117168664
cpu:0.00122766121474
log:0.00120948210433
prunes:0.00119152723564
luaces:0.00116773720558
ranilla:0.00116773720558
overfits:0.00116773720558
bahamonde:0.00116773720558
incremental:0.00116280026281
johannes:0.00115956519132
regularities:0.00114487698298
instances:0.00113749492322
avoidance:0.00109845628108
hepatitis:0.00109515772537
bias:0.00107970131404
sick:0.00104362398952
sparcstations:0.00104362398952
stop:0.00103974217495
white:0.00103430973886
asymptotic:0.0010272977665
setup:0.000996093048958
background:0.000977951116317
olshen:0.000970914322086
michie:0.000970914322086
growth:0.000967312336122
deliberately:0.00096267576625
artificial:0.000960924757567
accuracies:0.000955328372502
top:0.000943209308712
generalize:0.000939026606601
relations:0.000930677664453
merits:0.000927703463523
induction:0.00092747861138
michalski:0.000919250114644
covers:0.000910623278786
widmer:0.000898073993072
breast:0.000898073993072
deleting:0.00089595088529
mesh:0.000888809158967
tries:0.000888809158967
incompatibility:0.000879117470889
commonly:0.000869812080228
greedy:0.000850199656054
breiman:0.000846278647082
cart:0.000818473101137
domain:0.000811903613497
significance:0.000811895329619
sizes:0.000810296415695
confirms:0.000801709047713
tree:0.000795568356633
cancer:0.000794351490425
intermediate:0.000793524757733
oscar:0.00078338784734
aq:0.00078338784734
differences:0.000780931579388
inductive:0.000773849868898
uci:0.00077304346088
inefficiency:0.00077304346088
heuristics:0.000772325958383
prematurely:0.000763251321984
learnable:0.000763251321984
disjuncts:0.000763251321984
unseen:0.000763251321984
glass:0.000763251321984
costs:0.000761824539911
tested:0.000760565433823
caught:0.000736661458905
undone:0.00072858705332
fastest:0.000726654028091
entirely:0.000723626021755
slower:0.000723445640846
deletion:0.000720077692271
fires:0.000713424624041
greedily:0.00070628462708
wherever:0.000699409052473
mere:0.000699409052473
post pruning:0.0372207377952
pre pruning:0.0148561132228
pruning set:0.0116265233917
pruning phase:0.0116265233917
rule learning:0.00920344114587
reduced error:0.00875881156766
training set:0.00858046932625
error pruning:0.00853520422739
specific theory:0.00778671008903
rep grow:0.00778671008903
krk domain:0.00707882735366
final theory:0.00707882735366
cohen 1993:0.00707882735366
positive examples:0.00673615676674
pruning algorithm:0.00670623189295
pruning algorithms:0.00670623189295
pruning methods:0.00657153346325
conquer rule:0.00645917966207
concept description:0.0064058645705
dev range:0.00637094461829
accuracy stnd:0.00637094461829
starting theory:0.00637094461829
stnd dev:0.00637094461829
range time:0.00637094461829
predictive accuracy:0.0062181110858
negative examples:0.00605104717539
noisy domains:0.00581326169587
learning algorithms:0.0057079069456
cutoff parameter:0.00566306188293
initial rule:0.00548691700332
set sizes:0.00517220216493
time fossil:0.00516734372966
growing phase:0.00516734372966
decision tree:0.00502015151174
incremental reduced:0.00495517914756
run times:0.00495066295917
conquer learning:0.00452142576345
pruning approaches:0.0042472964122
training examples:0.0041596001423
stopping criterion:0.0039860873678
tree learning:0.00394765400858
cpu secs:0.00365794466888
run time:0.00359251650533
empty theory:0.00353941367683
cameron jones:0.00353941367683
rule growth:0.00353941367683
overfitting theory:0.00353941367683
quinlan 1990:0.00353941367683
furnkranz 1994:0.00353941367683
jones 1994:0.00353941367683
algorithm foil:0.00353941367683
return theory:0.00353941367683
positive 99:0.00353941367683
pazzani 1991:0.00322958983104
overly specific:0.00322958983104
theory figure:0.00304828722407
growing set:0.00304828722407
background knowledge:0.00295984482597
learning algorithm:0.00289897263113
pruning tdp:0.00283153094146
bratko 1992:0.00283153094146
initial top:0.00283153094146
foil quinlan:0.00283153094146
initial theory:0.00283153094146
simpler theory:0.00283153094146
subsequent clauses:0.00283153094146
integrating pre:0.00283153094146
foil 6:0.00283153094146
intermediate theory:0.00283153094146
rep 97:0.00283153094146
overfitting phase:0.00283153094146
pruning heuristics:0.00283153094146
rule growing:0.00283153094146
overfitting avoidance:0.00258367186483
maximum correlation:0.00258367186483
noise handling:0.00258367186483
learning decision:0.00250927943963
asymptotic complexity:0.00250927943963
training instances:0.00250927943963
holte 1993:0.00243862977926
machine learning:0.00226762493778
data sets:0.0022609442786
noisy examples:0.00225580229062
hill climbing:0.00224538558891
conquer strategy:0.00213528819017
search heuristic:0.00213528819017
grow fossil:0.0021236482061
initial overfitting:0.0021236482061
pruning sets:0.0021236482061
artificial noise:0.0021236482061
growth rep:0.0021236482061
pruning rep:0.0021236482061
domain initial:0.0021236482061
quinlan 1994:0.0021236482061
fossil foil:0.0021236482061
subsequent post:0.0021236482061
haussler 1990:0.0021236482061
krk endgame:0.0021236482061
pruning decisions:0.0021236482061
examples growingset:0.0021236482061
pruning time:0.0021236482061
growingset pruningset:0.0021236482061
mesh design:0.0021236482061
examples splitratio:0.0021236482061
negative cover:0.0021236482061
grow algorithm:0.0021236482061
pruning criterion:0.0021236482061
separate and conquer:0.0178605153965
reduced error pruning:0.00955120091935
post pruning algorithms:0.0074418814152
post pruning phase:0.0074418814152
pre and post:0.00707556508361
accuracy stnd dev:0.00669769327368
conquer rule learning:0.00669769327368
stnd dev range:0.00669769327368
dev range time:0.00669769327368
training set sizes:0.00660314999958
range time fossil:0.00595350513216
faster than rep:0.00595350513216
rep and grow:0.00520931699064
top down search:0.00520931699064
top down pruning:0.00446512884912
pruning i rep:0.00446512884912
incremental reduced error:0.00446512884912
decision tree learning:0.00420200454519
rule learning algorithms:0.00409337182258
complete and consistent:0.00387585410301
return theory figure:0.0037209407076
post pruning methods:0.0037209407076
post pruning algorithm:0.0037209407076
brunk and pazzani:0.0037209407076
grow i rep:0.0037209407076
initial rule growth:0.0037209407076
cameron jones 1994:0.0037209407076
rule learning algorithm:0.00341114318548
general to specific:0.00310123762625
foil quinlan 1990:0.00297675256608
initial rule growing:0.00297675256608
domain with 10:0.00297675256608
rule growing phase:0.00297675256608
post pruning approaches:0.00297675256608
good starting theory:0.00297675256608
dzeroski and bratko:0.00297675256608
growing and pruning:0.00258390273534
number of finite:0.00258390273534
terms of accuracy:0.002480990101
positive and negative:0.00230957581381
generate all theories:0.00223256442456
growth rep grow:0.00223256442456
positive 99 67:0.00223256442456
clark and niblett:0.00223256442456
loop exit loop:0.00223256442456
pre pruning algorithm:0.00223256442456
learned by fossil:0.00223256442456
pagallo and haussler:0.00223256442456
error pruning rep:0.00223256442456
conquer learning strategy:0.00223256442456
overfitting the noise:0.00223256442456
learning and pruning:0.00223256442456
domain initial rule:0.00223256442456
rep grow fossil:0.00223256442456
pruning and learning:0.00223256442456
pre pruning heuristics:0.00223256442456
foil 6 1:0.00223256442456
examples growingset pruningset:0.00223256442456
grow fossil foil:0.00223256442456
rule growth rep:0.00223256442456
integration of pruning:0.00223256442456
fossil foil 6:0.00223256442456
subsequent post pruning:0.00223256442456
rule learning systems:0.00204668591129
average run time:0.00204668591129
increasing training set:0.00204668591129
avoidance as bias:0.00204668591129
rules and conditions:0.00204668591129
p n p:0.00180085909079
description length principle:0.00180085909079
inductive logic programming:0.00180085909079
needed to encode:0.00180085909079
