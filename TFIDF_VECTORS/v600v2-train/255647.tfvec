pursuer:0.033009475094
learning:0.0264087747505
game:0.0251996108837
ga:0.0242027137031
games:0.0223720822914
nn:0.0223355609922
lazy:0.0170720962068
evasion:0.0130975332318
reinforcement:0.0116591431896
evasive:0.0105329488263
pursuers:0.00983075223789
genetic:0.00958803526776
maneuvers:0.00888716637146
evader:0.00888716637146
grefenstette:0.00738922856239
gll:0.00698277357758
pursuit:0.00667399423582
players:0.00628589240661
payoff:0.00581740783164
differential:0.00561734646563
editing:0.00531516129736
teacher:0.00497422722408
actions:0.00482753656713
nearest:0.00466760723854
reward:0.00440504948056
pedestrian:0.00439384554833
action:0.00439049696282
robot:0.00432692411313
evades:0.00421317953053
fitness:0.00383274121309
evade:0.00380878558777
agent:0.00359331886854
plan:0.00349081786049
training:0.00342737587673
car:0.0034109232043
neighbor:0.00324893720165
chauffeur:0.00317398798981
success:0.00299337784399
player:0.0028584998431
tesauro:0.00283663995016
atkeson:0.00283663995016
learner:0.00281292789325
homicidal:0.00280878635368
smoke:0.00272791214068
watkins:0.00272791214068
learn:0.00270492497196
barto:0.00263901020085
salzberg:0.00263901020085
play:0.00246910038957
escape:0.00240536810685
backgammon:0.00238136967851
eager:0.00232696313265
samuel:0.00230367474431
gle:0.00226931196013
playing:0.0022450761187
teaching:0.00223873987837
train:0.00210172872011
angle:0.00207324874479
database:0.0020664950122
population:0.00198158059467
strength:0.00197277380067
bootstrapping:0.00191160016545
plateau:0.00191160016545
markov:0.00190684974492
sheppard:0.00190439279388
widrow:0.00190439279388
clouse:0.00190439279388
delayed:0.00189331074611
curvature:0.00184345471341
sutton:0.00180880989966
agents:0.00179440517798
isaacs:0.00178602725888
rules:0.00177785224202
task:0.00173538961251
chess:0.00172728047253
strategies:0.00165972476425
edited:0.00163943961347
fired:0.00162019697967
utgoff:0.00158340612051
classifier:0.00157366955389
neighbors:0.00157309899135
planning:0.00155177532415
tasks:0.00154706001516
ramsey:0.00153827389705
plans:0.0015272490997
aha:0.00149915005341
temporal:0.00149887037437
learned:0.00149214751924
radius:0.00148852763133
wilson:0.00146600549848
markovian:0.00141769838972
outperformed:0.00141769838972
ritter:0.00140571361413
teach:0.00140571361413
othello:0.00140439317684
metagamer:0.00140439317684
devijver:0.00140439317684
colombetti:0.00140439317684
simulator:0.0013860756584
rewards:0.00135660742475
bad:0.00130305661815
stored:0.00128113270897
td:0.00127758040436
prioritized:0.00127758040436
decision:0.00127416021593
torras:0.00126959519592
ase:0.00126959519592
dorigo:0.00126959519592
tomek:0.00126959519592
millan:0.00126959519592
averaging:0.00126500102269
wins:0.00124474094905
speeds:0.00123819580909
sweeping:0.0012295797101
bearing:0.0012295797101
rule:0.0012290456712
difficulty:0.00122384013965
ace:0.00119068483925
gammon:0.00119068483925
mutation:0.00118820906573
neural:0.00118809704403
stores:0.00115651250763
parking:0.00113465598006
maze:0.00113465598006
maneuvering:0.00113465598006
competition:0.00112981129085
achieving:0.00111946553571
generations:0.00110927513058
sharply:0.00110927513058
chromosome:0.00109116485627
opponents:0.00109116485627
poor:0.00109078261013
attributes:0.00108362367377
percent:0.00107105496148
holland:0.00105560408034
abilities:0.00105484795997
successful:0.00102888940437
fuel:0.00102551593137
mccallum:0.00102551593137
littman:0.00102551593137
alone:0.00101907784476
matcher:0.000999433368938
moore:0.00097475659596
remained:0.00097475659596
began:0.000968402867467
striking:0.000956069319802
credit:0.000956069319802
escaping:0.00093714240942
threshold:0.000927305390528
learns:0.000921727356706
storing:0.00090000864752
connectionist:0.000889865898109
cart:0.000889865898109
speed:0.000889724672387
perfect:0.000879302292615
randomly:0.000876766470161
consisted:0.000876310250039
goals:0.000862045823658
shaping:0.000829827299366
trials:0.000828302594133
classifying:0.000816349866745
pruning:0.000816349866745
comparator:0.000810098489835
heading:0.000810098489835
turns:0.000787050935025
rl:0.000783728147286
goldberg:0.000783728147286
navigate:0.000783728147286
strategy:0.000780000857693
climbing:0.000775654377551
aircraft:0.000760416272398
pole:0.000753207527232
gordon:0.000746246626124
tolerating:0.000733002749242
strategic:0.000726690896294
maximally:0.000720568745011
facing:0.000714624960775
sensing:0.000708849194861
gas:0.000703231973313
sequential:0.000703026410587
olsder:0.000702196588421
noncooperative:0.000702196588421
imado:0.000702196588421
basar:0.000702196588421
k nn:0.0424218293988
q learning:0.0220732905702
lazy learning:0.0178967092776
two pursuer:0.0130837964602
one pursuer:0.0115445262884
evasive maneuvers:0.0107748912025
differential games:0.0105339720388
reinforcement learning:0.00927580227903
differential game:0.008427177631
genetic algorithm:0.00807380316287
two pursuers:0.00692671577307
pursuer task:0.00692671577307
state action:0.00634859264719
pursuer game:0.00615708068717
nearest neighbor:0.00590974749021
k nearest:0.00556677168071
lazy learner:0.00538744560127
grefenstette et:0.00538744560127
turn angle:0.00538744560127
pursuit games:0.00538744560127
pursuit game:0.00538744560127
temporal difference:0.00507887411775
learning algorithms:0.00504224323002
action pairs:0.00491585361808
genetic algorithms:0.00475048273536
maneuvers task:0.00461781051538
delayed reinforcement:0.00461781051538
pursuer problem:0.00461781051538
good examples:0.00429202872198
control tasks:0.0042135888155
games one:0.00384817542948
single pursuer:0.00384817542948
lazy q:0.00384817542948
lazy approach:0.00384817542948
lazy methods:0.00384817542948
markov decision:0.00381944799725
difference learning:0.00380915558831
game theory:0.00369497319068
control problems:0.00361797122927
bad examples:0.00317429632359
nearest neighbors:0.00313981234112
percent success:0.00307854034359
random games:0.00307854034359
chauffeur game:0.00307854034359
traditional lazy:0.00307854034359
pursuer two:0.00307854034359
homicidal chauffeur:0.00307854034359
based learning:0.00307684666216
either method:0.00306573480141
optimal strategies:0.00297700126776
sequential decision:0.00283691551262
learning algorithm:0.0027578821621
perfect performance:0.00253943705887
learning approach:0.00253035076396
state space:0.00251038630932
continuous state:0.00245258784113
example set:0.00238160101421
examples stored:0.00230890525769
stored state:0.00230890525769
barto et:0.00230890525769
generated games:0.00230890525769
isaacs 1963:0.00230890525769
maneuvers game:0.00230890525769
salzberg 1993:0.00230890525769
successful evasion:0.00230890525769
evader e:0.00230890525769
player pursuit:0.00230890525769
plan fitness:0.00230890525769
rule strength:0.00230890525769
eager learning:0.00230890525769
complete game:0.00230890525769
pursuers figure:0.00230890525769
game e:0.00230890525769
reinforcement problems:0.00230890525769
prioritized sweeping:0.00230890525769
pursuer evader:0.00230890525769
e evades:0.00230890525769
multi agent:0.0023004655374
decision problems:0.00217490932399
success rate:0.00214536515072
storing examples:0.00210679440775
optimal play:0.00210679440775
payoff function:0.00210679440775
action pair:0.00210679440775
utgoff 1992:0.00210679440775
correct action:0.00210679440775
helpful teacher:0.00210679440775
instance based:0.00205123110811
watkins 1989:0.00198852325307
classifier systems:0.00198852325307
two player:0.00198852325307
credit assignment:0.00198852325307
learning methods:0.00191178586587
game playing:0.00190457779416
tesauro 1992:0.00190457779416
al 1990:0.00189268534352
near perfect:0.00183944088085
optimal strategy:0.0018243679386
two pursuer task:0.00566379738096
grefenstette et al:0.00566379738096
state action pairs:0.00519224178998
evasive maneuvers task:0.0048546834694
performance of k:0.00445049296284
temporal difference learning:0.00404616150106
differential game theory:0.00404556955783
lazy learning approach:0.00404556955783
two pursuer game:0.00404556955783
lazy q learning:0.00404556955783
radius of curvature:0.00391594561062
set of examples:0.00365839746092
k nearest neighbors:0.00357274859228
number of examples:0.00332348531534
markov decision problems:0.00326328800885
one pursuer two:0.00323645564626
homicidal chauffeur game:0.00323645564626
pursuer two pursuers:0.00323645564626
k nearest neighbor:0.00297729049357
instance based learning:0.00247968470435
two pursuers figure:0.0024273417347
lazy learning methods:0.0024273417347
two pursuer problem:0.0024273417347
delayed reinforcement problems:0.0024273417347
state action pair:0.0024273417347
examples to k:0.0024273417347
learning s performance:0.0024273417347
randomly generated games:0.0024273417347
player pursuit games:0.0024273417347
barto et al:0.0024273417347
evasive maneuvers game:0.0024273417347
approaches to reinforcement:0.0024273417347
games one pursuer:0.0024273417347
near perfect performance:0.0024273417347
game where e:0.0024273417347
two player pursuit:0.0024273417347
et al 1990:0.00205143039219
one and two:0.0019992551313
sequential decision making:0.00195797280531
threshold was set:0.00190476283037
set of actions:0.0016789147981
standard q learning:0.00161822782313
learning the task:0.00161822782313
cart and pole:0.00161822782313
lazy learning algorithm:0.00161822782313
see sheppard salzberg:0.00161822782313
two pursuer evasion:0.00161822782313
two lazy methods:0.00161822782313
class of differential:0.00161822782313
many bad examples:0.00161822782313
e is facing:0.00161822782313
either method could:0.00161822782313
success stored games:0.00161822782313
maze like environments:0.00161822782313
evader pursuer pursuer:0.00161822782313
one pursuer task:0.00161822782313
either method alone:0.00161822782313
using simulation models:0.00161822782313
continued to improve:0.00161822782313
problems widrow 1987:0.00161822782313
large continuous state:0.00161822782313
ramsey and grefenstette:0.00161822782313
nn and q:0.00161822782313
models and competition:0.00161822782313
neighbor k nn:0.00161822782313
approach stores complete:0.00161822782313
finding in non:0.00161822782313
exceeding the ga:0.00161822782313
k nn alone:0.00161822782313
