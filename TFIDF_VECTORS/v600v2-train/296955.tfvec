robot:0.0350778911054
trash:0.0323897926256
learning:0.0280068792294
pavlov:0.0199515506917
sensory:0.0192789624765
navigation:0.0176004061359
neural:0.015351559889
robots:0.0142039645141
theocharous:0.012261047961
sensor:0.0114434034096
khaleeli:0.0110349431649
recycling:0.0108479905081
occupancy:0.0104886173374
net:0.00992710200269
cans:0.00980883836878
opening:0.00935854473946
wall:0.00901827907786
mahadevan:0.00872554828122
training:0.00748067220381
mobile:0.00744139982108
bias:0.00737896005919
detectors:0.00737448118554
trained:0.00685338364312
learn:0.00617631828577
odometric:0.00613052398049
specularities:0.00552956784622
alvinn:0.00552956784622
pose:0.00483840912832
sensors:0.00476464781388
sonar:0.00473754187171
rapid:0.00463937313526
concept:0.00402621018444
specular:0.00394795155975
concepts:0.00393975447799
camera:0.00389505999543
quadrants:0.00388448499874
front:0.0038568488919
robotics:0.00374341789578
pomdp:0.00367831438829
door:0.00366887527884
lab:0.00357348586041
nomad:0.00332525844862
approximator:0.00311858050037
images:0.00308166469452
soda:0.00297183259364
east:0.00287485081501
grids:0.00285085545391
navigating:0.00282902440735
behaviors:0.00281349710489
human:0.00280151335865
supervised:0.0026982993158
color:0.00268716937905
layer:0.00268277868961
floor:0.00265552426199
actuator:0.00261766448437
labeled:0.00261573591095
observable:0.00251000700842
south:0.00250971626538
feedforward:0.00245451520649
sonars:0.0024522095922
receptacle:0.0024522095922
litter:0.0024522095922
receptacles:0.0024522095922
grdt:0.0024522095922
quickprop:0.0024522095922
reactive:0.00241049728816
teacher:0.00236877093585
feature:0.00234809965666
yellow:0.00233069099924
reflections:0.00231563481695
steering:0.00229521345311
epochs:0.00229521345311
grid:0.00222669606157
spatial:0.00222669606157
markov:0.00221969901899
recognize:0.00221163499788
planner:0.00220132516731
noisy:0.00218046909668
learned:0.00217119780509
near:0.0021697052693
colored:0.00213338256193
decision:0.00208575932141
undefined:0.00207376955903
hypotheses:0.00204156983914
reports:0.00201091911862
corridor:0.0019812217291
designer:0.00197867825011
tasks:0.00192951430219
hundred:0.00192839783053
hsi:0.00190528191915
invariances:0.00190528191915
doors:0.00190528191915
onboard:0.00190528191915
autonomous:0.0018226661261
predict:0.00172526290898
irregularities:0.00170490878637
observations:0.00166966055863
image:0.00165347914795
task:0.00162594066807
learns:0.00160942726209
decomposition:0.0016012303357
pursuit:0.0015537939995
effectiveness:0.00153512075357
decomposable:0.00153014230207
belief:0.00153012449528
reinforcement:0.00150800139626
robotic:0.00150800139626
nets:0.00141863614761
deposit:0.00141451220367
err:0.00139848231166
pulse:0.00138315381408
estimation:0.00136526883192
planning:0.00135477670982
despite:0.00134727932659
accelerating:0.00134081476101
coded:0.00133107044905
inputs:0.00132972530511
post:0.00131708703355
probabilistic:0.00131708703355
features:0.001306808681
uncertainty:0.00129174692236
pixels:0.00128097347412
action:0.00127770709612
strategies:0.00126789417757
trees:0.0012396446126
symbolic:0.00123645259694
office:0.00123475098221
months:0.00122791268067
hallways:0.0012261047961
recy:0.0012261047961
cling:0.0012261047961
preclassified:0.0012261047961
bumper:0.0012261047961
wiping:0.0012261047961
ojs:0.0012261047961
boada:0.0012261047961
turret:0.0012261047961
klingspor:0.0012261047961
challenging:0.00118720695006
detector:0.00112728488473
successful:0.00112283925733
testbed:0.00111292782082
trainer:0.00110841948287
subservient:0.00110841948287
grabber:0.00110841948287
percepts:0.00110841948287
openings:0.00110841948287
bleed:0.00110841948287
trainers:0.00110841948287
odometry:0.00110841948287
aligment:0.00110841948287
department:0.00110079183148
electrical:0.00109911316535
abstract:0.0010906437623
programmed:0.00107931972632
picking:0.00105447993915
acquire:0.00105447993915
train:0.00104852186342
layered:0.00104852186342
burden:0.00104265783193
occupied:0.00104265783193
dimensional:0.00104149787283
curve:0.00104011514204
hallway:0.00103952683346
thrun:0.00103952683346
obliquely:0.00103952683346
arguable:0.00103952683346
corridors:0.00103952683346
intersec:0.00103952683346
blanco:0.00103952683346
uncer:0.00103952683346
shallower:0.00103952683346
rote:0.00103952683346
lifelong:0.00103952683346
angled:0.00103952683346
navigated:0.00103952683346
architecture:0.0010233479658
percept:0.000990610864548
thru:0.000990610864548
grasping:0.000990610864548
slippage:0.000990610864548
tainty:0.000990610864548
neural net:0.025308382482
concept learning:0.0218175711052
mobile robots:0.0138709046994
mahadevan g:0.0134395650114
g theocharous:0.0134395650114
theocharous n:0.0134395650114
rapid concept:0.0134395650114
feature detectors:0.0127321708814
sensory concept:0.0120956085103
n khaleeli:0.0120956085103
local occupancy:0.0120956085103
occupancy grid:0.0107516520091
recycling task:0.00940769550798
spatial decomposition:0.00858418950713
sensory concepts:0.00806373900684
back opening:0.00806373900684
opening wall:0.00806373900684
robot learning:0.00806373900684
real robot:0.0073578767204
mobile robot:0.00727792221721
robot navigation:0.00709444520655
high dimensional:0.00691694222885
wall right:0.0067197825057
ff post:0.0067197825057
navigation system:0.0067197825057
trash cans:0.0067197825057
lab position:0.0067197825057
state estimation:0.00665164505956
multi output:0.00623821904332
occupancy grids:0.00613156393367
east navigation:0.00537582600456
net feature:0.00537582600456
labeled examples:0.00476401280548
training examples:0.00473834552597
right wall:0.00462988032051
human designer:0.00428277171105
overall control:0.00403186950342
front far:0.00403186950342
called pavlov:0.00403186950342
soda cans:0.00403186950342
specular reflections:0.00403186950342
ff prior:0.00403186950342
virtual sensors:0.00403186950342
abstract observation:0.00403186950342
opening back:0.00403186950342
camera turn:0.00403186950342
far near:0.00403186950342
learning sensory:0.00403186950342
wall back:0.00403186950342
decision trees:0.00400336323263
markov decision:0.00381121024439
state distribution:0.00374629157955
behavior based:0.00374629157955
nomad 200:0.0036789383602
predict features:0.0036789383602
engineering building:0.0036789383602
despite significant:0.0036789383602
output net:0.0036789383602
south east:0.00349081137683
shared representation:0.00347241024038
sensor values:0.00347241024038
neural network:0.00340394618438
decision tree:0.00340394618438
partially observable:0.00332582252978
navigation task:0.00332582252978
multiple concepts:0.00332582252978
position c:0.00321207878328
two general:0.00315889701731
engineering department:0.00311910952166
left right:0.0030608037364
hypothesis space:0.00297233672131
feedforward neural:0.00297233672131
figure shows:0.00284452627301
machine learning:0.00269076105849
significant sensor:0.00268791300228
significant odometric:0.00268791300228
task pavlov:0.00268791300228
pose c2:0.00268791300228
bits indicating:0.00268791300228
dimensional sensor:0.00268791300228
hundred real:0.00268791300228
odometric trace:0.00268791300228
alvinn system:0.00268791300228
navigation robot:0.00268791300228
sufficient bias:0.00268791300228
six boolean:0.00268791300228
abstract observations:0.00268791300228
robot testbed:0.00268791300228
turn behavior:0.00268791300228
alvinn 22:0.00268791300228
colored trash:0.00268791300228
robot navigating:0.00268791300228
belief state:0.00268791300228
robot pavlov:0.00268791300228
successful traces:0.00268791300228
right opening:0.00268791300228
output neural:0.00268791300228
sensor representation:0.00268791300228
trash receptacles:0.00268791300228
sensor reports:0.00268791300228
across related:0.00268791300228
accelerating sensory:0.00268791300228
reports feature:0.00268791300228
wall opening:0.00268791300228
sensory state:0.00268791300228
extremely challenging:0.00268791300228
post scale:0.00268791300228
khaleeli 2:0.00268791300228
category learning:0.00268791300228
speed sensory:0.00268791300228
hsi values:0.00268791300228
right far:0.00268791300228
several months:0.00268791300228
net correctly:0.00268791300228
learning 20:0.00268791300228
sonar pulse:0.00268791300228
robot called:0.00268791300228
trash receptacle:0.00268791300228
interesting concepts:0.00268791300228
find trash:0.00268791300228
entire floor:0.00268791300228
navigation architecture:0.00268791300228
output learning:0.00268791300228
three successful:0.00268791300228
g soda:0.00268791300228
original sensory:0.00268791300228
navigation domain:0.00268791300228
robot starting:0.00268791300228
door wall:0.00268791300228
almost totally:0.00268791300228
front left:0.00268791300228
figure 9:0.0026744337933
learning for mobile:0.0155429766558
rapid concept learning:0.014129978778
mahadevan g theocharous:0.014129978778
g theocharous n:0.014129978778
theocharous n khaleeli:0.0127169809002
sensory concept learning:0.0113039830224
local occupancy grid:0.00989098514459
back opening wall:0.007064989389
opening wall right:0.007064989389
using a shared:0.00569885026724
south east navigation:0.0056519915112
neural net feature:0.0056519915112
navigation to lab:0.0056519915112
wall right wall:0.0056519915112
opening back opening:0.0042389936334
wall back opening:0.0042389936334
running the robot:0.0042389936334
state estimation procedure:0.0042389936334
trained to recognize:0.0042389936334
net is able:0.0042389936334
multi output net:0.0042389936334
right wall back:0.0042389936334
near very near:0.0042389936334
mobile robot navigation:0.0042389936334
learning sensory concepts:0.0042389936334
dimensional and noisy:0.0042389936334
data is often:0.00388606413866
detect and move:0.0028259957556
trained neural net:0.0028259957556
behavior based layer:0.0028259957556
due to specularities:0.0028259957556
strategies provide sufficient:0.0028259957556
camera turn behavior:0.0028259957556
shows the learning:0.0028259957556
decision process models:0.0028259957556
concepts from high:0.0028259957556
six boolean variables:0.0028259957556
investigate two general:0.0028259957556
sensor i reports:0.0028259957556
several hundred real:0.0028259957556
neural net correctly:0.0028259957556
lab position c:0.0028259957556
speed up learning:0.0028259957556
study how pavlov:0.0028259957556
requires the robot:0.0028259957556
concepts and behaviors:0.0028259957556
net to learn:0.0028259957556
real robot testbed:0.0028259957556
pavlov is required:0.0028259957556
n khaleeli 2:0.0028259957556
robot navigation architecture:0.0028259957556
shows an odometric:0.0028259957556
hundred real valued:0.0028259957556
learning feature detectors:0.0028259957556
feedforward neural net:0.0028259957556
curve for training:0.0028259957556
learning in robotics:0.0028259957556
extremely challenging problem:0.0028259957556
despite significant odometric:0.0028259957556
sensor and actuator:0.0028259957556
e g soda:0.0028259957556
trained on 872:0.0028259957556
net feature detectors:0.0028259957556
shows the neural:0.0028259957556
reports feature f:0.0028259957556
front back left:0.0028259957556
detectors for navigation:0.0028259957556
examples the neural:0.0028259957556
data collection requires:0.0028259957556
door wall opening:0.0028259957556
accelerating sensory concept:0.0028259957556
back left right:0.0028259957556
behavior based architecture:0.0028259957556
concepts simultaneously using:0.0028259957556
ff post scale:0.0028259957556
right opening back:0.0028259957556
robot called pavlov:0.0028259957556
high dimensional sensor:0.0028259957556
provide sufficient bias:0.0028259957556
three successful traces:0.0028259957556
decomposition and multi:0.0028259957556
could be trained:0.0028259957556
net was trained:0.0028259957556
robot navigation using:0.0028259957556
learning s mahadevan:0.0028259957556
net correctly predicts:0.0028259957556
net is trained:0.0028259957556
g soda cans:0.0028259957556
net feature detector:0.0028259957556
