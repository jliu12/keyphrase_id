undiscounted:0.0269814227799
policy:0.0184658070778
discounted:0.0171553691669
mdp:0.0122995304404
wandering:0.0116390451208
mixing:0.0115382579792
payo:0.0107665759578
mdps:0.0102771861416
payos:0.0101437656125
reinforcement:0.00934049102127
exploration:0.00813125480512
attempted:0.00811219719355
exploitation:0.00775483274291
ms:0.00757138404732
learning:0.00750933374358
return:0.00697501140787
markov:0.00669941922351
balanced:0.00563202014711
unichain:0.0047614275494
absorbing:0.00421318546579
multichain:0.00417684466397
explorations:0.00406108305272
policies:0.00402611999727
horizon:0.00396284665066
denition:0.00376782198705
actions:0.00372346223952
action:0.00369143389575
pr:0.00368208839111
asymptotic:0.00365030210861
transition:0.00362467711409
exploitations:0.00317428503294
visited:0.00303303742296
littman:0.00264523752745
singh:0.00243664983163
agent:0.0024335401807
probability:0.00242513634634
ergodic:0.00241215241871
ntg:0.00238676837941
visits:0.00233153596541
opt:0.00227477806722
decision:0.00226364896422
transitions:0.00211901052488
tsitsiklis:0.00196451778206
reward:0.00185382677636
sutton:0.00184476109466
states:0.00180385308105
strehl:0.00179007628456
unvisited:0.00179007628456
rewards:0.0017570757772
achievable:0.00174865197406
reaching:0.00174673076196
stationary:0.00171685204723
denitions:0.00171381952624
polynomial:0.00160682782913
rm:0.00158812186891
discounting:0.00158714251647
compete:0.00153182741068
unknown:0.00147282680721
barto:0.0014681694488
jaakkola:0.0014681694488
technical:0.00142644867399
walks:0.0014103865874
pigeonhole:0.00138357082099
watkins:0.00138357082099
near:0.0013657652124
probabilities:0.00135398626891
polynomially:0.00135203286559
halt:0.00134866656398
induced:0.00134078913405
nite:0.00128957310744
cherno:0.00126395563974
su:0.00125754664089
explore:0.00125260271422
innite:0.00124766482997
var:0.00122546192855
ij:0.0012082257047
farias:0.00119338418971
pucci:0.00119338418971
bertsekas:0.00117871066924
simulation:0.00113157973519
exploit:0.00110421062005
saul:0.00105809501098
daniela:0.00105809501098
dayan:0.00105809501098
redirected:0.00105809501098
paths:0.00103944850414
rst:0.00101889190999
targeted:0.00101149992299
easiest:0.00101149992299
appeal:0.00101149992299
trade:0.00100443303878
competing:0.000990711662665
mannor:0.000978779632531
shie:0.000978779632531
quickly:0.000966396467322
exceeds:0.000963615852158
ciently:0.000948596573117
distributions:0.000948596573117
failure:0.000939452035665
dene:0.000929420866942
replicate:0.000922380547328
cross:0.000921443994579
nm:0.000888565746976
settle:0.0008785378886
engaged:0.0008785378886
dierent:0.000858830482261
michael:0.000847604209951
eventually:0.000825025922481
nd:0.000825025922481
alexander:0.00082290467127
roy:0.000812216610545
gaining:0.000812216610545
executes:0.000811219719355
asynchronous:0.000811219719355
cumulative:0.000799933852688
sketch:0.000786957146742
bonn:0.000785807112824
fewest:0.000785807112824
visit:0.000768203391075
meantime:0.000762460851862
brie:0.000758259355741
exposition:0.000748598897985
variance:0.00074769884932
max:0.000746462285481
stochastic:0.000745914974928
pac:0.000741530710543
intuitions:0.000722555125344
banff:0.000722555125344
nished:0.000722555125344
xed:0.000717599886728
tom:0.000705193293701
executed:0.000703822834389
knows:0.000687652259925
bias:0.000679753118098
approaching:0.000674333281992
notion:0.000674333281992
analyses:0.00067202600243
hope:0.000663888581747
variances:0.000660474441777
backwards:0.000660474441777
discover:0.000657055685944
executing:0.000657055685944
upon:0.000653154499796
aspect:0.000651082752202
iteration:0.000637296171999
alberta:0.000635248747565
worst:0.00062454929443
mix:0.000612730964273
nding:0.000602659823266
traverse:0.000602311043589
heart:0.000602311043589
wiewiora:0.000596692094853
lihong:0.000596692094853
pieter:0.000596692094853
deshpande:0.000596692094853
ives:0.000596692094853
yishay:0.000596692094853
apprenticeship:0.000596692094853
abbeel:0.000596692094853
controllably:0.000596692094853
theologos:0.000596692094853
explo:0.000596692094853
dering:0.000596692094853
diuk:0.000596692094853
gullapalli:0.000596692094853
nimrod:0.000596692094853
fiechter:0.000596692094853
mery:0.000596692094853
conductance:0.000596692094853
unichains:0.000596692094853
youthful:0.000596692094853
rum:0.000596692094853
introduc:0.000596692094853
satinder:0.000596692094853
wanders:0.000596692094853
jalali:0.000596692094853
niranjan:0.000596692094853
chrisman:0.000596692094853
sarsa:0.000596692094853
bountourelis:0.000596692094853
erman:0.000596692094853
tommi:0.000596692094853
competes:0.000596692094853
zachary:0.000596692094853
reveliotis:0.000596692094853
episodic:0.000596692094853
ploitation:0.000596692094853
mixing time:0.0199288652496
discounted case:0.016607387708
undiscounted case:0.0161122540534
balanced wandering:0.014614501183
markov decision:0.0142039063901
known state:0.0132859101664
reinforcement learning:0.0116674945348
return mixing:0.00996443262479
mdp m:0.00996443262479
decision process:0.00996099861717
step return:0.00863584160815
known states:0.00797154609983
value iteration:0.00724353188545
asymptotic return:0.00664295508319
attempted explorations:0.00664295508319
horizon time:0.00656425165139
optimal policy:0.00636268661005
undiscounted return:0.00597865957487
absorbing state:0.00596750150126
optimal return:0.00465006855823
u ms:0.00465006855823
attempted exploitation:0.00465006855823
o line:0.00463495492734
learning algorithm:0.00458420238652
discounted return:0.00417725105088
near optimal:0.004134895467
let m:0.00403774191056
partial model:0.00398577304991
attempted exploration:0.00398577304991
attempted exploitations:0.00398577304991
multichain mdps:0.00398577304991
expected payos:0.00398577304991
ms ms:0.00358050090076
transition probabilities:0.00343945562
simulation lemma:0.0033214775416
step attempted:0.0033214775416
high return:0.0033214775416
r pr:0.0033214775416
policy whose:0.0033214775416
whose return:0.0033214775416
unknown mdp:0.0033214775416
ms ij:0.0033214775416
exploitation policy:0.0033214775416
general mdps:0.0033214775416
expected return:0.00298375075063
r max:0.00298375075063
every state:0.00294951677194
start state:0.0028459996049
main theorem:0.00282104762248
q learning:0.0027859738021
computation time:0.00266941263926
exploration policy:0.00265718203328
small transitions:0.00265718203328
state mdp:0.00265718203328
actual return:0.00265718203328
line computation:0.00265718203328
discounted cases:0.00265718203328
state becomes:0.00265718203328
m known:0.00265718203328
achieve near:0.00265718203328
exploitation exploration:0.00265718203328
mixing times:0.00265718203328
unichain case:0.00265718203328
exploit lemma:0.00265718203328
decision processes:0.00253641185538
u m:0.00252815723309
total computation:0.00244718715771
step policy:0.0023870006005
small transition:0.0023870006005
becomes known:0.0023870006005
polynomial bounds:0.0023870006005
simulation accuracy:0.0023870006005
optimal expected:0.00222877904168
current state:0.00220307883033
polynomial time:0.00212484332117
nite time:0.00211639591707
learning p:0.00206732513125
value function:0.00199664083142
algorithm executes:0.00199288652496
nm known:0.00199288652496
one small:0.00199288652496
l strehl:0.00199288652496
l littman:0.00199288652496
step discounted:0.00199288652496
o ntg:0.00199288652496
payo case:0.00199288652496
alexander l:0.00199288652496
v ms:0.00199288652496
discounted value:0.00199288652496
step undiscounted:0.00199288652496
known steps:0.00199288652496
asymptotic undiscounted:0.00199288652496
expected asymptotic:0.00199288652496
markov decision process:0.0119436455685
return mixing time:0.00985388497234
policy in m:0.00915003604574
number of actions:0.00763598632552
steps of balanced:0.00492694248617
states of m:0.00477446573358
explore or exploit:0.00422309355957
number of attempted:0.00422309355957
transitions in m:0.00422309355957
undiscounted and discounted:0.00422309355957
process and let:0.00417765751688
unknown mdp m:0.00351924463298
denition of known:0.00351924463298
probability at least:0.00322896017913
state becomes known:0.00281539570638
currently known states:0.00281539570638
policy for time:0.00281539570638
return in m:0.00281539570638
m that start:0.00281539570638
achieve near optimal:0.00281539570638
state i 2:0.00281539570638
o line computation:0.00281539570638
optimal t step:0.00281539570638
known state mdp:0.00281539570638
action a p:0.00281539570638
markov decision processes:0.00273480842706
number of visits:0.00227498010828
machine learning p:0.00226710326375
conference on machine:0.00219677215066
total computation time:0.00218784674165
ms ms ij:0.00211154677979
whose return mixing:0.00211154677979
discounted case let:0.00211154677979
model of m:0.00211154677979
step of balanced:0.00211154677979
time convergence results:0.00211154677979
p a ms:0.00211154677979
notion of mixing:0.00211154677979
targeted mixing time:0.00211154677979
undiscounted value iteration:0.00211154677979
asymptotic undiscounted return:0.00211154677979
actions and computation:0.00211154677979
approximation of m:0.00211154677979
reach the absorbing:0.00211154677979
least one small:0.00211154677979
alexander l strehl:0.00211154677979
one small transition:0.00211154677979
opt t m:0.00211154677979
set of currently:0.00211154677979
exploration trade o:0.00211154677979
reinforcement learning problems:0.00211154677979
cross at least:0.00211154677979
exploitation exploration trade:0.00211154677979
nite time convergence:0.00211154677979
michael l littman:0.00211154677979
pr r pr:0.00211154677979
two o line:0.00211154677979
source of failure:0.00211154677979
start in state:0.00211154677979
steps to approach:0.00211154677979
discounted value iteration:0.00211154677979
