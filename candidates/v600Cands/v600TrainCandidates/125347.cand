sentences
verb
connectionist
boy
chases
mcclelland
boys
nouns
grammatical
noun
representations
localist
verbs
sentence
linguistic
clause
hidden
elman
neural
1989
recurrent
girls
press
grammar
clauses
word
1988
trajectories
training
lexical
activation
kawamoto
cleeremans
servan
cognition
hinton
dog
network
agreement
cognitive
networks
chase
rumelhart
units
learning
girl
connectionism
dogs
ungrammatical
representational
learned
learn
trained
1986
hare
srn
schreiber
language
mary
pca
feed
sees
plural
pollack
who
dimensions
9d
prediction
phonology
miikkulainen
spoken
1990
principal
stimuli
encode
constituent
ended
neuroscience
relationships
taught
insert
syntactic
internal
constituency
pronoun
wermter
gasser
mental
henderson
8c
8d
predictions
corpus
apparently
richness
phenomena
filler
cats
object
items
smolensky
compositional
9c
cat
10d
broke
consisted
verbal
embedding
singular
regimen
kirsh
inanimate
macwhinney
maqsood
pylyshyn
ajith
kutas
imran
lexicon
dyer
paradox
demonstrations
nature
soft
predict
task
human
stefan
outset
0126
bates
persuasively
garfield
stinchcombe
underlined
regularities
learns
8b
st
sanger
touretzky
cottrell
fodor
eigenvectors
rules
structural
category
abstract
encodes
8a
mozer
gelder
contextually
lock
optional
accommodated
linguistics
facts
production
chomsky
sejnowski
skeletonization
pdp
sheila
ucsd
4c
jordan
phrases
theme
theories
subject
role
shortcoming
generalizations
houses
clausal
listeners
1987
possessed
cooccurrence
inputs
3f
rosenberg
causal
10c
minds
retract
emergent
tendency
agent
subordinate
dell
animate
subjects
trajectory
ow
4a
3a
feedforward
the network
direct object
relative clauses
in press
relative clause
argument structure
connectionist models
distributed representations
verb argument
chases boy
simple sentences
hidden units
hidden unit
of language
the hidden
a connectionist
about here
the verb
simple recurrent
insert figure
representations which
complex sentences
mcclelland in
structural relationships
main clause
through state
trajectories through
linguistic representations
internal representations
neural networks
connectionist networks
grammatical structure
elman 1990
the localist
network s
in connectionist
a verb
representations are
state space
who boys
recurrent network
dog who
ended nature
unit activation
boys who
schreiber cleeremans
john mcclelland
boy who
boy chases
cleeremans mcclelland
clause verb
localist approach
who chases
servan schreiber
complex structural
sentences were
open ended
the task
in simple
sentences the
constituent structure
words are
st john
nature of
the networks
hinton 1988
network predictions
localist representations
mary chases
verb in
who mary
in elman
sees girl
context units
network was
relationships such
of embedding
the trajectories
principal component
words in
of words
two sentences
mcclelland 1986
verb is
000 sentences
and verb
of connectionist
the nature
of linguistic
computational power
sentences which
sentences with
current work
the grammar
prediction task
clauses the
the sentences
the sentence
the apparently
the representational
the representations
boys chase
noun is
relative pronoun
underlined words
these sentences
feed cats
sentences there
representational space
mcclelland 1989
kawamoto mcclelland
initial noun
connectionist account
nouns are
language be
singular verb
pollack 1988
miikkulainen dyer
in sentence
apparently open
causal properties
spoken word
chases feed
connectionist model
can complex
words maximum
subject nouns
unit patterns
than hard
1986 miikkulainen
fixed resource
as constituency
rumelhart mcclelland
as nouns
mean sentence
the dimensions
sentences in
recurrent neural
hidden layer
each word
language processing
network to
natural language
learned to
to context
human cognition
stefan wermter
direct objects
sentence length
and 8d
recurrent networks
figure 9d
predictions following
demonstrations of
minimum 3
nouns and
of verb
sentences this
broke the
8c and
james henderson
for language
context dependent
network has
the agreement
agreement and
the training
d o
representations of
and 8b
lexical items
for sentences
abstract representations
the 70
8a and
classical models
resource system
net work
noun phrases
3 words
of complex
the net
encode the
noun and
sentences are
representations may
word recognition
limited resources
of hidden
for cognitive
ow can
has learned
to focus
provide for
the prediction
the relative
trained to
output units
internal states
word was
verb argument structure
through state space
mcclelland in press
the relative clause
the hidden unit
trajectories through state
the hidden units
a direct object
the main clause
in simple sentences
the network s
nature of language
st john mcclelland
ended nature of
simple recurrent network
complex structural relationships
boy chases boy
the localist approach
hidden unit activation
by the hidden
servan schreiber cleeremans
open ended nature
relative clauses the
schreiber cleeremans mcclelland
cleeremans mcclelland in
relationships such as
direct object and
requires a direct
boy who chases
10 000 sentences
main clause verb
who chases boy
structural relationships such
the direct object
state space for
the nature of
the current work
that the network
the network to
the net work
recurrent neural networks
such as constituency
presence of relative
mean sentence length
simple recurrent networks
apparently open ended
the trajectories through
and verb argument
a fixed resource
of the 70
kawamoto mcclelland 1986
the apparently open
language be accommodated
of language be
3 words maximum
the internal representations
boys who mary
accommodated by a
who mary chases
the agreement and
chases boy who
1986 miikkulainen dyer
space for sentences
minimum 3 words
rather than hard
fixed resource system
can the apparently
who boys chase
network has learned
mary chases feed
can complex structural
connectionist models and
hidden unit patterns
sequences of words
of language are
the relative pronoun
sensitivity to context
presented to the
the prediction task
units on the
in connectionist networks
8a and 8b
8c and 8d
a simple recurrent
representations which are
to focus on
how the network
of the representational
to the network
for the network
the subject is
in the sentence
be accommodated by
the network through
the network was
and so this
what is meant
to account for
each word in
of 10 000
the ways in
at the outset
ways in which
principal component analysis
trained to predict
into which classes
the subordinate clause
sentences 3a d
main hidden and
were novel and
localist approach would
relative pronoun who
the lexicon in
network solves the
d o optional
1990 a network
based on neuroscience
nouns verbs etc
the initial noun
subject noun is
a connectionist model
it must learn
of network predictions
graph of network
distributed representations are
verb has been
john mcclelland 1989
an optional direct
spoken word recognition
than localist representations
argument structure facts
dimensions of variation
with relative clauses
of relative clauses
order of words
000 sentences which
to grammatical structure
on neuroscience towards
follow the verb
unit activation vectors
grammatical relations and
building blocks as
simple sentences the
the trained network
the simulations reported
neural networks proceedings
the dynamics involved
a verb in
of grammatical structure
move the network
sentences with multiple
the lock into
the simple recurrent
b verb argument
a gap following
maximum 13 words
to noun phrases
broke the window
are items which
a singular verb
as words are
nouns and their
hinton mcclelland rumelhart
these two sentences
hidden unit vectors
verb in this
which information can
single bit was
the underlined words
sentence boys who
solves the task
learn the task
i see as
here the network
account of language
the sentence boys
the linguistic representations
to hidden units
represen tations in
although distributed representations
relative clause modifying
of hidden unit
agreement in simple
chases requires a
nature of linguistic
must learn which
the open ended
of linguistic representations
sentences such as
after each word
richness of linguistic
computational architectures based
fodor pylyshyn 1988
sejnowski rosenberg 1987
structure sensitive operations
who cat chases
chases boy the
predict the order
generalizations which are
network predictions following
