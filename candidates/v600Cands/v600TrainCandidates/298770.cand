backgammon
champion
tesauro
games
challenger
learning
game
players
pubeval
racing
gammon
player
evolutionary
td
dice
teacher
winning
student
reinforcement
rolls
co
playing
play
000
pollack
wins
challengers
mgl
hillclimbing
gammontool
foil
evolution
1992
win
expert
tac
contact
mutant
generations
toe
endgame
dynamics
funes
weights
training
success
opponent
100k
self
annealing
angeline
000th
neurogammon
buster
tic
against
hill
weaknesses
moves
climbing
agent
mediocre
neural
1994
pablo
coevolution
played
edwin
gerald
fitness
rms
temporal
expertise
jong
roll
dietterich
jordan
genetic
evolved
move
feedforward
coevolutionary
champions
maynard
monopoly
hod
sklar
hornby
schraudolph
falter
mediocrity
probability
equilibria
chess
novice
prisoner
lipson
crites
abandoned
board
suboptimal
chaos
competitive
fogel
multiagent
bias
samuel
questions
boyan
annealed
mathew
population
teaching
sims
networks
harder
pieces
meta
collusion
barto
nash
bidding
michie
learner
positions
early
successful
littman
skill
davies
cellular
douglas
sampled
education
schedule
tournament
hidden
elizabeth
spiral
1995
rewards
lucky
streams
evolving
throwing
network
adjustment
setup
randomness
evolve
beat
dilemma
learn
learnability
worked
ev
human
reward
autonomous
bear
apparently
payoff
agents
neighborhood
york
walker
learned
robots
averaged
200
weak
legal
race
generation
1k
packard
4000
units
mitchell
checkers
100
maxsolve
junkyard
acheivement
juille
chutzpah
biting
unlearnable
backammon
menace
somol
climber
cartlidge
yeongho
sevan
virulence
reportedly
performanceto
oligopoly
humble
co evolutionary
of backgammon
td gammon
self play
the game
of winning
the challenger
the champion
co evolution
temporal difference
game of
against pubeval
tesauro s
the student
the dice
of games
evolutionary learning
s 1992
dice rolls
reinforcement learning
the teacher
probability of
annealing schedule
tic tac
tac toe
difference learning
gerald tesauro
move number
pablo funes
benchmark networks
dice streams
dynamics of
10 000
meta game
of wins
at generation
the foil
relative fitness
1992 paper
the dynamics
multi agent
000 and
move n
teacher and
the mutant
agent system
de jong
hill climbing
a game
weaknesses in
evolutionary computation
backgammon which
the mgl
evolution proceeds
000 generations
at move
players against
bear off
racing stage
successful challenges
edwin d
mediocre stable
td learning
challenger is
his 1992
100 000th
jordan b
buster douglas
the racing
and pollack
challenger success
b pollack
of learning
machine learning
the player
network against
our 100
000 games
d de
playing against
of chaos
000 10
100 000
the probability
and 100
an expert
in weights
initially when
the games
of play
stable states
our original
tesauro 1992
to win
games for
and student
equilibria in
70 000
by playing
learning is
of successful
games against
student s
against the
the learning
feedforward neural
player s
the co
our network
player is
success rate
its success
hidden units
generation 1
1 000
learning to
evaluation function
of questions
neural network
learning of
al 1994
the bear
backgammon players
contact positions
make mostly
game still
hod lipson
current champion
winning is
near 50
the backgammon
alternate games
1992 result
or racing
by tesauro
play proceeds
crites and
first 35
backgammon and
suboptimal equilibria
unit net
algorithms web
schraudolph et
challenger wins
wins for
champion network
successful learning
rolls and
mutant wins
or tic
mathew davies
187 202
lucky novice
chess or
1996 walker
few games
rolls to
champion and
this worked
rms distance
challengers in
wins against
000th generation
integrating reinforcement
for contact
1996 crites
robots artificial
success rather
players evolved
human education
games angeline
self teaching
ten players
stronger players
tesauro in
contact racing
and racing
backgammon strategy
1992 results
one game
of tesauro
life v
douglas effect
relative expertise
to pubeval
rate exceeded
for backgammon
co ev
champion by
still contact
pollack hod
backgammon in
good challengers
pollack 1994
network playing
1992 networks
perfect policy
angeline and
maynard smith
roll at
probability of winning
tesauro s 1992
co evolutionary learning
the dice rolls
10 000 and
of the game
tic tac toe
the game of
temporal difference learning
the dynamics of
dynamics of backgammon
frequency of successful
percentage of wins
the probability of
number of games
and 100 000
in the game
mediocre stable states
generation 1 000
jordan b pollack
at generation 1
at move n
our 100 000th
game of backgammon
edwin d de
of co evolutionary
meta game of
challenger success rate
of successful challenges
1992 paper which
of self play
the challenger success
a relative fitness
edge of chaos
d de jong
a co evolutionary
1 000 10
000 10 000
000 and 100
teacher and student
our network s
games for the
the student s
multi agent system
evolutionary computation v
feedforward neural network
is it to
et al 1994
as chess or
robots artificial life
the teacher and
16 6 rolls
required to win
co evolution in
networks from our
the challenger is
sklar mathew davies
artificial life v
suboptimal equilibria in
barto 1996 walker
a lucky novice
b pollack hod
whenever the challenger
the backgammon domain
and barto 1996
hidden unit net
integrating reinforcement learning
pollack hod lipson
game of learning
the current champion
three benchmark networks
pieces on each
from our original
angeline and pollack
learning bidding and
genetic algorithms web
through self play
networks at generation
or tic tac
initially when the
the champion by
more input features
of move number
learning of backgammon
for contact positions
and racing positions
1996 crites and
relative fitness environment
reinforcement or temporal
make mostly the
of the champion
buster douglas effect
over 1000 generations
system integrating reinforcement
each player s
the bear off
still being in
rate exceeded 15
usa jordan b
or temporal difference
and pollack 1994
nontrivial chance of
and dietterich 1996
reinforcement learning bidding
the co evolutionary
and 70 000
15 when averaged
winning as a
move n figure
the co evolution
of winning as
chess or tic
in the contact
surprisingly this worked
bidding and genetic
the end game
in the mgl
s 1992 result
game of go
p 187 202
adding more input
the successful learning
dietterich 1996 crites
averaged over 1000
of wins against
of the dice
contact or racing
a game still
all legal moves
player s pieces
players against the
algorithms web intelligence
performance against pubeval
first 35 000
changes in weights
exceeded 15 when
000 and 70
100 000th generation
a nontrivial chance
equilibria in the
stage at move
games against the
which the teacher
zhang and dietterich
the meta game
which make mostly
function of move
coevolution evolutionary computation
elizabeth sklar mathew
game still being
racing stage at
number of rolls
good challengers in
of backgammon strategy
of tesauro s
schraudolph et al
agent system integrating
benchmark networks from
success rate exceeded
of backgammon which
between teacher and
crites and barto
features of backgammon
1996 walker et
4 p 187
of winning is
early convergence in
of rolls to
the reinforcement and
the challenger wins
and temporal difference
different rms distances
the mutant wins
of td gammon
of the foil
reinforcement and temporal
in human education
or racing stage
successful learning of
autonomous agents and
small changes in
the frequency of
our original algorithm
as the player
that the reinforcement
the three benchmark
walker et al
weaknesses in the
mostly the same
iterated prisoner s
for its success
td gammon is
mitchell et al
such as chess
cellular automata to
the contact or
prisoner s dilemma
the edge of
when averaged over
of these games
