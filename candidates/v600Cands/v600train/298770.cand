backgammon
champion
tesauro
games
challenger
learning
game
players
pubeval
racing
gammon
player
evolutionary
td
dice
teacher
winning
student
reinforcement
rolls
co
playing
play
000
pollack
wins
challengers
mgl
hillclimbing
gammontool
foil
evolution
1992
win
expert
tac
contact
mutant
generations
toe
endgame
dynamics
funes
weights
training
success
opponent
100k
self
annealing
angeline
000th
neurogammon
buster
tic
against
hill
weaknesses
moves
climbing
agent
mediocre
neural
1994
pablo
coevolution
played
edwin
gerald
fitness
rms
temporal
expertise
jong
roll
dietterich
jordan
genetic
evolved
move
feedforward
coevolutionary
champions
maynard
monopoly
hod
sklar
hornby
schraudolph
falter
co evolutionary
of backgammon
td gammon
self play
the game
of winning
the challenger
the champion
co evolution
temporal difference
game of
against pubeval
tesauro s
the student
the dice
of games
evolutionary learning
s 1992
dice rolls
reinforcement learning
the teacher
probability of
annealing schedule
tic tac
tac toe
difference learning
gerald tesauro
move number
pablo funes
benchmark networks
dice streams
dynamics of
10 000
meta game
of wins
at generation
the foil
relative fitness
1992 paper
the dynamics
multi agent
000 and
move n
teacher and
the mutant
agent system
de jong
hill climbing
a game
weaknesses in
evolutionary computation
backgammon which
the mgl
evolution proceeds
000 generations
at move
players against
bear off
racing stage
successful challenges
edwin d
mediocre stable
td learning
challenger is
his 1992
100 000th
jordan b
buster douglas
the racing
and pollack
challenger success
b pollack
of learning
machine learning
the player
network against
our 100
000 games
d de
playing against
of chaos
000 10
100 000
the probability
and 100
an expert
in weights
initially when
the games
of play
probability of winning
tesauro s 1992
co evolutionary learning
the dice rolls
10 000 and
of the game
tic tac toe
the game of
temporal difference learning
the dynamics of
dynamics of backgammon
frequency of successful
percentage of wins
the probability of
number of games
and 100 000
in the game
mediocre stable states
generation 1 000
jordan b pollack
at generation 1
at move n
our 100 000th
game of backgammon
edwin d de
of co evolutionary
meta game of
challenger success rate
of successful challenges
1992 paper which
of self play
the challenger success
a relative fitness
edge of chaos
d de jong
a co evolutionary
1 000 10
000 10 000
000 and 100
teacher and student
our network s
games for the
the student s
multi agent system
evolutionary computation v
feedforward neural network
is it to
et al 1994
as chess or
robots artificial life
the teacher and
16 6 rolls
required to win
co evolution in
networks from our
the challenger is
sklar mathew davies
artificial life v
suboptimal equilibria in
barto 1996 walker
a lucky novice
b pollack hod
whenever the challenger
the backgammon domain
and barto 1996
hidden unit net
integrating reinforcement learning
pollack hod lipson
game of learning
the current champion
three benchmark networks
pieces on each
from our original
angeline and pollack
learning bidding and
genetic algorithms web
through self play
networks at generation
or tic tac
initially when the
the champion by
more input features
of move number
learning of backgammon
for contact positions
and racing positions
1996 crites and
relative fitness environment
reinforcement or temporal
make mostly the
