proximal
differentiable
convex
convergence
differentiability
rockafellar
h2
lipschitz
zk
monotone
fz
subdifferential
newton
super
hypotheses
ppa
valued
gamma1
smoothness
operator
kd
metric
proposition
operators
hilbert
expansive
vmppa
yosida
bundle
fukushima
nonsmooth
bfgs
moreau
origin
rd
secant
psi
kz
hessian
inclusion
broyden
continuous
hypothesis
aeib
gph
regularization
dist
modulus
lipschitzian
continuity
41
limsup
semi
criteria
fh
ir
h1
zg
ib
mor
iterates
luque
dennis
quasi
resolvent
gradient
echet
propositions
establish
maximal
lemar
rademacher
ffiib
mifflin
sagastiz
domt
iusem
echal
abal
converges
updating
boundedness
characterization
hf
mini
variationelles
sufficiency
rf
nonempty
qi
proximal point
t gamma1
variable metric
differentiable at
convex programming
point algorithm
linear convergence
metric proximal
is differentiable
super linear
z k
operator t
the operator
fz k
k z
k g
convex function
monotone operators
finite valued
the proximal
hypothesis h2
kd k
the differentiability
the subdifferential
subdifferential of
of convex
gamma1 is
smoothness hypotheses
differentiability of
lipschitz continuous
gamma1 0
convergence of
at z
global convergence
monotone operator
line search
maximal monotone
d k
the operators
single valued
function f
the origin
non expansive
matrix secant
operators n
rd z
approximation criteria
gamma1 bounded
hilbert space
sequence fz
lipschitz continuity
the super
41 theorem
moreau yosida
with modulus
t z
k k
h k
rockafellar 41
propositions 9
the vmppa
finite dimensional
z with
is lipschitz
continuous at
all k
search routine
generalized hessian
not differentiable
h2 is
w k
the inclusion
convergence results
the moreau
by rockafellar
newton like
18 20
the convergence
secant updating
weak cluster
f zg
7 ir
h2 then
41 proposition
the ppa
z gamma1
a convex
20 24
hypotheses are
f is
proximal point algorithm
super linear convergence
metric proximal point
variable metric proximal
t gamma1 is
k z k
t gamma1 0
is differentiable at
the operator t
of convex programming
fz k g
the proximal point
the variable metric
z k k
kd k z
differentiable at z
at z with
gamma1 is differentiable
the super linear
the subdifferential of
the differentiability of
subdifferential of a
differentiable at the
a variable metric
18 20 24
at the origin
gamma1 is lipschitz
operator t gamma1
the function f
operator t is
of t gamma1
convergence of the
case of convex
linear convergence of
sequence fz k
d is differentiable
the operators n
lipschitz continuous at
operators n k
a convex function
the sequence fz
of a convex
is the subdifferential
gamma1 0 is
is lipschitz continuous
2 t z
is single valued
differentiable at 0
set t gamma1
t gamma1 and
line search routine
z gamma1 bounded
3 7 10
propositions 9 and
20 24 25
7 10 18
10 18 20
n k are
at y with
for all k
a line search
the moreau yosida
the convex programming
the method when
the global convergence
bounded if and
of the proximal
fh k g
that t gamma1
matrix secant updating
24 25 36
weak cluster point
k g satisfies
are non expansive
characterization of super
establish the super
is finite valued
at w with
finite dimensional convex
a generalized hessian
newton s method
if the operator
cluster point of
moreau yosida regularization
not differentiable at
the hilbert space
linear convergence is
of super linear
z 2 gamma
and d k
