loggp
sweep3d
mpi
mk
sweeps
mmi
octants
processor
octant
grid
synchronization
sp
processors
message
receive
4kb
logp
smp
cluster
wavefront
send
roundtrip
1kb
posted
communication
intra
measured
sweep
totalcomm
startp
messages
west
primitives
poems
jt
handshake
asci
micro
projections
deelman
ewa
projected
fortran
anomaly
blocking
vernon
pipelined
45x45x1000
total_comm
150
latency
calculations
vikram
adve
costs
modeled
sizes
north
benchmarks
1000
rizos
sakellariou
scalability
billion
128
usec
ruoming
gagan
developers
configurations
rajive
25000
twelve
delay
accurately
15000
bagrodia
principal
execution
overhead
10000
analytic
contention
analyzing
thousand
scaled
trip
measurements
mary
neighbor
the loggp
loggp model
of sweep3d
the sweeps
the sweep3d
mpi send
processor grid
sp 2
the sp
the mpi
execution time
message size
mk 10
mpi communication
problem size
mpi receive
mk 1
of processors
per processor
problem sizes
loggp mmi
than 4kb
for octants
total problem
intra cluster
processor p
sweeps for
synchronization structure
send and
processors time
the roundtrip
size per
and mpi
the model
the measured
the processor
complex synchronization
sweeps from
communication parameters
fixed total
the message
j dimensions
octant pair
it jt
been posted
loggp mk
synchronization structures
measured mk
fixed problem
grid points
the west
cluster communication
150 150
sweep3d application
micro benchmarks
mmi 3
mpi software
sweep3d is
measured execution
synchronization costs
synchronization delays
receive has
at message
two neighbor
receive primitives
corresponding receive
in equation
of mpi
6 1000
for messages
message sizes
smp node
l and
t 7
four processor
processor configurations
communication times
for intra
each processor
loggp equations
billion grid
1 loggp
3 mk
4kb the
octants 7
communication micro
measured application
projected sweep3d
projected execution
roundtrip communication
the loggp model
the sp 2
of the mpi
and mpi receive
mpi send and
model of sweep3d
send and mpi
number of processors
loggp model of
the mpi send
the processor grid
problem size per
the sweeps for
size per processor
sweeps for octants
for the sweeps
t 5 6
version of sweep3d
processor p n
t 7 8
of processors time
fixed problem size
loggp model for
o and g
and j dimensions
the sweep3d application
mpi receive primitives
6 6 1000
fixed total problem
of the sweep3d
the application developers
for intra cluster
execution time for
the message from
of the processor
on the sp
the corresponding receive
l o and
the mpi communication
of table 2
m 1 l
up to 128
measured execution time
processor in the
and bandwidth in
of the sweep
at processor p
communication on the
message from the
interest to the
groups and 10
projected sweep3d execution
the sweeps from
mpi communication on
from the west
total problem size
two neighbor processors
for octants 7
message size equal
10 000 time
system is scaled
mmi 3 mk
sweep for octant
time of sweep3d
intra cluster communication
beyond one or
total problem sizes
1 loggp mmi
mk 1 loggp
the projected execution
parameters l o
of mpi send
the roundtrip communication
has been posted
to two neighbor
corresponding receive has
for fixed total
at message size
of the loggp
octants 7 and
in equation 6
the i and
the 3d grid
grid is mapped
and t 7
for messages larger
intra cluster and
messages larger than
the processing overhead
and 10 000
