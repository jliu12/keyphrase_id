featuremine
mining
winnow
pruning
fire
features
wwwwww
frequent
spelling
training
feature
classification
ignite
eid
idlist
bulldozer
maxw
raises
classifiers
freq
sequences
parity
idlists
betting
classifier
distinctive
bb
bayes
bet
time8
subsequence
word
time0
mine
plan
irrelevant
sequential
suffix
bets
ab
tag
subsequences
bd2
digat
spade
bd1
y8
naive
prune
p3
pos
mined
accuracy
covers
label
bc
000
y4
domains
frequency
redundant
vs
terrain
x3
547
x7
millions
boolean
dataset
folds
p2
weights
bulldozers
truei
burned
moveto
politics
time32
time6
subsumes
chess
selecting
correction
traces
meta
dna
correlated
subsumed
corpus
eids
hcmac
poker
mines
brown
ith
criteria
additionally
datasets
p1
y3
monitoring
million
conf
integrates
item
chih
learning
produced
511
vegetation
weigh
www
heuristic
games
6g
predictive
plans
patterns
web
min
items
classify
122
observable
hf
attributes
trained
confidence
exponentially
primitives
x6
parent
cell
phoenix
nm
scalable
thousands
moves
bd
75
personalized
someone
efficiently
database
partition
rare
337
your
events
forest
lattice
rules
selection
association
water
wins
3n
predicting
discover
categorical
215
simulation
ci
win
success
iff
me
2917
novices
xrules
bayesfm
ignites
4331
xiaonan
dig
ferr
falsei
winnowfm
dug
igniting
arificial
unburnable
featurem
garbay
y9
yih
merl
shiou
winnowtf
burning
2019
rialle
bayestf
winnows
duchne
dichotomic
bez
ria
guozhu
3802
classification algorithms
features produced
feature set
by featuremine
frequent sequences
the fire
wwwwww wwwwww
feature value
class c
the features
feature f
pruning rules
training examples
data mining
value pairs
naive bayes
feature mining
d bb
featuremine algorithm
1 raises
bb d
min freq
the featuremine
pruning rule
of features
features to
classification accuracy
boolean features
fire world
sequential domains
f 1
features for
produced by
d d
standard classification
spelling correction
features which
mining techniques
f i
features that
selecting features
as features
execution traces
each example
ith bet
frequent for
b pruning
www www
the idlists
mined features
sequence mining
non sequential
the feature
the training
class label
exponentially large
decision lists
max l
be frequent
sequence data
target word
of feature
of examples
dataset there
all frequent
features and
for classification
to class
features are
the frequency
1 covers
test examples
examples and
correlated with
examples in
mining algorithm
frequent and
potential features
features out
2 covers
meta features
betting sequences
75 75
features features
1 547
sequence lattice
sequential features
parent partition
observable features
raises twice
improve classification
the idlist
bets 3
bd2 x7
parity problems
ignite x3
examples d
example features
idlist for
idlists of
000 examples
raises 2
547 122
any feature
maxw and
features we
for sequence
feature sets
n a
features is
for class
a feature
to search
in sequential
each feature
time time
a b
training data
a fire
new examples
feature subset
all features
boolean feature
possible features
a bc
frequent sequence
brown corpus
be subsumed
redundant features
sensitive spelling
classification performance
for features
irrelevant features
label c
c j
of sequence
mining for
search over
mining algorithms
the weights
criteria for
the plan
feature selection
in d
features from
f 2
sequential data
features should
large space
class labels
for selecting
millions of
features can
for describing
of potential
sequences as
is frequent
our selection
on feature
the mining
which features
sequences and
of training
there were
a plan
the classifier
cpu seconds
p 1
the ith
to classify
training set
selection criteria
each parent
an instance
instance is
subsumed by
a min
the examples
the sequence
a classifier
context sensitive
the frequent
a occurs
a sequence
features in
to efficiently
ones played
pos 2
non distinctive
features each
specified min
irrelevant boolean
raises by
featuremine with
efficiently search
length sequences
integrates pruning
p3 raises
vs then
sequence classifier
features produced by
the features produced
produced by featuremine
feature value pairs
wwwwww wwwwww wwwwww
feature f i
d bb d
the featuremine algorithm
p 1 raises
bb d d
data mining techniques
that the features
number of features
class c j
examples in d
f 1 covers
d d bb
dataset there were
of feature value
test examples and
a b pruning
the ith bet
standard classification algorithms
all frequent sequences
criteria for selecting
the target word
d d d
for class c
to class c
mining techniques to
in the training
f 2 covers
this feature set
www www www
of potential features
sequences as features
be subsumed by
the feature set
used a min
each feature f
each parent partition
the mining algorithm
1 547 122
the idlist for
a sequence is
will be subsumed
the idlists of
features out of
the fire world
context sensitive spelling
sensitive spelling correction
for selecting features
features should be
space of all
an instance is
the a b
for each example
that f 2
of examples in
features that are
data mining algorithms
of all subsets
set of examples
the frequency of
all subsets of
we used a
the training examples
of the features
an example simulation
and disk based
examples d and
and max l
the sequence lattice
the meta features
mined features from
web usage data
a b relations
evidence for different
1 raises and
freq maxw and
1 true f
be frequent for
for describing each
the resulting idlist
this enables it
ones played by
scalable and disk
75 75 75
based on length
we search over
simulation the fire
min freq maxw
intersecting the idlists
mining algorithm itself
different features to
sequence data the
you re vs
to feed into
maxw and max
selection criteria for
the same eid
the subsequence relation
feature set f
some constant 1
frequent sequences as
f 1 subsumes
feature is true
have label c
plan execution traces
eids f1 2
irrelevant features for
features of length
rule described in
algorithm itself instead
primitives for describing
the non sequential
reduce classification accuracy
pruning rule described
to efficiently mine
boolean features to
the frequent sequences
examples that f
two pruning rules
classify new examples
n a 94
our selection criteria
different min freq
b indicates a
547 122 1
pruning as a
by classification algorithms
vector of feature
for sequence classification
post processing step
y3 time8 ignite
selecting features to
features to feed
p3 raises 2
ith bet and
than vs then
c than f
classification algorithms furthermore
a boolean feature
in the fire
a sequence and
raises and then
present criteria for
by featuremine in
in the brown
n a 86
first pruning rule
there vs their
of selecting features
class label on
frequent and distinctive
frequent for class
scalable feature mining
large feature sets
on or off
target word and
and trained the
is exponentially large
i vs me
digat bd2 x7
an example sequence
time32 ignite x6
significantly correlated with
a chi squared
respect to class
value pairs for
seconds cpu seconds
the primitives for
that all features
the pruning rules
for the spelling
of possible features
d and parameters
1 raises twice
p2 calls p3
for different features
we mined features
to mine for
itself instead of
use data mining
to search over
belonging to class
a fire line
all elements do
someone bets 3
value pairs we
to classify new
122 1 547
with the features
ab bd bc
can reduce classification
bd2 x7 y4
in sequential domains
irrelevant or redundant
user specified min
trained the classifier
