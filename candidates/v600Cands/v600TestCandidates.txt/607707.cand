watanabe
singularities
learning
eq
asymptotic
neural
pole
bayesian
amari
blowing
identifiable
perceptron
stochastic
2001a
kullback
expansion
dw
layered
parametric
statistical
algebraic
samples
analytic
density
poles
1999b
shun
murata
geometrical
generalization
units
ciently
likelihood
exp
estimation
training
regression
neighborhood
clarify
ichi
priori
dxdy
opper
sumio
su
artificial
hidden
layer
resolution
hierarchical
ups
aic
hironaka
haussler
machines
fisher
ect
ozeki
1974
mixtures
curves
nh
inequality
meromorphic
zeta
unrealizable
fukumizu
atiyah
solla
tomoko
geometry
regular
nk
compact
singular
jw
inequalities
bias
trained
foregoing
entropic
sato
tishby
bic
largest
contained
mellin
conic
minimizes
probability
networks
variance
posteriori
gaussian
akaike
perceptrons
mackay
error
firstly
asymptotics
universal
1993
satisfies
clarified
boltzmann
mathematically
definite
analytically
proven
levin
schwarz
algorithmically
mathematical
continued
realizable
1995
extensively
1970
manifolds
1985
wavelets
rational
statistics
secondly
radial
1980
1992
curve
jensen
1989
log
jacobian
degenerate
miki
dacunha
cousseau
hiroyuki
nakahara
combing
keisuke
1013
castelle
neuromanifolds
hyperfunctions
shinomoto
abic
aoyagi
prehomogeneous
gassiat
2001c
identifiablity
reys
hyperparatemeter
prespective
kashiwara
holomorphic
paramaters
hyeyoung
perceptorn
merhav
1949
rissanen
yamazaki
haikun
estimator
mixture
conditional
calculation
rank
1990
cramer
ork
2001b
donsker
hagiwara
hartigan
florent
identifiability
1007
machine
adopt
fujita
homogenous
933
shintani
1065
ror
pth
1038
843
1999a
924
subsection
speaking
interpolation
inference
empirical
1033
chui
yamanishi
asymptotic expansion
generalization error
stochastic complexity
hierarchical learning
learning machines
the true
learning machine
bayesian estimation
regular statistical
h w
f n
the generalization
the parameter
j z
three layer
true distribution
probability density
g n
an asymptotic
the stochastic
true probability
parameter space
the bayesian
y x
hidden units
x w
neural networks
largest pole
p y
w j
training samples
parameter w
artificial neural
layer perceptron
n has
watanabe 2001a
algebraic geometrical
w dw
singularities in
learning curves
assumption a
of singularities
analytic function
the learning
parametric case
kullback information
non identifiable
by eq
distribution is
density function
su ciently
the kullback
w is
pole of
priori distribution
regression function
algebraic geometry
1999b watanabe
true regression
watanabe 1999b
error g
q x
the parametric
w 0
not contained
the asymptotic
w w
blowing up
an analytic
neural computation
ciently large
q y
statistical model
g u
of learning
of w
blowing ups
extensively large
shun ichi
independently taken
ichi amari
priori probability
a priori
statistical models
of training
parametric model
neighborhood of
compact then
eq 30
and eq
contained in
the a
v t
of parameters
clarify the
learning theory
case when
perceptron with
of layered
error by
probability distribution
z has
neural network
w z
exp 2
has an
if g
samples independently
information matrix
analytically continued
2001a in
kullback distance
sumio watanabe
exp nh
algebraic variety
asymptotic property
x dxdy
complexity f
a three
an inequality
estimation is
statistical estimation
that minimizes
units and
maximum likelihood
the neighborhood
learning in
an algebraic
z w
error of
fisher information
its order
pole and
units k
eq 18
singularities and
geometrical structure
poles of
asymptotic theory
the largest
n w
of j
singular points
likelihood method
the fisher
layered neural
resolution theorem
the singularities
using samples
n satisfies
a parametric
parameter that
resolution of
n is
natural number
nk 2
the pole
in algebraic
parametric models
learning curve
x q
w and
g x
bounds of
expansion of
e ect
approximation error
function j
input units
dimensional vectors
trained using
likelihood function
output units
the assumption
conditional probability
a universal
if n
learning model
eq 19
function approximation
bias and
ciently small
then g
a su
an arbitrary
w then
log p
expansion for
log n
computation v
firstly we
function of
previous paper
1992 amari
eq 35
samples watanabe
true parameters
far smaller
levin tishby
and unrealizable
the generalization error
the stochastic complexity
an asymptotic expansion
p y x
has an asymptotic
y x w
n has an
the true distribution
the parameter space
three layer perceptron
true distribution is
hierarchical learning machines
when the true
the true probability
g n has
a three layer
the largest pole
of training samples
in the parametric
artificial neural networks
probability density function
the kullback information
hierarchical learning machine
regular statistical models
of j z
the a priori
of the stochastic
of the generalization
the learning machine
a priori distribution
if g n
contained in the
h w is
regular statistical model
the true regression
the bayesian estimation
h w z
generalization error by
true regression function
a regular statistical
an analytic function
bayesian estimation is
watanabe 1999b watanabe
resolution of singularities
true probability distribution
q y x
generalization error g
the assumption a
the asymptotic expansion
error g n
parameter w j
su ciently large
of the parameter
case when the
the parameter w
in the parameter
is not contained
not contained in
number of training
asymptotic expansion of
on the parameter
g n is
a priori probability
hidden units and
function of w
priori probability density
z w dw
the parametric model
of singularities in
parameter that minimizes
a hierarchical learning
parametric case when
support of w
f n has
1999b watanabe 2001a
then g n
true probability density
y x q
the parameter that
shun ichi amari
layer perceptron with
of learning curves
conditional probability density
w z w
the maximum likelihood
generalization error of
that the generalization
for an arbitrary
that the true
error of the
extensively large then
log p y
algebraic geometrical structure
the parametric models
of hierarchical learning
a case when
poles of j
asymptotic expansion for
by eq 18
eq 18 and
samples independently taken
a parameter w
the three layer
non parametric case
complexity f n
18 and eq
geometrical structure of
asymptotic property of
and its order
end of proof
respectively the largest
the regular statistical
function j z
function approximation error
learning machine is
pole and its
q x dxdy
perceptron with k
19 is trained
independently taken from
trained using samples
w w dw
and eq 19
an algebraic variety
analytic function of
using samples independently
a parametric case
watanabe 2001a in
x w be
stochastic complexity f
h w and
then j z
eq 19 is
the algebraic geometrical
largest pole and
number of parameters
is defined by
distribution is not
of w is
neighborhood of the
w is a
neural computation v
f n is
the neighborhood of
upper bounds of
the pole of
density function on
error by the
v w j
the previous paper
regression function is
if the support
in algebraic geometry
layered neural networks
maximum likelihood method
generalization error is
singularities in the
natural number n
x q x
the learning curve
is an analytic
if the true
ciently large n
then f n
is trained using
0 is a
function on the
q x is
w w 0
w j are
w be a
n is equal
a non parametric
w j and
su ciently small
f n and
of f n
and m 1
is a positive
of a hierarchical
by the assumption
and f n
given by eq
probability distribution is
the support of
w is an
to clarify the
e ect of
it is well
we consider a
the model is
w is the
in practical applications
a su ciently
up technology in
distribution is contained
geometrical method we
on learning curves
arbitrary natural number
is to clarify
