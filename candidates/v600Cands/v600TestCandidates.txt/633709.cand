nns
performances
nn
stepdisc
regression
bonnlander
ruck
selection
relevance
saliency
neural
leray
moody
czernichow
cibas
criterion
ecd
backward
ocd
obd
dorizzi
classification
derivatives
yacoub
0000000000000000000
refenes
hessian
feature
retraining
damage
pruning
validation
training
mutual
sv
weight
stopping
monotonous
mse
derivative
stepwise
perf
prediction
parametric
measures
wilks
kittler
frontier
criteria
wave
stop
fisher
correlated
multilayer
recognition
brain
forward
ard
1994
statistical
coefficient
noisy
rossi
obs
learning
stopped
pathological
frontiers
discriminating
hypothesis
mclachlan
epanechnikov
souli
fogelman
ebd
saliencies
priddy
hajlmarsson
ssr
intensive
heuristic
entropy
prohibitive
sensitivity
1977
weights
1996
ingredients
computationally
eliminated
percentage
fukunaga
surgeon
fraser
regressor
aberrant
gustafson
axis
ranking
1986
battiti
retrained
networks
outputs
cell
variance
risk
absolute
gaussian
narendra
perceptrons
mlp
bootstrapping
estimates
representative
1992
rely
density
threshold
estimated
correlations
nonparametric
generalization
floating
estimation
correlation
fs
trained
heuristics
network
search
feedforward
perceptron
uncorrelated
cross
squared
hidden
wrt
selects
classical
meaningless
fitted
discriminant
1978
attractive
modelling
bayesian
deletion
subsets
diagonal
1970
dependencies
neurons
exhaustive
statistics
restrictive
statistic
difficulty
error
layer
differentiable
averaged
branch
confidence
1993
salient
deletes
artificial
selecting
discarding
regularization
kernels
gradient
variation
marginal
covariance
costly
selected
determinant
gasca
swinney
wertz
pertinence
001011111111111110100
hermans
10001111111111111111
kullbak
econometrics
representativeness
000110111111111011100
rdle
infraction
committees
stahlberger
multinormal
baxt
000001100111101010000
000001111110111000000
01011011101110111111
utans
feature selection
variable selection
selection methods
selected variables
the nn
mutual information
neural networks
for regression
choice criterion
validation set
weight pruning
relevance measures
stop criterion
l l
generalization error
a validation
sv p
order methods
for classification
wave problem
p selected
backward search
different variable
non linear
of variables
the hessian
the relevance
performance comparison
the selection
these methods
methods which
selection for
the saliency
variables perf
saliency is
selected variable
for nns
correlated variables
stepdisc 4
feature evaluation
good performances
non pathological
axis percentage
cell damage
method p
bonnlander 4
variable set
x l
pattern recognition
the generalization
5 2
selection method
variable subset
evaluation criterion
parametric methods
performances and
c r
take into
between variables
relevance of
cross validation
neural network
these measures
evaluation criteria
nn is
regression and
a variable
f x
r p
of variable
fisher test
of nns
model independent
pure noise
moody 5
gaussian problem
stepwise methods
fs p
relevance measure
variable relevance
two gaussian
refenes 5
dorizzi 5
for stopping
czernichow 5
architecture selection
original wave
floating search
x f
linear models
measures for
is stopped
input variables
al 1996
methods are
p a
variables in
several authors
for neural
noise variables
based feature
remaining variables
stopping the
final prediction
or classification
prediction error
training set
search and
the performances
non parametric
l x
percentage of
selection is
criterion is
performances of
methods use
5 3
variable is
best subset
best performances
input variable
these derivatives
class j
two weight
p 1
the training
been proposed
et al
estimated on
on heuristics
regression or
correct classification
the search
a feature
f c
several methods
absolute values
comparison of
of feature
authors use
optimal search
into account
of different
computed using
computationally intensive
r all
the derivative
the different
classification and
of remaining
density estimation
the network
the selected
linear regression
and regression
either for
input i
al 1994
p b
the weight
is usually
model selection
search methods
s method
propose to
diagonal approximation
cibas leray
or stepwise
to obd
since variables
backward methods
noisy wave
fitted for
regressor f
p variables
variables bonnlander
saliency of
refenes et
fogelman souli
select is
representative methods
branch bound
dependence measure
stop criteria
discriminating power
community which
each nn
relevance criterion
fixed threshold
3 dorizzi
difficulty here
data refenes
early brain
set sv
early cell
bonnlander s
performances but
derivative absolute
wilks lambda
nn variable
variable selection methods
different variable selection
of different variable
a validation set
the generalization error
feature selection methods
l l l
p selected variables
performance comparison of
f c r
of the generalization
comparison of different
method p selected
axis percentage of
feature selection method
selected variable set
selected variables perf
r p 2
stepdisc 4 2
bonnlander 4 3
l l x
of a variable
the relevance of
x l l
methods which have
feature selection for
relevance of a
may be used
all these methods
either for regression
selection is stopped
backward search and
the best performances
of selected variables
original wave problem
l x x
the original wave
5 2 7
moody 5 2
dorizzi 5 2
pure noise variables
for regression or
the two gaussian
the choice criterion
of remaining variables
czernichow 5 2
the nn is
c r all
refenes 5 2
selection for neural
using a validation
selection methods on
the selected variable
gaussian problem with
methods which use
for stopping the
order methods which
two gaussian problem
f x l
percentage of selected
compute the mutual
the final prediction
sv p 1
relevance measures for
4 3 2
et al 1996
for neural networks
4 2 2
take into account
f x f
x f x
5 2 3
non linear models
a feature selection
1 n p
best subset of
several authors have
the best subset
which have been
the mutual information
classification and regression
5 2 8
variables on the
a variable by
5 2 1
the training set
not take into
proposed to use
of feature selection
in 5 1
p a b
we have used
sets of variables
on the model
methods on the
an estimate of
in this problem
methods may be
for non linear
et al 1994
a local minimum
have used a
to take into
do not take
computed using a
the importance of
been proposed by
5 1 1
only one variable
of the relevance
set sv p
using mutual information
the nn with
to select is
density p a
methods in feature
perf stepdisc 4
optimal cell damage
o the importance
c r non
1994 proposed to
salient features for
difficulty here is
l sv p
with uncorrelated variables
percentage of variables
fs p forward
variable is found
variables selected y
cibas 5 3
table 1 selection
p th variable
variable selection in
the saliency of
variable is usually
early cell damage
stop criterion or
with good performances
the stop criterion
priddy et al
vs percentage of
of variables selected
does not delete
noisy wave problem
bonnlander s method
validation set since
correlations between variables
architecture selection for
methods vs percentage
nn is trained
stopping the selection
the wilks lambda
are nearly linear
and regression tasks
in the nn
a simple backward
families of methods
estimated on a
a choice criterion
refenes et al
each nn in
each variable deletion
the classification or
the evaluation measure
a variable subset
gustafson and hajlmarsson
bottom of table
for regression and
several methods propose
good performances but
time series modelling
proposed a series
2 3 dorizzi
simple backward search
allows to take
of correct classification
validation or algebraic
is retrained after
the noisy wave
fitted for nns
pathological data refenes
feature selection pattern
the selected variables
or stepwise methods
of weights of
data refenes 5
variables the second
the partial correlation
ruck 5 2
sub optimal search
order methods several
leray 5 3
and the nn
on a validation
ingredients of feature
partial correlation coefficient
zero order methods
regression or classification
propose two weight
as a choice
