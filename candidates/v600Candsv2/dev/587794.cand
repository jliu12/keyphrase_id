norms
kax
bk
norm
squares
watson
separable
uncertainties
ghaoui
tting
lebret
subdierential
perturbations
kxkb
dundee
typied
kdka
el
kek
satised
attained
dierentiable
smooth
kbka
attracted
posed
dened
rst
fitting
arguing
descent
wheat
minimizing
nding
dieren
secant
chandrasekaran
newton
derivative
convex
singular
iowa
min
polyhedral
vk
columns
estimation
robust
connections
kvk
emphasis
dk
rearranged
conventional
dierent
ke
readily
modications
frobenius
uncertain
reposed
gawatson
fkax
kdkb
krka
tiating
kr
coincide
rows
unrestricted
errors
moduli
draper
nondierentiable
aand
krein
osborne
mately
nonconvexity
rise
occurring
convergent
root
monotonic
algorithmic
iteration
facilitating
orthogonally
maths
nonexistence
scotland
wood
stack
minimization
submatrix
derivatives
variants
contribute
connection
max
mn
kck
attainment
treatment
dual
krk
simax
solves
satisfactory
denition
robustness
minimize
overestimated
subgradients
approxi
thrust
tiable
eect
deter
ltering
kwk
matrices
dene
coincidence
inertia
ict
unconstrained
stationary
valuable
arguably
perturbation
exploited
damped
perturb
satises
restated
introductory
signicance
interpreted
normally
indenite
remarkably
observations
subject
solved
exceed
presence
chebyshev
descending
jc
locally
minimized
ka
justied
loan
inde
suggested
ideas
diers
goodness
amenable
golub
minimizes
possibilities
permitting
neighborhood
reordering
facilitates
substituting
bg
journals
seeking
allowable
discontinuity
commonly
submatrices
electronically
complicated
nn
augment
bounds
daniel
eects
ple
hull
least squares
kax bk
squares norms
k k
simple iteration
squares case
data tting
total approximation
matrix norm
el ghaoui
separable norm
r m
derivative methods
x kax
separable norms
positive root
theorem 2
otherwise u
exist v
k b
total least
min max
one positive
least norm
fitting problems
squares norm
data uncertainties
standard convex
lebret 8
norms k
singular value
norm problems
locally convergent
tting problems
conventional data
data fitting
solves 2
vector norms
algorithmic development
b 2
v 2
convex analysis
descent direction
approximation problem
known bounds
p norms
n components
min min
norm solution
parameter estimation
rst n
tting problem
wheat data
loss data
stack loss
min problem
let otherwise
ke dk
known structure
polyhedral norms
table simple
nding min
bounded uncertainties
separable matrix
computationally convenient
lebret 9
bounded data
secant method
max type
iowa wheat
attracted interest
result follows
maximum value
l p
value decomposition
problem subject
original problems
x minimizes
chandrasekaran et
given norm
r mn
result theorem
commonly occurring
robust solutions
using g
main emphasis
g a watson
least squares norms
norm on r
least squares case
k k b
norms are least
b 2 range
minimizing with respect
ghaoui and lebret
u is arbitrary
norms k k
x kax bk
maximum in 4
respect to x
one positive root
total least squares
x the maximum
data fitting problems
least squares norm
v 2 kax
arbitrary but 1
data tting problems
total approximation problem
presence of bounded
rst n components
standard convex analysis
least norm problems
theorem 2 3
exactly one positive
l p norms
result is proved
problem of minimizing
following result theorem
problem of nding
see for example
newton s method
table simple iteration
solves 2 2
bounds on kek
minimum a norm
min max type
minimized at x
bound on ke
b on r
considered by el
convergent to proof
norm is one
v 2 kbka
may be posed
contribute to algorithmic
separable matrix norm
simple iteration process
components of r
exist v 2
m s components
kax t bk
perturbations this gives
observations which contribute
conventional data tting
typied by l
last s components
let otherwise arbitrary
solution to denote
stack loss data
iowa wheat data
interest so far
bounded data uncertainties
