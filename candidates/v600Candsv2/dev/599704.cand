vf
reinforcement
frontier
munos
splitting
discretization
mdp
policy
stdev
car
disagreement
emi
influence
hill
resolution
criterion
moore
variance
bellman
acrobot
pole
linearity
barycentric
rectangle
cart
split
corner
learning
influencers
discontinuity
andrew
markov
discounted
inf
criteria
rectangles
areas
corners
dupuis
trajectories
barto
grids
simplex
trajectory
reward
triangulation
simplexes
kushner
sigma
splits
velocity
dv
control
deviation
sutton
kuhn
gymnast
baird
xi
dp
dynamics
chain
exploration
refinement
discretized
deduce
coordinates
trie
cells
gradient
highest
viscosity
around
coarse
exits
probabilities
interpolated
frontiers
feed
kd
grid
states
refined
puterman
grne
interpolators
meuleau
reinforcements
rebalance
averager
cumulated
bourgine
semmler
performances
ffl
discretizations
prioritized
situated
discontinuous
terminal
inside
willi
atkeson
xij
kaelbling
hamilton
transition
actuator
calculable
chained
approximated
area
indexes
vertices
uniform
sk
crandall
gray
piecewise
thrust
lars
sweeping
notions
cell
iterated
tree
cumulative
projection
bertsekas
jacobi
sorting
measures
interpolate
banff
ahead
successors
successive
respective
wherever
interpolation
leaf
spend
2d
policies
contraction
triangulations
uncertainty
reach
inconsistency
richard
differential
places
oe
illustrated
hjb
fleming
simplical
finkel
freudenthal
precup
brussel
wee
argmax
mons
atke
gravitation
boone
verhaert
neuronlike
elbow
beneficially
dearden
kuta
waist
jodogne
optimism
schutter
coxeter
justus
manoever
gammachained
soner
prohibitely
bradtke
sammut
piater
bonuses
proximateperfectly
angle
dynamic
position
value function
optimal control
variable resolution
state space
value non
policy disagreement
non linearity
resolution discretization
stdev inf
splitting criteria
reinforcement learning
inf criterion
emi munos
frontier 2
value difference
corner value
frontier 1
split criterion
andrew moore
hill problem
linearity criterion
cart pole
bellman equation
disagreement criterion
reinforcement r
control u
splitting criterion
uniform grids
barycentric coordinates
markov chain
standard deviation
j u
control problem
kuhn triangulation
local splitting
discretized mdp
several splitting
discretization obtained
control problems
kushner dupuis
around frontier
dupuis 1992
current reinforcement
frontier 3
dynamic programming
upper part
figure 15
optimal policy
refinement process
optimal trajectories
local criteria
see figure
feed back
deviation oe
markov decision
vertical position
criteria based
dimensional control
given discretization
terminal reinforcement
pole problem
back control
velocity figure
kd trie
whose splitting
discretization resulting
gradient dv
every corner
difference criterion
approximated gradient
splitting rate
highest standard
subset sigma
hill control
vf along
control derived
good approximation
viscosity solutions
machine learning
figure 19
simplex containing
local measures
whole state
sutton 1996
new non
learning p
point inside
bottom part
intuitive idea
optimal performance
value non linearity
variable resolution discretization
discretization in optimal
stdev inf criterion
munos and andrew
corner value difference
non linearity criterion
policy disagreement criterion
part of frontier
states of policy
several splitting criteria
local splitting criteria
probabilities of transition
kushner dupuis 1992
figure 15 b
standard deviation oe
states of highest
cart pole problem
areas of change
xi we define
shows the discretization
notion of influence
hill problem using
using the corner
feed back control
current reinforcement r
dimensional control problem
value difference criterion
hill control problem
new non local
whole state space
machine learning p
takes into account
conference on machine
using the value
p s k
quality of approximation
variance oe 2
order to define
