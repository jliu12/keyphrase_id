tn
policy
mdps
bellman
mdp
dened
bk
kt
rust
lipschitz
planning
actions
discounted
kk
poly
samples
kv
curse
action
transition
pollard
measurable
rewards
markov
contraction
stochastic
policies
lip
log
jaj
dimensionality
probability
xed
kuffner
jared
denition
proposition
stationary
sampling
reward
markovian
polynomially
thuc
pomdps
limes
multigrid
uniformly
probabilities
logarithmic
selects
horizon
animations
inequality
nite
operators
myopic
maximal
triangle
kearns
vu
lhs
law
rst
discount
firstly
logarithmically
whilst
immediate
innite
rhs
sup
mappings
norm
nish
cf
processes
banach
sucient
denitions
supremum
vehicle
pseudo
drawing
kx
eurographics
thenn
yuskevich
siggraph
polynomialy
majorize
jajn
szepes
telescopicing
measurability
negativ
jgrid
downscale
mindmaker
draw
observable
cover
elementary
proving
reals
monte
integrals
carlo
behaviors
normalizing
phase
decision
random
storage
characterizing
dynkin
ications
nishing
followings
sharpening
acrobot
grenoble
auxilliary
pick
powers
readers
james
entropy
calculations
autonomous
elegant
statement
inequal
originates
dynam
sample
understood
eective
cache
bracketing
victoria
sketches
sta
dynamic
operator
prac
krk
yk
telescoping
homepage
precompute
worrying
dependence
modied
greedy
polynomial
outer
nally
proven
tice
cherno
basics
animation
bounds
deterministic
inequalities
selecting
covering
superior
regularity
phases
article
pseudocode
speeding
worry
tsitsiklis
interactive
spaces
chow
lets
corollary
appropriately
economic
ics
whichever
negatively
ities
nished
factorized
xj
kf
shall
cite
dp
akin
clean
holder
suppressed
optimal actions
policy dened
v x
lemma 5
state space
let log
bellman operator
x tn
kv v
high probability
poly logarithmic
stationary policy
x 1
random bellman
v tn
good actions
bk x
maximal inequalities
line phase
immediate rewards
dynamic programming
decision processes
v k
d log
k r
l p
state x
transition probabilities
action space
log log
lipschitz constant
proposition 5
markov decision
us pick
uniformly optimal
line planning
markovian decision
v a2a
maximal inequality
cf 9
r d
theorem 5
random action
planning algorithm
norm kk
continuous state
horizon time
k p
log d
denition 5
optimal policies
triangle inequality
function v
log let
polynomial complexity
optimal policy
transition probability
xed point
main result
stochastic policy
space polynomial
samples depends
policy x
kk p
autonomous behaviors
stochastic stationary
go thuc
p jaj
eective horizon
probabilities characterizing
uniformly approximately
jared go
operators tn
uniformly good
optimal planning
inner probability
sparse sampling
metric entropy
bellman error
n 8e
depend polynomially
bellman operators
n a2a
poly logarithmically
nite action
vehicle animations
could call
greedy w
cache c
p lipschitz
interactive vehicle
rust 12
approximate planning
kuffner autonomous
elementary event
tn tn
j kuffner
scale exponentially
space discounted
vu james
considered algorithms
e tn
depend poly
k log
o line
log 1
random sample
n v
f x
lip p
partially observable
outer inner
limes superior
kk 1
action spaces
importance sampling
programming deterministic
compute v
measurable functions
reward function
given xed
kearns et
selecting good
tn a v
x 1 n
kv v k
f x 1
x a 0
tn a e
v x tn
proposition 5 8
log d log
actions with high
curse of dimensionality
number of samples
markov decision processes
dened by a2a
v a2a tn
z n v
line planning algorithm
d log log
theorem 5 13
o line phase
planning in large
markov s inequality
let log 1
theorem 5 18
cover of f
corollary 5 3
let and let
interactive vehicle animations
distributed over x
eective horizon time
uniformly optimal policies
yields a uniformly
approximately optimal policy
let us pick
variables then sup
polynomial complexity theorem
need some denitions
let the stochastic
random bellman operators
denitions and results
selecting good actions
pollard s maximal
let the stationary
optimal and given
r k p
models the complexity
jaj and 1
markovian decision problems
dened by tn
dened by dened
n v x
greedy w r
set of mappings
prove an inequality
let log d
vu james j
functions let x
x a random
sup n 8e
set of measurable
let k log
v x g
transition probabilities characterizing
stationary policy dened
stochastic stationary policy
algorithm for near
norm of vectors
james j kuffner
planning in continuous
phase one computes
outer inner probability
continuous state space
near optimal planning
jared go thuc
random bellman operator
log l p
integer t 0
uniformly approximately optimal
j kuffner autonomous
selects only optimal
depend poly logarithmically
