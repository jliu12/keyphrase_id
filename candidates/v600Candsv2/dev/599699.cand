undiscounted
policy
discounted
mdp
wandering
mixing
payo
mdps
payos
reinforcement
exploration
attempted
exploitation
ms
learning
return
markov
balanced
unichain
absorbing
multichain
explorations
policies
horizon
denition
actions
action
pr
asymptotic
transition
exploitations
visited
littman
singh
agent
probability
ergodic
ntg
visits
opt
decision
transitions
tsitsiklis
reward
sutton
states
strehl
unvisited
rewards
achievable
reaching
stationary
denitions
polynomial
rm
discounting
compete
unknown
barto
jaakkola
technical
walks
pigeonhole
watkins
near
probabilities
polynomially
halt
induced
nite
cherno
su
explore
innite
var
ij
farias
pucci
bertsekas
simulation
exploit
saul
daniela
dayan
redirected
paths
rst
targeted
easiest
appeal
trade
competing
mannor
shie
quickly
exceeds
ciently
distributions
failure
dene
replicate
cross
nm
settle
engaged
dierent
michael
eventually
nd
alexander
roy
gaining
executes
asynchronous
cumulative
sketch
bonn
fewest
visit
meantime
brie
exposition
variance
max
stochastic
pac
intuitions
banff
nished
xed
tom
executed
knows
bias
approaching
notion
analyses
hope
variances
backwards
discover
executing
upon
aspect
iteration
alberta
worst
mix
nding
traverse
heart
wiewiora
lihong
pieter
deshpande
ives
yishay
apprenticeship
abbeel
controllably
theologos
explo
dering
diuk
gullapalli
nimrod
fiechter
mery
conductance
unichains
youthful
rum
introduc
satinder
wanders
jalali
niranjan
chrisman
sarsa
bountourelis
erman
tommi
competes
zachary
reveliotis
episodic
ploitation
mixing time
discounted case
undiscounted case
balanced wandering
markov decision
known state
reinforcement learning
return mixing
mdp m
decision process
step return
known states
value iteration
asymptotic return
attempted explorations
horizon time
optimal policy
undiscounted return
absorbing state
optimal return
u ms
attempted exploitation
o line
learning algorithm
discounted return
near optimal
let m
partial model
attempted exploration
attempted exploitations
multichain mdps
expected payos
ms ms
transition probabilities
simulation lemma
step attempted
high return
r pr
policy whose
whose return
unknown mdp
ms ij
exploitation policy
general mdps
expected return
r max
every state
start state
main theorem
q learning
computation time
exploration policy
small transitions
state mdp
actual return
line computation
discounted cases
state becomes
m known
achieve near
exploitation exploration
mixing times
unichain case
exploit lemma
decision processes
u m
total computation
step policy
small transition
becomes known
polynomial bounds
simulation accuracy
optimal expected
current state
polynomial time
nite time
learning p
value function
algorithm executes
nm known
one small
l strehl
l littman
step discounted
o ntg
payo case
alexander l
v ms
discounted value
step undiscounted
known steps
asymptotic undiscounted
expected asymptotic
markov decision process
return mixing time
policy in m
number of actions
steps of balanced
states of m
explore or exploit
number of attempted
transitions in m
undiscounted and discounted
process and let
unknown mdp m
denition of known
probability at least
state becomes known
currently known states
policy for time
return in m
m that start
achieve near optimal
state i 2
o line computation
optimal t step
known state mdp
action a p
markov decision processes
number of visits
machine learning p
conference on machine
total computation time
ms ms ij
whose return mixing
discounted case let
model of m
step of balanced
time convergence results
p a ms
notion of mixing
targeted mixing time
undiscounted value iteration
asymptotic undiscounted return
actions and computation
approximation of m
reach the absorbing
least one small
alexander l strehl
one small transition
opt t m
set of currently
exploration trade o
reinforcement learning problems
cross at least
exploitation exploration trade
nite time convergence
michael l littman
pr r pr
two o line
source of failure
start in state
steps to approach
discounted value iteration
