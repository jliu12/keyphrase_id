kkt
identification
phi
gammarg
facchinei
active
kanzow
mfcq
fischer
rg
regularity
quasi
dist
complementarity
variational
isolated
differentiable
smfcq
licq
ae
xg
neighborhood
semismooth
multipliers
analytic
continuity
nonsingular
ir
stationary
sufficiently
tucker
robinson
continuously
constrained
kuhn
karush
primal
xx
nonlinear
strict
regular
lim
newton
subdifferential
ffl
klatte
xiu
independence
multiplier
identifies
nonsmooth
sumptions
bonnans
slackness
singleton
quadratic
uniqueness
qualification
inequalities
older
strongly
inequality
fromovitz
lipschitz
nonconvex
locally
lc
differentiability
jacobian
converging
stress
clarke
quadratically
mangasarian
subsection
ffi
compact
lipschitzian
everywhere
goes
perturbed
shall
enjoys
superlinearly
identifying
la
assertion
andreas
fulfilled
lagrange
fx
mild
minimization
jianzhong
kyparisis
naihua
gauvin
klug
matrixb
complementatiry
lifeng
subcover
daryina
lojasiewicz
guoping
cktk
correctly
relax
perturbation
optimization
satisfied
complementary
componentwise
optimality
weaker
semiconvex
gongyun
houyuan
izmailov
quires
fffl
ktk
recalling
diag
convergent
continuous
stronger
stability
dealt
identify
obviously
euclidean
rademacher
defeng
fij
sapienza
convergence
trust
convex
substitute
subproblem
derivatives
gradient
gradients
invariably
multifunction
semismoothness
folklore
differen
twice
interior
index
remark
dual
cover
probl
solves
october
violated
vicente
jiang
tiable
therein
nonnegative
aware
indices
matrices
taylor
lus
particu
critically
determinants
kojima
lar
devoted
optimiza
weakened
pang
growth
cite
zhao
transposed
mere
ods
identification function
active constraints
kkt point
problem p
c kanzow
f facchinei
quasi regularity
phi y
assumption 3
regular kkt
xg b
dist x
quasi regular
ae 3
strict complementarity
identification technique
b ffl
ffl 3
f xg
variational inequalities
inequality constrained
small neighborhood
system 8
xx l
kkt system
point x
phi x
continuously differentiable
ae x
stationary point
x 2
theorem 3
q quadratic
strongly regular
linear independence
r x
set k
identification functions
mfcq condition
rg j
ae 2
assumption 4
constrained optimization
smfcq holds
tucker system
isolated stationary
variational inequality
l x
sufficiently small
kkt systems
newton type
kuhn tucker
x l
b ffi
set newton
isolated kkt
l rg
upper h
function ae
lemma 3
system 1
karush kuhn
x k
strong regularity
b subdifferential
every kkt
mfcq holds
correct identification
y phi
set included
sufficiently close
local solution
ir m
active set
order sufficient
g y
ir n
ffl 2
quadratic convergence
nonlinear case
sequence fx
point b
fx k
twice continuously
x solves
h older
functions f
complementarity condition
primal variables
computational optimization
applications v
second order
constrained minimization
let x
lim ae
known rate
lim dist
projection type
locally reduces
regularity implies
p 78
lc 1
regularity 33
min operator
kkt set
analytic around
call quasi
guarantees q
j gammarg
identifies active
local active
particular perturbation
matrix xx
identification of active
fischer and c
facchinei a fischer
assumption 3 4
xg b ffl
f xg b
regular kkt point
dist x k
function for k
sufficiently small neighborhood
f and g
kkt point x
gammarg t gammarg
kkt system 8
r x l
theorem 3 7
continuous on k
assumption 4 1
x 2 f
ae 3 x
quasi regular kkt
q quadratic convergence
b ffl 3
neighborhood of x
x l x
point of problem
x 2 k
isolated stationary point
inequality constrained optimization
kuhn tucker system
solution of inequality
theorem 3 11
goes to 0
upper h older
y phi y
inequality constrained minimization
active set newton
let the mfcq
mfcq and assumption
xx l rg
g are analytic
robinson s strong
strongly regular kkt
identify the active
b ffl 2
define an identification
tucker system 1
empty set included
rg i 0
j i 00
karush kuhn tucker
lemma 3 5
strict complementarity condition
order sufficient condition
every kkt point
definition of phi
sequence fx k
second order sufficient
condition for optimality
stationary point x
assume that f
fx k g
holds at x
x i 0
point of p
every t 2
order to identify
g i x
definition of g
optimization and applications
phi is differentiable
