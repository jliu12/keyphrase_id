rl
qos
cac
policy
revenue
admission
reinforcement
smdp
fairness
action
learning
rejection
reward
routing
arrival
capacity
peak
aggressive
calls
link
greedy
policies
marbach
congestion
dziong
ss
departures
learned
transition
conservative
network
departure
traffic
exploration
ratios
markov
mitra
brown
service
packet
mason
aggregation
randomized
route
df
ja
rewards
configuration
gammaff
multiplexing
hop
decision
descriptor
allocation
bertsekas
tsitsiklis
tong
probability
gammavector
gammarandom
visited
violate
constraint
lookup
routes
networks
holding
rate
arrivals
stationary
heuristic
figs
broadband
training
aggregated
bandwidth
rates
meeting
gabor
formulates
sa
accepting
atm
states
transitions
develops
constrained
confidence
event
reject
fig
spends
feinberg
gallager
gammadirected
tauberian
approximators
rej
acceptg
earns
beutler
multimedia
configurations
allowable
payoff
sn
accepts
turned
normalized
routed
gammath
xn
node
acceptance
probabilities
actions
exponential
past
statistical
learn
watkins
departs
discrepancy
vs
semi
processes
violated
od
memoryless
bellman
progress
optimality
ross
curse
links
ongoing
simulations
bursty
gradually
metrics
maximization
rejecting
quantized
admitted
req
quality
control
learner
maximizes
destination
feasible
chain
truncated
simplified
simulation
straints
ignoring
collects
constraining
accept
contraction
period
source
neural
guarantees
met
history
transient
nth
stochastic
randomizes
fset
multiplexers
exponentialon
nips
rejectg
departuresg
telecommu
gammaconstraints
adhered
untruncated
earn
gammadimensional
capacity constraint
q learning
reinforcement learning
peak rate
fairness constraint
call arrival
admission control
call admission
congestion probability
qos constraints
optimal policy
state action
ss 0
q values
rejection ratios
cac problem
qos given
single link
new call
al 1998
action pairs
rl qos
q value
total reward
state aggregation
greedy policy
df ss
qos learned
state space
decision process
semi markov
markov decision
greedy qos
call departures
packet level
combined cac
non randomized
gammaff df
mason 1994
mitra et
rl policy
marbach et
rate allocation
call departure
z 1e
configuration x
statistical multiplexing
greedy peak
feasible action
action set
level qos
different policies
rejection ratio
dependent constraints
total rewards
policies exponential
route r
past dependent
aggressive capacity
link calls
states associated
state descriptor
rl algorithm
reward comparison
action space
state dependent
c c
stationary policy
call arrivals
conservative approach
et al
xn 1
state transitions
large state
lookup table
section develops
h tong
call class
policies learned
conservative capacity
network revenue
ongoing calls
randomized policies
multi link
x brown
learn q
aggressive approach
constrained semi
ratio discrepancy
gabor et
aggregated problem
truncated system
ffl gammarandom
m sa
rl peak
average reward
service types
policy obtained
link network
allowable configurations
violate qos
link case
learning qos
gammarandom exploration
action needs
learning converges
greedy total
network routing
qos requirements
stationary distribution
call admission control
learning for call
cac and routing
et al 1998
state action pairs
ss 0 ja
df ss 0
semi markov decision
class i call
number of calls
peak rate allocation
gammaff df ss
dziong and mason
mitra et al
rl qos learned
greedy qos given
marbach et al
z 1e gammaff
c c p
greedy peak rate
packet level qos
c a p
link i j
total reward comparison
calls in progress
constrained optimal policy
aggressive capacity constraint
call being turned
state s 0
state s n
comparison of different
markov decision process
quality of service
call arrival rate
single link case
violate the capacity
comparison of rejection
ffl gammarandom exploration
associated with call
rl peak rate
formulates the cac
feasible action set
constrained semi markov
control and routing
comparison of total
holding time 1
learned in fig
greedy total reward
rejection ratio discrepancy
gabor et al
conservative capacity constraint
due to rl
state dependent constraints
different policies exponential
bertsekas and tsitsiklis
markov decision processes
rejection rates figure
learned 3 rl
node 12 link
