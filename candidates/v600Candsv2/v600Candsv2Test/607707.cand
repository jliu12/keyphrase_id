watanabe
singularities
learning
eq
asymptotic
neural
pole
bayesian
amari
blowing
identifiable
perceptron
stochastic
kullback
expansion
dw
layered
parametric
statistical
algebraic
samples
analytic
density
poles
shun
murata
geometrical
generalization
units
ciently
likelihood
exp
estimation
training
regression
neighborhood
clarify
ichi
priori
dxdy
opper
sumio
su
artificial
hidden
layer
resolution
hierarchical
ups
aic
hironaka
haussler
machines
fisher
ect
ozeki
mixtures
curves
nh
inequality
meromorphic
zeta
unrealizable
fukumizu
atiyah
solla
tomoko
geometry
regular
nk
compact
singular
jw
inequalities
bias
trained
foregoing
entropic
sato
tishby
bic
largest
contained
mellin
conic
minimizes
probability
networks
variance
posteriori
gaussian
akaike
perceptrons
mackay
error
firstly
asymptotics
universal
satisfies
clarified
boltzmann
mathematically
definite
analytically
proven
levin
schwarz
algorithmically
mathematical
continued
realizable
extensively
manifolds
wavelets
rational
statistics
secondly
radial
curve
jensen
log
jacobian
degenerate
miki
dacunha
cousseau
hiroyuki
nakahara
combing
keisuke
castelle
neuromanifolds
hyperfunctions
shinomoto
abic
aoyagi
prehomogeneous
gassiat
identifiablity
reys
hyperparatemeter
prespective
kashiwara
holomorphic
paramaters
hyeyoung
perceptorn
merhav
rissanen
yamazaki
haikun
estimator
mixture
conditional
calculation
rank
cramer
ork
donsker
hagiwara
hartigan
florent
identifiability
machine
adopt
fujita
homogenous
shintani
ror
pth
subsection
speaking
interpolation
inference
empirical
chui
yamanishi
asymptotic expansion
generalization error
stochastic complexity
hierarchical learning
learning machines
learning machine
bayesian estimation
regular statistical
h w
f n
j z
three layer
true distribution
probability density
g n
true probability
parameter space
y x
hidden units
x w
neural networks
largest pole
p y
w j
training samples
parameter w
artificial neural
layer perceptron
watanabe 2001a
algebraic geometrical
w dw
learning curves
analytic function
parametric case
kullback information
non identifiable
density function
su ciently
priori distribution
regression function
algebraic geometry
true regression
watanabe 1999b
error g
q x
w 0
w w
neural computation
ciently large
q y
statistical model
g u
blowing ups
extensively large
shun ichi
independently taken
ichi amari
priori probability
statistical models
parametric model
eq 30
learning theory
probability distribution
neural network
w z
exp 2
samples independently
information matrix
analytically continued
kullback distance
sumio watanabe
exp nh
algebraic variety
asymptotic property
x dxdy
complexity f
statistical estimation
maximum likelihood
z w
fisher information
units k
eq 18
geometrical structure
asymptotic theory
n w
singular points
likelihood method
layered neural
resolution theorem
using samples
n satisfies
natural number
nk 2
parametric models
learning curve
x q
g x
e ect
approximation error
function j
input units
dimensional vectors
trained using
likelihood function
output units
conditional probability
learning model
eq 19
function approximation
ciently small
log p
log n
computation v
previous paper
eq 35
samples watanabe
true parameters
far smaller
levin tishby
p y x
y x w
three layer perceptron
hierarchical learning machines
artificial neural networks
probability density function
hierarchical learning machine
regular statistical models
regular statistical model
h w z
true regression function
watanabe 1999b watanabe
resolution of singularities
true probability distribution
q y x
generalization error g
error g n
parameter w j
su ciently large
number of training
function of w
priori probability density
z w dw
parameter that minimizes
support of w
true probability density
y x q
shun ichi amari
conditional probability density
w z w
log p y
algebraic geometrical structure
poles of j
samples independently taken
non parametric case
complexity f n
end of proof
respectively the largest
function j z
function approximation error
q x dxdy
perceptron with k
trained using samples
w w dw
using samples independently
stochastic complexity f
number of parameters
neural computation v
v w j
layered neural networks
maximum likelihood method
natural number n
x q x
ciently large n
w w 0
n is equal
su ciently small
given by eq
distribution is contained
arbitrary natural number
