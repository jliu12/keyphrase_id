nns
performances
nn
stepdisc
regression
bonnlander
ruck
selection
relevance
saliency
neural
leray
moody
czernichow
cibas
criterion
ecd
backward
ocd
obd
dorizzi
classification
derivatives
yacoub
refenes
hessian
feature
retraining
damage
pruning
validation
training
mutual
sv
weight
stopping
monotonous
mse
derivative
stepwise
perf
prediction
parametric
measures
wilks
kittler
frontier
criteria
wave
stop
fisher
correlated
multilayer
recognition
brain
forward
ard
statistical
coefficient
noisy
rossi
obs
learning
stopped
pathological
frontiers
discriminating
hypothesis
mclachlan
epanechnikov
souli
fogelman
ebd
saliencies
priddy
hajlmarsson
ssr
intensive
heuristic
entropy
prohibitive
sensitivity
weights
ingredients
computationally
eliminated
percentage
fukunaga
surgeon
fraser
regressor
aberrant
gustafson
axis
ranking
battiti
retrained
networks
outputs
cell
variance
risk
absolute
gaussian
narendra
perceptrons
mlp
bootstrapping
estimates
representative
rely
density
threshold
estimated
correlations
nonparametric
generalization
floating
estimation
correlation
fs
trained
heuristics
network
search
feedforward
perceptron
uncorrelated
cross
squared
hidden
wrt
selects
classical
meaningless
fitted
discriminant
attractive
modelling
bayesian
deletion
subsets
diagonal
dependencies
neurons
exhaustive
statistics
restrictive
statistic
difficulty
error
layer
differentiable
averaged
branch
confidence
salient
deletes
artificial
selecting
discarding
regularization
kernels
gradient
variation
marginal
covariance
costly
selected
determinant
gasca
swinney
wertz
pertinence
hermans
kullbak
econometrics
representativeness
rdle
infraction
committees
stahlberger
multinormal
baxt
utans
feature selection
variable selection
selection methods
selected variables
mutual information
neural networks
choice criterion
validation set
weight pruning
relevance measures
stop criterion
l l
generalization error
sv p
order methods
wave problem
p selected
backward search
different variable
non linear
performance comparison
variables perf
selected variable
correlated variables
stepdisc 4
feature evaluation
good performances
non pathological
axis percentage
cell damage
method p
bonnlander 4
variable set
x l
pattern recognition
selection method
variable subset
evaluation criterion
parametric methods
c r
cross validation
neural network
evaluation criteria
f x
r p
fisher test
model independent
pure noise
moody 5
gaussian problem
stepwise methods
fs p
relevance measure
variable relevance
two gaussian
refenes 5
dorizzi 5
czernichow 5
architecture selection
original wave
floating search
x f
linear models
input variables
al 1996
several authors
noise variables
based feature
remaining variables
final prediction
prediction error
training set
non parametric
l x
methods use
best subset
best performances
input variable
class j
two weight
p 1
et al
correct classification
f c
several methods
absolute values
authors use
optimal search
computed using
computationally intensive
density estimation
linear regression
al 1994
p b
model selection
search methods
diagonal approximation
cibas leray
since variables
backward methods
noisy wave
regressor f
p variables
variables bonnlander
refenes et
fogelman souli
representative methods
branch bound
dependence measure
stop criteria
discriminating power
relevance criterion
fixed threshold
data refenes
early brain
set sv
early cell
derivative absolute
wilks lambda
nn variable
variable selection methods
different variable selection
feature selection methods
l l l
p selected variables
f c r
comparison of different
method p selected
feature selection method
selected variable set
selected variables perf
r p 2
stepdisc 4 2
bonnlander 4 3
l l x
x l l
may be used
either for regression
selection is stopped
original wave problem
l x x
moody 5 2
dorizzi 5 2
pure noise variables
czernichow 5 2
refenes 5 2
selection for neural
using a validation
methods which use
two gaussian problem
f x l
percentage of selected
compute the mutual
sv p 1
et al 1996
take into account
f x f
x f x
non linear models
classification and regression
proposed to use
p a b
sets of variables
et al 1994
set sv p
using mutual information
methods in feature
perf stepdisc 4
optimal cell damage
o the importance
c r non
l sv p
percentage of variables
fs p forward
variable is found
variables selected y
cibas 5 3
table 1 selection
p th variable
variable is usually
early cell damage
priddy et al
noisy wave problem
bonnlander s method
validation set since
correlations between variables
methods vs percentage
nn is trained
stopping the selection
families of methods
refenes et al
gustafson and hajlmarsson
bottom of table
several methods propose
time series modelling
proposed a series
simple backward search
allows to take
validation or algebraic
fitted for nns
pathological data refenes
feature selection pattern
data refenes 5
variables the second
ruck 5 2
sub optimal search
order methods several
leray 5 3
ingredients of feature
partial correlation coefficient
zero order methods
regression or classification
propose two weight
