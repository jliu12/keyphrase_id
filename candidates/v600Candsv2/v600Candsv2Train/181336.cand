chunk
chunks
workers
scheduling
hs
scheduler
worker
msg
workload
load
gss
mhlw
loop
redistribution
hsp
migrated
processors
static
loaded
hsr
mllw
messages
tc
loops
requesting
processor
locality
send
remote
traveling
dsss
sss
mm
iterations
migration
transferlimit
numa
duties
body
trapezoid
lightly
heavily
charge
exit
memories
balance
unbalance
request
parallelized
idle
efficiency
migrations
queue
finished
message
break
compile
remotely
multiprocessors
affinity
loopbodysize
firsttime
classification
overhead
cm
self
execute
executed
irregular
balancing
ads
sends
owner
guided
machines
hybrid
lds
transitive
partitioned
pending
multiprocessor
msgs
migrate
uniform
outermost
schedulings
replicated
memory
factoring
evenly
synchronization
schedule
closure
doall
passing
performer
uler
multiplication
schedul
redistributing
subtasks
communication
returned
sched
dead
failed
received
alive
heavy
counters
slowest
iteration
accomplished
nested
arrange
shared
gg
ss
foreverf
doserial
potencial
sensi
polychronopou
tawbi
dynamicps
workloadredistribution
unbalances
dynamicp
chaimpaign
supercomputers
benchmark
periodically
sent
adjoint
parallelism
classify
else
specially
elapsed
galicia
tapering
orchestrate
hap
xunta
sideration
innermost
allocation
matrix
strategies
processed
balanced
convolution
actions
execution
hamidzadeh
tiveness
ories
activated
prevented
tries
slow
bodies
allocating
pens
bet
intolerable
lilja
doacross
cmmd
cuted
dynamic
coordination
dynamic scheduling
self scheduling
static scheduling
local chunks
non executed
local chunk
loop body
traveling exit
remote chunk
loop scheduling
distributed memory
local queue
data locality
load messages
messages traveling
parallel loop
dynamic level
loaded worker
exit break
load migrated
scheduling level
load finished
requesting processor
load request
local loop
processors efficiency
break case
heavily loaded
parallel loops
worker sends
migrated msg
load needed
finished msg
optimal static
data returned
workload redistribution
static level
scheduling algorithms
distribution scheme
guided self
case load
next non
local memories
communication overhead
executed chunk
request failed
scheduling duties
request msg
returned msg
scheme chosen
non uniform
data distribution
becomes idle
efficiency number
synchronization overhead
scheduling strategies
loaded processors
local computations
shared memory
lightly loaded
memory machines
cm 5
workload counter
send load
needed msg
workload counters
redistribution process
scheduling hs
failed msg
hs performs
uniform parallel
remotely processed
chunk else
trapezoid scheduling
remote chunks
classification list
gss 0
scheduling parallel
memory multiprocessors
static schedule
compile time
overhead associated
data partitioned
f send
parallelized loop
hybrid scheduling
message passing
input matrix
executed local
matrix multiplication
processed data
pending messages
fully dynamic
else send
transitive closure
parallel programs
processor becomes
case data
scheduling performance
communication cost
processor allocation
memory multiprocessor
static load
load among
major concern
outermost loop
scheduling strategy
symbolic analysis
workers alive
remaining chunks
purely dynamic
dynamic schedulings
supercomputers utilizing
multidimensional loop
chunk number
send a load
messages traveling exit
performance of hs
local loop body
traveling exit break
load finished msg
next non executed
load migrated msg
guided self scheduling
number of processors
optimal static schedule
exit break case
data returned msg
load request msg
non executed chunk
heavily loaded worker
dynamic scheduling strategies
number of processors0
data distribution scheme
static and dynamic
scheduling on distributed
number of workers
static scheduling level
chunks of iterations
remotely processed data
empty no messages
workers are dead
near optimal static
processed data block
distribution scheme chosen
dynamic scheduling level
processors efficiency number
hybrid scheduling hs
request failed msg
msg to mhlw
chunk else send
load needed msg
dead my local
non executed local
non uniform parallel
break case load
level in hs
among the processors
distributed memory machines
scheduling parallel loops
communication overhead associated
designed for shared
processor becomes idle
dynamic scheduling algorithms
memory parallel machines
distributed memory multiprocessor
queue is empty
local queue execute
break case request
scale parallel processor
last local chunk
redistribution of load
migrated msg execute
received f case
exit break gg
dynamic scheduling duties
message received f
fully dynamic schedulings
passing distributed memory
executed local chunk
passing scheduler based
loaded worker send
execute the chunk
break case data
efficiency of hsp
remove a chunk
associated with migrations
