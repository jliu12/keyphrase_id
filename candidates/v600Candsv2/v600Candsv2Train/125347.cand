sentences
verb
connectionist
boy
chases
mcclelland
boys
nouns
grammatical
noun
representations
localist
verbs
sentence
linguistic
clause
hidden
elman
neural
recurrent
girls
press
grammar
clauses
word
trajectories
training
lexical
activation
kawamoto
cleeremans
servan
cognition
hinton
dog
network
agreement
cognitive
networks
chase
rumelhart
units
learning
girl
connectionism
dogs
ungrammatical
representational
learned
learn
trained
hare
srn
schreiber
language
mary
pca
feed
sees
plural
pollack
dimensions
9d
prediction
phonology
miikkulainen
spoken
principal
stimuli
encode
constituent
ended
neuroscience
relationships
taught
insert
syntactic
internal
constituency
pronoun
wermter
gasser
mental
henderson
8d
predictions
corpus
apparently
richness
phenomena
filler
cats
object
items
smolensky
compositional
cat
broke
consisted
verbal
embedding
singular
regimen
kirsh
inanimate
macwhinney
maqsood
pylyshyn
ajith
kutas
imran
lexicon
dyer
paradox
demonstrations
nature
soft
predict
task
human
stefan
outset
bates
persuasively
garfield
stinchcombe
underlined
regularities
learns
st
sanger
touretzky
cottrell
fodor
eigenvectors
rules
structural
category
abstract
encodes
mozer
gelder
contextually
lock
optional
accommodated
linguistics
facts
production
chomsky
sejnowski
skeletonization
pdp
sheila
ucsd
jordan
phrases
theme
theories
subject
role
shortcoming
generalizations
houses
clausal
listeners
possessed
cooccurrence
inputs
rosenberg
causal
minds
retract
emergent
tendency
agent
subordinate
dell
animate
subjects
trajectory
ow
feedforward
direct object
relative clauses
relative clause
argument structure
connectionist models
distributed representations
verb argument
chases boy
simple sentences
hidden units
hidden unit
simple recurrent
insert figure
complex sentences
structural relationships
main clause
linguistic representations
internal representations
neural networks
connectionist networks
grammatical structure
elman 1990
state space
recurrent network
ended nature
unit activation
schreiber cleeremans
john mcclelland
boy chases
cleeremans mcclelland
clause verb
localist approach
servan schreiber
complex structural
open ended
constituent structure
st john
hinton 1988
network predictions
localist representations
mary chases
sees girl
context units
principal component
two sentences
mcclelland 1986
computational power
current work
prediction task
boys chase
relative pronoun
underlined words
feed cats
representational space
mcclelland 1989
kawamoto mcclelland
initial noun
connectionist account
singular verb
pollack 1988
miikkulainen dyer
apparently open
causal properties
spoken word
chases feed
connectionist model
words maximum
subject nouns
unit patterns
fixed resource
rumelhart mcclelland
mean sentence
recurrent neural
hidden layer
language processing
natural language
human cognition
stefan wermter
direct objects
sentence length
recurrent networks
figure 9d
predictions following
minimum 3
james henderson
context dependent
d o
lexical items
abstract representations
classical models
resource system
net work
noun phrases
representations may
word recognition
limited resources
output units
internal states
verb argument structure
mcclelland in press
trajectories through state
nature of language
st john mcclelland
simple recurrent network
complex structural relationships
boy chases boy
hidden unit activation
servan schreiber cleeremans
open ended nature
schreiber cleeremans mcclelland
requires a direct
boy who chases
main clause verb
recurrent neural networks
presence of relative
mean sentence length
simple recurrent networks
apparently open ended
kawamoto mcclelland 1986
language be accommodated
boys who mary
space for sentences
minimum 3 words
rather than hard
fixed resource system
network has learned
mary chases feed
hidden unit patterns
sequences of words
sensitivity to context
principal component analysis
trained to predict
sentences 3a d
localist approach would
d o optional
based on neuroscience
nouns verbs etc
graph of network
john mcclelland 1989
spoken word recognition
argument structure facts
dimensions of variation
order of words
follow the verb
unit activation vectors
neural networks proceedings
move the network
sentences with multiple
b verb argument
maximum 13 words
broke the window
hinton mcclelland rumelhart
hidden unit vectors
solves the task
learn the task
account of language
although distributed representations
relative clause modifying
agreement in simple
nature of linguistic
richness of linguistic
computational architectures based
fodor pylyshyn 1988
sejnowski rosenberg 1987
structure sensitive operations
predict the order
network predictions following
