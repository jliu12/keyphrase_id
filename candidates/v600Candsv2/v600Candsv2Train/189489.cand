learning
training
sampling
selective
neural
trained
ive
querying
uncertainty
active
concept
backpropagation
network
na
sg
queries
concepts
neuron
deltaw
bias
learner
region
engelbrecht
generalization
networks
draw
sampled
feedforward
cohn
classifiers
hwa
deltag
background
disagree
drawing
selectively
train
uncertain
valiant
haussler
classification
rebecca
inductive
layer
linguistics
configurations
oversampling
speech
membership
hidden
learnable
baum
configuration
mining
huan
learned
drawn
rectangles
batch
maytal
nicks
motherese
backpropagate
tsechansky
edmonton
sample
exhibited
sigkdd
mooney
ji
consistent
alberta
utility
infinitesimally
pabitra
saar
weights
raymond
randomly
unlabeled
converge
superset
prem
miles
melville
blumer
motoda
infant
trainable
osborne
pathological
mackay
unclassified
classifying
classify
classifies
mitchell
angluin
intelligent
steeper
rumelhart
pass
dmitry
chervonenkis
outweigh
canada
provost
squashing
target
domingos
supervised
vapnik
harrold
domain
fit
limitations
oracle
implements
iterations
acquisition
mitra
random
santa
preference
failure
passive
discovery
recognition
pac
unknown
bayesian
learn
thresholded
secure
baseline
corpora
fundamenta
informaticae
sydney
dimensions
hwang
learners
threshold
foster
distribution
baker
regions
labeling
domains
hiroshi
banff
pedro
error
committee
modes
iteration
corpus
letters
kiyonori
barbanon
soderland
vorobeychik
backpropa
adomavicius
tieu
baldridge
emre
fernald
anuradha
velipasaoglu
ruhlen
hockenmaier
balcan
gediminas
madani
baah
ratsaby
brits
deduplication
smartedit
aggoune
bowring
steedman
kocz
predisposition
selective sampling
active learning
version space
neural network
concept c
na ive
training set
training examples
concept class
random sampling
neural networks
ive querying
sg network
inductive bias
training example
distribution p
selectively sampled
deltaw ji
pass sampling
power system
querying algorithm
background examples
target concept
concept learning
machine learning
example x
set size
networks trained
actual training
p engelbrecht
valiant 1984
sampled networks
randomly sampled
sampling approach
generalization error
sampled data
hidden layer
feedforward neural
network implementation
rebecca hwa
space search
unknown target
al 1990
sg net
haussler 1989
valued threshold
g networks
engelbrecht sensitivity
neuron j
mitchell 1982
sampling pass
network generalization
background example
background learning
computational linguistics
membership queries
initial random
axis parallel
input distribution
class c
learning rate
parallel rectangles
sample selection
sampling may
failure modes
consistent concepts
specific configuration
network c
learning algorithm
vs training
connection weights
training data
point x
international conference
two dimensions
network configurations
backpropagation algorithm
distribution information
two networks
error vs
huan liu
inside r
learning p
data mining
training algorithm
intelligent user
output node
general concept
configuration c
within r
network configuration
acm sigkdd
learning proceedings
learning v
learning algorithms
alberta canada
first hidden
trainable classifiers
specific general
network refers
examples alone
background patterns
new background
ive algorithm
r s m
region of uncertainty
training set size
na ive querying
x t x
version space search
selective sampling approach
learning from examples
concept class c
training example x
ive querying algorithm
examples s m
neural network implementation
et al 1990
unknown target concept
engelbrecht sensitivity analysis
real valued threshold
rectangles in two
sampling pass sampling
neural network generalization
p engelbrecht sensitivity
randomly sampled data
concept c 2
selective sampling may
error vs training
background learning rate
active learning proceedings
vs training set
axis parallel rectangles
set s m
feedforward neural networks
intelligent user interfaces
according to p
conference on intelligent
machine learning v
perform selective sampling
describe the sg
neural networks fundamenta
simple concept classes
arbitrary distribution p
b on right
configurations that disagree
done while david
power system static
learning is provably
increased concept complexity
concept c 1
selection for statistical
iteration of selective
pass sampling pass
valued threshold function
selectively sampled data
set size 50
need to draw
networks fundamenta informaticae
generalization when learning
calculate the output
first hidden layer
iterations of 10
maytal saar tsechansky
selectively sampled networks
querying various parts
denotes the true
adjusting weights according
case of maintaining
network adjusting weights
actual training examples
analysis for selective
initial random sampling
provably more powerful
na ive algorithm
batch of training
hwa sample selection
size for random
learned by 150
system static security
