dem
cwa
mwa
tasks
quota
scheduling
load
twa
balancing
subcube
walking
node
balanced
hypercube
avg
hops
subcubes
overloaded
prescheduling
runtime
processors
locality
normalized
cube
balance
subtree
mesh
copt
sends
communication
underloaded
exchange
migrated
calculates
calculated
nodes
row
child
jj
weight
dimension
quotas
receive
bitwise
sent
ffi
ching
topologies
multiprocessors
calculation
topology
willebeek
lemair
task
grain
workload
balances
spends
evenly
parent
scheduled
schedule
initiated
cooperate
fully
costs
static
tree
maximize
records
jobs
submesh
reeves
parallelized
send
adaptive
incremental
schedul
transmitted
preorder
vectors
utilizes
yeh
mod
liao
gammaj
ary
exchanged
overhead
jung
capacity
processor
minimized
imbalance
yeon
cdem
sungchun
hlne
hwakyung
nrics
rips
vivien
cmwa
kook
mourtos
xianliang
ximing
ojsterek
acwn
tdem
ccwa
renard
brest
janez
schedules
scan
multicomputers
heuristic
unbalanced
iteration
receiver
loads
randomized
kim
cubes
ready
hypertool
topt
quickness
umer
legrand
viljem
gamman
compile
migration
massively
collect
exchanges
update
spirakis
pyrros
antonis
garofalakis
sunggu
milan
communications
collects
phi
minimize
exchanging
activity
reserved
automates
abstractparallel
wook
chung
corrected
quality
spend
calculate
arnaud
matroids
mcp
byung
eight
sender
minimizes
multipro
jang
rescheduled
listed
sacrifices
nv
spread
sharing
rim
hee
maximizes
traversal
scalability
keller
nicol
stealing
accurately
ahmad
edge
parallel scheduling
walking algorithm
load balancing
dem algorithm
w avg
task hops
load information
fully balanced
local tasks
communication steps
normalized communication
cost weight
balanced cases
fully balance
scheduling algorithms
normalized cost
cube walking
receive jj
global information
global load
communication cost
tree walking
scheduling algorithm
maximize locality
node j
static scheduling
task exchange
tree hypercube
twa cwa
mesh walking
communication costs
balanced load
non local
dynamic problems
j tasks
system phase
iteration 0
quota q
node calculates
algorithm mwa
quota calculation
w tasks
sends two
w vector
weight processors
load calculation
normalized locality
algorithm cwa
runtime parallel
sum reduction
processors 0
processors processors
total number
dynamic scheduling
optimal scheduling
collection perform
receive tasks
sends three
information collection
w k
step 4
dynamic load
many tasks
tasks per
load distribution
evenly divided
node pairs
node 7
node d0
weight dem
ffi vectors
l tasks
mwa algorithm
quality load
overloaded subcube
algorithm twa
subcubes f0
quota proof
collect load
mwa algorithms
four nodes
minimum number
three tasks
node computes
processors figure
j l
runtime scheduling
negative cycle
processors cooperate
balancing decision
dimensional subcube
algorithms minimize
static problems
node 5
average load
k dimensional
r tasks
average number
w 0
running example
test cases
distributed memory
vectors q
average weight
number of tasks
tasks to node
values of w
balance the load
non local tasks
number of task
fully balanced cases
normalized cost weight
tasks from node
cube walking algorithm
cwa or mwa
node is equal
tree walking algorithm
hypercube and mesh
j i 0
parallel scheduling algorithm
global load information
minimize the communication
cwa and mwa
avg and r
mesh walking algorithm
normalized communication costs
j is child
parallel scheduling algorithms
number of non
number of communication
w and ffi
walking algorithm cwa
execution of twa
global information collection
sends three tasks
sends two tasks
information collection perform
processors 0 5
runtime parallel scheduling
average load calculation
dynamic load balancing
jj i 0
tasks as well
tasks per node
algorithms for tree
shown in figure
number of nodes
quality load balancing
collect load information
tasks in node
optimal scheduling problem
load calculation 3
walking algorithm twa
calculation 3 quota
cost weight dem
normalized communication cost
k dimensional subcube
walking algorithm mwa
high quality load
therefore the total
tasks are sent
able to fully
jobs or tasks
load balancing decision
divided by n
large memory space
listed as follows
number of communications
shows the normalized
