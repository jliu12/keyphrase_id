nge
hyperrectangle
hyperrectangles
bnge
knn
fwmi
training
rectangles
nearest
salzberg
seeds
domains
wettschereck
hungarian
nonge
cv
rectangle
obnge
theta
neighbor
dietterich
overlapping
weights
exemplar
learning
voting
aha
exemplars
iris
feature
nested
bias
mutual
nn
letter
fuzzy
artmap
batch
cleveland
noc
missing
validation
waveform
inferior
nesting
axis
seed
statistically
recognition
nintervals
ngecv
fmmc
jurisica
replmissfeatures
createhyperrectangle
hypotheses
classification
weight
inappropriate
incremental
heuristic
gammann
cross
poor
greedy
task
features
cover
superior
irrelevant
generalized
merge
gles
classifier
tasks
match
inappropriateness
bourennane
ffmc
nfeatures
distance
closest
ties
trained
percentage
decision
detrano
luaces
ranilla
mitran
bahamonde
mining
tested
euclidean
led
carpenter
neural
substantially
worse
permits
venkateswarlu
overlap
eleven
rectan
kolluri
discovery
modifications
inductive
glasgow
evidence
initialized
significantly
nest
noisy
neighbors
appendix
provost
irvine
adjusted
simpson
interpretable
outperforms
disallows
classified
classes
symbolic
conducted
classify
oscar
generalize
permit
steven
igor
leave
quinlan
employed
foster
covers
sensitivity
adopted
inside
boundaries
covered
learned
fw
experiment
classifications
avoids
poorly
toavoid
bohnebeck
hyperrectan
arshadi
basedlearning
mitkas
venerable
niloofar
indatabases
brockhausen
athanasiadis
ungeneralized
browsingfor
ramamohanarao
luft
hyperrect
wolfley
bouillant
detitta
rectanglesconstructedby
deeps
fortier
jinyan
kotagiri
performs
policy
hybrid
attains
theta theta
feature weights
overlapping rectangles
training examples
nearest hyperrectangle
nearest neighbor
c c
nge cv
task c
d wettschereck
mutual information
axis parallel
nested rectangles
g dietterich
table a2
bnge fwmi
training example
neighbor algorithm
nge algorithm
letter recognition
training set
cross validation
second match
nge limit
hyperrectangle comparison
generalized exemplar
detailed numbers
salzberg 1991
k nearest
c theta
task b
aha 1990
missing features
match heuristic
knn fwmi
led 7
iris task
second nearest
new hyperrectangle
starting seeds
generalized exemplars
h j
new example
compare h
fuzzy artmap
greedy nge
irrelevant features
statistically significant
performance relative
significantly inferior
information feature
leave one
c task
decision boundaries
nge fwmi
nearest hyperrectangles
overlapping hyperrectangles
cleveland hungarian
nge called
lower f
rectangle c
however nge
hungarian voting
nearest rectangle
batch nge
feature weight
domains nge
nge without
point differences
waveform 40
closest e
search algorithm
test example
output class
percentage point
best version
simple nearest
machine learning
test set
single point
training data
batch algorithm
significantly better
test examples
decision boundary
entire training
significantly worse
search procedure
euclidean distance
significantly superior
voting domains
original nge
nested hyperrectangles
outperforms nge
replmissfeatures h
point rectangles
seeds nge
letter hungarian
point hyperrectangles
parallel hyperrectangles
one hyperrectangle
nge relative
theta theta theta
performance of nge
c c c
better than nge
c theta theta
nearest neighbor algorithm
number of seeds
c c theta
nge and knn
nearest hyperrectangle comparison
version of nge
appendix for detailed
second match heuristic
k nearest neighbor
one out cross
number of starting
iris task c
number of hyperrectangles
domains and significantly
inferior to knn
mutual information feature
information feature weights
see table a2
relative to figure
nge s performance
theta theta c
theta c c
order of presentation
task c task
nested or overlapping
percentage point differences
versions of nge
shown are percentage
relative to knn
simple nearest neighbor
experiment we tested
entire training set
number of training
axis parallel hyperrectangles
second nearest hyperrectangles
difference between nge
single point rectangle
weights were adjusted
implementation of nge
hungarian and voting
task a recognition
nge a indicates
relative to nge
nearest neighbor nn
led 7 domain
h second closest
batch algorithm bnge
differences between nge
feature weight mechanism
salzberg s method
replmissfeatures h e
variant of nge
hyperrectangle is zero
gammann on task
axis parallel rectangle
training set test
fuzzy min max
set test set
tasks a b
nearest neighbor algorithms
p 0 05
machine learning v
detailed numbers 5
bias of nge
fwmi and bnge
however in domains
