dts
mpo
slice
rcp
fmm
dag
volatile
schedule
fu
yang
sparse
cholesky
tasks
rapid
slices
scheduling
dcg
processor
lu
irregular
dependence
rma
objects
inspector
megaflops
memory
processors
gerasoulis
pt
factorization
task
permanent
schedules
object
management
priority
executor
map
px
merging
dags
cq
ra
multipole
scheduled
jiao
remote
rec
ddg
parents
ready
article
particle
meiko
allocate
active
goodwin
dtsm
parallelization
usage
acyclic
rothberg
tot
suspended
owner
inc
overhead
parallelism
tw
deadlock
priorities
messages
allocated
tx
dead
send
pivoting
executable
packages
nonzeros
recycling
unscheduled
executing
nonexecutable
requirement
static
topological
accessed
reads
execution
cray
submatrices
efficiency
addresses
heejo
addrs
buffering
mem
execute
edges
sarkar
sunggu
blumofe
shmem
elimination
ins
normally
cilk
tp
communication
sequential
degradation
block
waiting
mixed
ordering
message
children
sent
particles
read
clusters
million
executed
downward
heuristics
upward
fig
stage
blelloch
schreiber
legal
destination
chain
2d
deliver
deallocate
blocked
activities
scalability
column
circular
scientific
1d
address
exit
writes
speedup
assignment
matrix
siblings
ssa
delta
granularity
architectures
produced
content
contradiction
numa
maps
access
addr
strongly
reused
log
consistency
allocation
req
leaf
sung
sci
jong
grained
freed
multiprocessor
live
vi
aggressive
guided
ta
c fu
data objects
memory management
data object
sparse lu
p x
fu delta
volatile objects
active memory
sparse cholesky
parallel time
pt inc
column block
map pt
slice merging
dts schedule
volatile object
ready task
data node
task graph
run time
memory requirement
dependence path
p y
space efficiency
inc map
processor px
management scheme
time efficient
volatile data
processor p
irregular computations
dts algorithm
address packages
yang 1996b
volatile space
m produced
al 1995
efficient scheduling
space needed
space requirement
cholesky factorization
dependence edges
sparse matrix
processor assignment
reduction ratio
inspector stage
active messages
log v
time efficiency
space usage
scheduling algorithms
permanent data
direct remote
yang 1997
l slices
dependence complete
slice l
assignment r
fast multipole
multipole method
d 7
based sparse
fast communication
e log
distributed memory
object m
object d
sequential space
schedule execution
parallel irregular
time priority
memory scalability
data access
scheduling heuristics
merging algorithm
available amount
million nonzeros
given dag
exit task
ready tasks
block based
data nodes
path information
meiko cs
critical path
strongly connected
o e
node d
v log
per processor
p 32
et al
proposed techniques
memory machines
computes rule
remote memory
address buffers
permanent objects
space priorities
requirement reduction
address consistency
circular chain
x mpo
1d column
fu 1997
jiao 1996
fu table
using dts
fmm computation
map allocate
address package
task ordering
gerasoulis 1992
memory managing
processor end
lu 100
mixed granularity
yang and c
active memory management
c fu delta
fu and yang
map pt inc
task t x
memory management scheme
processor p x
pt inc map
inc map pt
sparse cholesky factorization
et al 1995
mpo and dts
block based sparse
o e log
set of tasks
critical path information
fast multipole method
direct remote memory
data node d
ra and cq
space for d
parallel irregular computations
associated with data
distributed memory machines
meiko cs 2
support for parallel
owner computes rule
sparse lu factorization
remote memory access
ratio x mpo
volatile space requirement
c fu table
requirement reduction ratio
ready task list
schedule is executable
yang and gerasoulis
volatile data objects
based sparse lu
1d column block
processors memory requirement
memory requirement reduction
column block based
e log m
computations on distributed
reduction ratio x
time efficient scheduling
lu 100 75
processor assignment r
data access patterns
total object space
according to property
column block k
x is associated
comparison of memory
given a dag
available memory space
parallel sparse cholesky
x t y
v log v
computation and communication
number of processors
figure 1 c
waiting to receive
number of slices
factorization with partial
sparse gaussian elimination
static single assignment
amount of memory
run time support
