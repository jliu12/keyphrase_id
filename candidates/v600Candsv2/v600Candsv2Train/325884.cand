trees
datasets
regression
classifiers
subnode
attributes
classification
dataset
smoothed
quinlan
surrogate
training
neural
breiman
learning
attribute
witten
tree
numeric
pruned
versicolor
decision
colic
umt
glass
smoothing
breast
leaf
lr
tumor
lymphography
virginica
instances
srt
cr
missing
nominal
unsmoothed
seventeen
horse
iris
hypothyroid
soybean
classjx
audiology
probability
leaves
probabilities
accurate
oblique
eibe
inducer
waikato
torgo
opaque
prediction
hepatitis
ionosphere
vowel
autos
kernel
heart
sick
anneal
statlog
setosa
smooths
indians
hollow
sharp
il
duncan
pima
estimators
disastrous
predicted
significantly
potts
smyth
outperforms
sonar
split
labor
cedure
cancer
uci
outperform
frank
sixteen
german
waveform
circles
fifteen
target
accuracy
ten
splitting
vote
classifications
wang
australian
learner
seagrass
quinlans
enko
retrofitted
karras
ressom
panayotou
virnstein
zoo
gyoerfi
accom
appice
floriana
solomatine
intimates
tweedale
incomprehensibility
saso
srirangam
zorkadis
musavi
modated
softened
twenty
predicting
paired
density
deleted
vehicle
correcting
setiono
lagoon
rulequest
donated
siek
ceci
subnodes
sammut
soklic
zwitter
classifier
surprisingly
binary
validation
thirty
art
trigg
holmes
lugosi
soften
model trees
model tree
linear regression
classifiers based
regression functions
decision tree
smoothed model
decision trees
regression trees
right subnode
missing values
class probabilities
class probability
using model
training instances
significantly less
binary attributes
machine learning
p il
attributes target
surrogate split
smoothed regression
less accurate
neural networks
witten 1997
pruned decision
trees generated
smoothing process
classification problems
accurate classifiers
probability functions
left subnode
trees produced
primary tumor
standard datasets
oblique class
horse colic
linear model
quinlan 1993
art decision
value v
predicted class
kernel density
p cr
conditional class
class boundaries
linear models
learning v
class value
class problem
two attributes
probability function
trees machine
first question
outperforms m5
waveform noise
nominal attribute
missing value
continuous classes
trees srt
unsmoothed model
heart statlog
trees umt
way classification
combined linearly
sharp step
significantly more accurate
trees for classification
using model trees
significantly less accurate
linear regression functions
accuracy of classifiers
smoothed model trees
based on smoothed
wang and witten
datasets and significantly
pruned decision trees
generated by m5
smoothed regression trees
class probability functions
accurate on five
conditional class probability
numeric and binary
model tree algorithm
compare the accuracy
split s v
machine learning v
figure a 1
based on model
used for classification
table 3 shows
trees machine learning
multi class problem
model trees umt
whenever the model
technique of model
induction of model
performance of model
shows that m5
class probability function
model tree inducer
thirty three standard
tree with linear
produced by c5
generated by c5
sharp step function
described by wang
outperforms m5 0
linear regression lr
