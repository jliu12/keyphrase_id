oc1
fat
moc1
moc2
margin
pdt
perceptron
impurity
pdts
margins
twoing
fold
classifiers
shattering
prognosis
hyperplane
cv
learning
di
trees
decision
tree
gdt
topdown
housing
generalization
separating
cancer
overfitting
erence
pruning
erent
validation
svm
split
enlarging
vc
outperforms
hyperplanes
vapnik
cross
accuracy
kasif
bupa
classification
bright
paired
capacity
instances
classifier
node
pima
iris
covering
perceptrons
4em
4192
c4
separable
subsamples
inducer
mtr
00
growth
flexibility
shattered
ect
bias
splits
category
heart
hirsch
multicategory
randomized
xy
discrimination
controlling
soft
training
salzberg
variances
risk
sample
stopping
dataset
nodes
breast
chervonenkis
leaf
erently
oc1 on
perceptron decision
10 fold
the margin
of oc1
large margin
and oc1
data sets
moc1 and
decision trees
fold cv
as oc1
twoing rule
impurity measure
and moc2
outperforms oc1
fold cross
separating hyperplane
the 10
fat shattering
decision nodes
cross validation
of moc1
of moc2
10 data
fat and
the generalization
generalization error
of instances
decision tree
the split
the tree
optimal separating
than oc1
support vector
the di
the impurity
w number
average accuracy
oc1 and
modified twoing
cv average
by oc1
fat moc1
capacity control
x x
of fat
category i
instances on
in category
di erence
higher means
di erent
the margins
large margins
sets studied
oc1 10
split i
algorithm fat
shattering dimension
fat has
oc1 pdt
moc2 has
oc1 fat
moc2 and
a perceptron
input space
the decision
tree size
generalization performance
enlarging the
the fat
mean than
margin at
paired t
soft margin
smaller mean
vector machines
current node
margin the
function class
margin in
performs as
not significant
tree sizes
for fat
the covering
margin is
means and
instances in
perceptron decision trees
10 fold cross
10 fold cv
the 10 fold
fold cross validation
outperforms oc1 on
moc1 and moc2
number of instances
10 data sets
of the split
perceptron decision tree
as oc1 on
x x x
instances in category
the 10 data
oc1 on the
in category i
of instances on
instances on the
the decision nodes
data sets and
the di erence
of the 10
w number of
fat moc1 and
modified twoing rule
optimal separating hyperplane
that of oc1
of instances in
the impurity measure
the optimal separating
of the margin
the input space
the generalization error
them are significantly
split i e
and performs as
the split i
enlarging the margin
category i on
a perceptron decision
e w number
well as oc1
the fat shattering
data sets studied
fat and oc1
sets studied in
fat shattering dimension
lemma 3 7
support vector machines
on the generalization
generalization performance of
studied in 18
the tree size
paired t test
are significantly higher
is a gdt
on 9 out
fold cv average
than oc1 on
a gdt over
the node classifiers
line it indicates
large margin is
the modified twoing
slightly smaller mean
of 10 data
moc1 outperforms oc1
oc1 on it
the large margin
moc2 and oc1
the split number
oc1 fat moc1
t test is
cv average accuracy
moc2 uses a
of oc1 fat
with large margin
means and p
mean than oc1
sets and performs
versa the figure
measure the learning
point is above
oc1 if the
x p value
versus oc1 if
significant x y
results of moc1
the twoing rule
