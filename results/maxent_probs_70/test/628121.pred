folding networks
empirical error
fat shattering
shattering dimension
function class
learning algorithm
initial context
folding architecture
real error
activation function
valid generalization
small empirical
distribution independent
concrete learning
activation functions
vc dimension
forward part
input height
folding network
maximum input
information theoretical
standard feed
r l
input tree
structured data
recursive part
underlying regularity
folding architectures
theoretical learnability
neural networks
generalization error
input space
processing dynamics
combinatorial quantity
distribution dependent
context neuron
sigmoidal function
context neurons
unlimited size
recurrent neural
cannot exist
valued function
o w
distributed representation
y 2
fat shattering dimension
bounds on the
small empirical error
number of examples
maximum input height
probability of high
deviation of the
algorithm with small
us to derive
information theoretical learnability
initial context y
independent uced property
empirical error and
distribution independent uced
error and the
theoretical learnability of
stratification of the
concrete learning algorithm
valued function class
folding networks a
shattering dimension of
learnability of folding
recurrent neural networks
explicit bounds on
bounds for the
vc dimension of
learning algorithm with
error from the
learning algorithm is
derive explicit bounds
real error of
tree structured inputs
time series prediction
