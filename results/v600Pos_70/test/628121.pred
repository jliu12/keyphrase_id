folding
shattering
learning
trees
inputs
networks
generalization
bounds
encoding
dynamics
dealing
architectures
decoding
mapping
generalizes
concerning
folding networks
empirical error
fat shattering
shattering dimension
function class
learning algorithm
initial context
folding architecture
real error
activation function
valid generalization
distribution independent
concrete learning
activation functions
vc dimension
forward part
high trees
folding network
context y
input trees
r l
input tree
derive bounds
structured data
recursive part
underlying regularity
folding architectures
theoretical learnability
neural networks
generalization error
input space
fat ffl
processing dynamics
combinatorial quantity
connectionistic methods
sigmoidal function
context neurons
unlimited size
recursive nature
explicit bounds
function classes
distributed representation
r m
fat shattering dimension
small empirical error
number of examples
feed forward networks
maximum input height
us to derive
information theoretical learnability
initial context y
independent uced property
empirical error and
concrete learning algorithm
valued function class
guarantee valid generalization
folding networks a
shattering dimension of
learnability of folding
recurrent neural networks
real vector space
learning algorithm is
derive explicit bounds
concrete training set
feed forward architectures
tree structured inputs
time series prediction
