pruning
rep
tdp
fossil
learning
theories
overfitting
cutoff
training
post
foil
grow
clause
conquer
learned
krk
growing
literals
noise
clauses
stnd
noisy
accuracy
quinlan
furnkranz
pruned
cn2
literal
predictive
rule
dev
bratko
stopping
pre
cohen
separateandconquer
krkpa7
pruningset
1993
lymphography
endgame
domains
1994
covered
rules
brunk
mushroom
niblett
dzeroski
cameron
correlation
pazzani
holte
secs
datasets
phase
97
criterion
chess
dolsak
splitratio
muggleton
growingset
pagallo
king
89
99
98
climbing
frnkranz
classification
1990
750
rook
negative
heuristic
concept
prolog
prune
learn
relational
decision
votes
austrian
1000
cover
separate
learns
haussler
illegal
propositional
overly
84
87
hill
79
1989
learner
euthyroid
elc
c4
jones
clark
explanations
faster
exit
cpu
95
log
prunes
1991
500
88
luaces
ranilla
overfits
bahamonde
incremental
1992
johannes
regularities
instances
avoidance
hepatitis
250
bias
76
66
sick
sparcstations
stop
93
white
asymptotic
setup
background
olshen
michie
growth
down
deliberately
b2
artificial
accuracies
top
generalize
relations
67
merits
induction
michalski
covers
42
78
widmer
breast
deleting
mesh
tries
incompatibility
commonly
75
greedy
breiman
cart
domain
significance
sizes
confirms
77
94
tree
cancer
id3
intermediate
oscar
aq
differences
inductive
uci
inefficiency
heuristics
prematurely
learnable
disjuncts
unseen
glass
costs
tested
caught
96
undone
fastest
entirely
slower
deletion
fires
greedily
wherever
mere
i rep
post pruning
pre pruning
separate and
the pruning
pruning set
pruning phase
and conquer
rule learning
than rep
reduced error
training set
error pruning
rep s
specific theory
rep grow
of rep
rep and
the krk
krk domain
final theory
cohen 1993
the cutoff
positive examples
pruning algorithm
pruning algorithms
pruning methods
and pruning
conquer rule
concept description
dev range
accuracy stnd
starting theory
stnd dev
range time
the training
predictive accuracy
and post
top down
pre and
negative examples
noisy domains
learning algorithms
cutoff parameter
initial rule
theory that
set sizes
time fossil
growing phase
and grow
decision tree
down search
pruning i
incremental reduced
in cohen
rep in
run times
accuracy on
conquer learning
pruning and
of pruning
rep is
down pruning
pruning approaches
by fossil
growing and
training examples
stopping criterion
tree learning
the learned
most specific
cpu secs
run time
empty theory
cameron jones
rule growth
overfitting theory
tdp is
fossil s
quinlan 1990
furnkranz 1994
jones 1994
algorithm foil
and bratko
that tdp
return theory
tdp s
positive 99
brunk and
and tdp
clause is
the growing
theory is
pazzani 1991
grow i
overly specific
theories that
a theory
theory figure
growing set
10 noise
background knowledge
in noisy
theory will
learning algorithm
faster than
of post
this theory
pruning tdp
bratko 1992
initial top
in cameron
foil quinlan
initial theory
fossil with
simpler theory
subsequent clauses
dzeroski and
pruning as
integrating pre
the grow
foil 6
pruning with
intermediate theory
rep 97
overfitting phase
pruning heuristics
rule growing
and pazzani
the clause
of theories
pruning is
overfitting avoidance
maximum correlation
noise handling
the learning
learning decision
asymptotic complexity
training instances
is learned
and consistent
rep has
an overfitting
holte 1993
the overfitting
the run
pruning in
positive and
of overfitting
the noise
a correlation
machine learning
data sets
clark and
noisy examples
been learned
the post
pruning the
hill climbing
learned by
clause will
pruning a
examples that
a little
conquer strategy
search heuristic
in holte
and niblett
grow fossil
67 negative
initial overfitting
grow is
pruning sets
that rep
42 correct
pagallo and
artificial noise
growth rep
pruning rep
domain initial
further deletion
quinlan 1994
rep on
fossil foil
subsequent post
haussler 1990
of tdp
krk endgame
1994 i
pruning decisions
examples growingset
pruning time
growingset pruningset
mesh design
examples splitratio
all theories
99 67
negative cover
grow algorithm
pruning criterion
separate and conquer
and post pruning
the pruning set
reduced error pruning
post pruning algorithms
post pruning phase
the final theory
on the pruning
pre and post
and conquer rule
accuracy stnd dev
conquer rule learning
stnd dev range
dev range time
training set sizes
range time fossil
the krk domain
faster than rep
in the krk
rep and grow
most specific theory
top down search
the pruning phase
and conquer learning
a theory that
theory that is
of post pruning
i rep s
the cutoff parameter
in cohen 1993
top down pruning
pruning i rep
of the cutoff
incremental reduced error
the most specific
decision tree learning
the post pruning
rule learning algorithms
in noisy domains
complete and consistent
accuracy on the
return theory figure
in the pruning
post pruning methods
and pazzani 1991
a pre pruning
post pruning algorithm
brunk and pazzani
grow i rep
initial rule growth
cameron jones 1994
i rep is
the training set
the negative examples
rule learning algorithm
the separate and
the background knowledge
pruning methods for
general to specific
in the data
foil quinlan 1990
for separate and
initial rule growing
down search for
integrating pre and
post pruning is
and i rep
domain with 10
rule growing phase
than rep s
initial top down
post pruning approaches
good starting theory
dzeroski and bratko
the intermediate theory
to specific order
down pruning tdp
in cameron jones
and bratko 1992
than rep in
the initial rule
post pruning in
concept description and
the growing set
the concept description
predictive accuracy on
growing and pruning
of finite elements
number of finite
terms of accuracy
of the negative
run time of
positive and negative
the asymptotic complexity
and conquer strategy
i rep grow
generate all theories
rep grow i
growth rep grow
post pruning with
of pre pruning
positive 99 67
that post pruning
in holte 1993
while negative cover
the pre pruning
clark and niblett
and niblett 1989
positive examples are
loop exit loop
error pruning i
literals has to
of pruning and
pre pruning algorithm
learned by fossil
pagallo and haussler
error pruning rep
a search heuristic
a simpler theory
that i rep
conquer learning strategy
overfitting the noise
krk domain with
the initial overfitting
learning and pruning
until any further
domain initial rule
of rep grow
rep grow fossil
pruning and learning
the grow algorithm
a subsequent post
an overfitting theory
pre pruning heuristics
in i rep
any further deletion
foil 6 1
a starting theory
and pruning sets
with 10 noise
examples growingset pruningset
grow fossil foil
and haussler 1990
99 67 negative
rule growth rep
integration of pruning
fossil foil 6
subsequent post pruning
search for a
examples in the
and negative examples
to the final
as a search
a good starting
the run time
of the training
combining pre and
noisy domains the
pruning set and
examples that have
overfitting avoidance as
the best theory
is learned from
with increasing training
theory will be
rule learning systems
average run time
increasing training set
will be learned
avoidance as bias
rules and conditions
accuracy of the
the costs of
a cutoff of
a tight integration
the training instances
tight integration of
cn2 induction algorithm
in this domain
the positive examples
in decision tree
for a good
p n p
asymptotic complexity of
description length principle
the learning of
inductive logic programming
needed to encode
of pre and
most commonly used
of the run
that is complete
the clause is
to the concept
is a little
seems to be
the fastest algorithm
the predictive accuracy
noise in the
is faster than
