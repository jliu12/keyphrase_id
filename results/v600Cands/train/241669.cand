cfs
cmmd
cmf
file
files
strided
jobs
ipsc
cm
workloads
workload
opened
traced
bytes
charisma
request
requests
job
accessed
860
accesses
tracing
sda
read
ncsa
scientific
trace
mode
mb
consecutive
segments
access
cdf
platforms
fraction
mimd
collector
multiprocessor
patterns
unix
sharing
sfs
uniprocessor
writes
users
thinking
disk
collective
opens
records
intel
library
modes
regularity
instrumented
sequential
node
multiprocessors
nesting
nodes
ames
concurrent
consecutively
sizes
studies
pointer
gropp
wrote
supercomputing
kotz
shared
mpi
weeks
reads
traces
production
instrumentation
scalable
machines
percent
anecdotal
tos
choudhary
1760
gennaro
lusk
pnfs
untraced
hildebrand
blocks0
cremonesi
had
nasa
platform
coordinating
supercomputers
cray
kb
synchronized
period
disks
pattern
nas
timeshared
fig
interfaces
globally
spmd
honeyman
hypercube
io
idle
sequentially
interval
thakur
gokhan
ewing
partitions
surprising
across
memik
intensive
imbalance
synchronous
server
thread
sequentiality
supercomputer
miller
tendency
transferred
segment
evenly
buffered
ellis
bucket
host
parallelized
alok
byte
blocks
ethernet
coalesce
nested
concurrently
spent
kilobytes
characteristics
classify
1000
strides
collected
william
mahmut
interleaved
fewer
programmer
claudio
katz
kandemir
open
interprocess
foster
temporary
locally
storage
hours
programmers
studying
speculate
event
focusing
processors
write
issued
trends
caching
megabytes
gb
dean
project
cp
tend
percentage
examined
kbytes
cdrom
interface
cluster
characterization
characterizing
phillimore
olaru
accesses50150250
mpii
leigh
2237
welge
nht
dachsel
i o
file system
cm 5
the ipsc
of files
the cm
only files
parallel file
file systems
files were
request sizes
strided access
in cfs
the file
compute nodes
file access
simple strided
multiprocessor file
concurrent file
in cmf
a file
write only
ipsc 860
cfs and
consecutive access
accessed with
cmmd cmf
were accessed
files that
access patterns
compute node
file pointer
and cmmd
the files
parallel i
strided pattern
in cmmd
files had
scientific applications
to files
of strided
a strided
node jobs
with cfs
bytes shared
the sda
the charisma
consecutive accesses
file sharing
o requests
read only
each file
scientific workloads
system workloads
non consecutive
fraction of
the cmmd
thinking machines
of jobs
files in
intel ipsc
strided segments
cfs in
cmf cmmd
cfs cmmd
control parallel
charisma project
a cfs
system workload
workloads on
interval sizes
the compute
parallel scientific
per file
per job
cdf of
single node
file is
of nesting
jobs were
small requests
trace records
that file
we traced
request size
instrumented library
cmmd i
cmmd figure
with cmmd
files being
by request
cmf cfs
ipsc was
workload studies
ipsc and
files fraction
independent mode
were tracing
cmmd s
mode 0
cmf jobs
data collector
that were
being accessed
there were
files on
machines cm
to open
mb file
bytes read
were read
100 bytes
file across
access pattern
there was
jobs and
different request
o request
workloads we
unix file
read and
of file
access to
single file
o node
data parallel
parallel applications
be opened
o intensive
or environment
o nodes
interval size
programming models
file and
of bytes
same file
total i
pattern if
of compute
interleaved access
how parallel
workloads files
common trends
cmmd files
cfs applications
applications traced
each compute
tracing library
were opened
global open
under cfs
block sharing
cfs were
percent shared
860 at
o modes
files percent
nested patterns
read bytes
scalable file
opened locally
cmmd applications
write bytes
at ncsa
many files
the cfs
files by
cfs provides
strided segment
local opens
cmf with
uniprocessor file
their bytes
observed workloads
the tracing
of segments
the i
had a
trace file
sharing between
o in
jobs that
the cmf
that files
write blocks
weeks in
like mode
o behavior
partition size
and wrote
temporary files
synchronized access
william gropp
of traced
mimd multiprocessors
some files
mpi io
job was
5 had
strided patterns
the cm 5
fraction of files
write only files
on the ipsc
on the cm
read only files
number of files
parallel i o
of the files
multiprocessor file systems
file access patterns
parallel file systems
the i o
files that were
access to files
cfs and cmmd
concurrent file system
i o requests
parallel file system
the ipsc 860
file system workload
were accessed with
the charisma project
file system workloads
only files had
files were accessed
a simple strided
number of jobs
files on the
of i o
and the cm
a file is
machines cm 5
strided access pattern
of multiprocessor file
single node jobs
only files were
the ipsc was
cmmd cmf cfs
we were tracing
files being accessed
a strided pattern
cmmd i o
files fraction of
of consecutive access
the ipsc and
the files being
file sharing between
by request size
of strided access
in each file
different request sizes
the compute node
of files fraction
the write only
intel ipsc 860
thinking machines cm
i o in
files in the
of the file
cm 5 was
total i o
that were accessed
number of segments
i o nodes
cm 5 at
number of compute
i o request
i o node
parallel scientific applications
of compute nodes
a single file
i o intensive
of parallel scientific
the same file
i o is
in i o
i o for
the machine was
ipsc 860 at
scalable file system
in a strided
ipsc and the
simple strided patterns
platform or environment
their i o
i o modes
files percent shared
blocks read bytes
percentage of consecutive
of files figure
being accessed with
accessed with only
of write only
consecutive access to
multiprocessor file system
files that had
concurrent file sharing
a parallel file
their bytes shared
each compute node
parallel file access
a control parallel
simple strided access
cm 5 had
cfs cmmd cmf
of the charisma
the concurrent file
of total i
their applications traced
of files percent
cm 5 were
read and written
number of bytes
thread of control
of the write
in the file
to files in
each file across
file across all
the compute nodes
of files opened
was idle for
during the tracing
had a single
i o behavior
simple strided pattern
file system design
60 of the
an intel ipsc
levels of nesting
ipsc 860 and
stored in row
try to identify
to be accessed
cdf of the
there were some
on an intel
each node that
the intel ipsc
the read only
file i o
on storage tos
sizes used in
transactions on storage
storage tos v
number of different
conference on supercomputing
the single node
single thread of
file system the
unix file system
file system for
tend to be
to the host
out of core
an application that
suggests that there
size of 32
of the time
in row major
used in each
fig 9 shows
and distributed systems
read and write
the file system
jobs number of
interprocess spatial locality
files had all
locally or globally
file systems a
the instrumented library
no bytes shared
both cfs and
of segments number
o intensive cray
spmd applications and
gives each process
cmf cmmd cmf
access in cmf
file system of
becoming a significant
small request sizes
byte and block
that distributed the
the observed workloads
regular strides in
for mimd multiprocessors
disk directed i
write fraction of
o becoming a
2002 paolo cremonesi
shared write bytes
request sizes cmf
a sequential request
the indicated percentage
o for mimd
read write files
that was run
the cmmd workload
