dgefmm
modgemm
strassen
morton
tile
padding
dgemmw
recursion
quadrants
cache
winograd
truncation
submatrices
matrices
multiplication
ultra
matrix
size0
peeling
40normalized
tiles
conversion
miata
contiguous
padded
alpha
801
layout
chatterjee
misses
siddhartha
outperforms
dec
layouts
submatrix
lebeck
001
201
16kb
rectangular
mithuna
thottethodi
implementations
lean
overlap
conquer
ultrasparc
alvin
dimensions
500
sizes
laid
interference
nw
mokbel
0mflops
size5
spacefilling
rows
1024
variability
quadtree
miss
size50
huss
patnala
gemmw
unfold
sun
recursive
070
gpu
lederman
pad
blas
aref
conventional
storage
sw
hierarchical
mellor
crummey
090
dgemm
v5
platforms
convert
divide
columns
multiplies
supercomputing
512mb
eliminating
cray
dimension
atom
walid
conflict
faster
2mb
truncate
routine
015
praveen
whalley
column
odd
additions
array
contiguously
quad
depth
multiplications
arithmetic
8kb
bailey
sandeep
compilers
division
subprograms
ordering
8b
memory
temporary
locality
major
mohamed
static
rhodes
unfolding
converting
8a
kennedy
career
513
cda
nests
5b
overhead
sen
vs
080miss
12356
crays
expec
ferizis
kreczmar
vibhor
1024x256
hossam
01979
3678f
tilesize
96kb
dumir
finesses
elgindy
nondense
32x32x8
mundhra
empirically
execution
stability
ratio
op
lay
douglas
irregular
efforts
filling
extra
associativity
curves
multiply
ratios
ken
cprof
gatlin
henson
bilmes
02547
2637
fatahalian
ldc
0400
galoppo
coauthors
sugerman
frens
loop
eleventh
reuse
sized
invocations
jacm
greece
workloads
interface
150
hierarchy
strassen s
of strassen
truncation point
matrix multiplication
recursion truncation
morton order
tile size
the recursion
matrix sizes
dynamic peeling
matrix size
column major
dgefmm for
morton ordering
static padding
the ultra
for matrix
dynamic overlap
s algorithm
matrix size0
than dgefmm
strassen winograd
size0 801
40normalized execution
201 40normalized
contiguous submatrices
001 201
801 001
non contiguous
our implementation
execution time
the alpha
padded matrix
time modgemm
implementation modgemm
vs dgefmm
morton conversion
from morton
minimize padding
the tile
generally faster
to morton
rectangular matrices
input matrices
the padded
siddhartha chatterjee
winograd algorithm
dgefmm is
highly rectangular
the strassen
that modgemm
modgemm is
outperforms dgefmm
dec miata
ultra figure
size t
of cache
the matrix
of tile
contiguous sub
multiplication algorithm
size selection
s matrix
submatrices that
leading dimension
tile sizes
conventional algorithm
uses dynamic
level 3
a dec
interface level
alpha and
sub matrices
s variant
the morton
while dgefmm
modgemm outperforms
miata b
larger while
conversion cost
overlap dgemmw
winograd s
matrices 500
convert matrices
multiplication proceedings
alpha our
b sun
tile dimensions
odd sized
modgemm matrix
fixed tile
desired ratio
sun ultra
the conventional
small matrices
500 and
data layout
cache behavior
implementations on
in morton
cray 2
conversion time
array layouts
into quadrants
150 to
30 slower
wide variability
base matrix
to 15
or column
performance of
row or
matrix elements
sizes from
into submatrices
dynamically select
stable performance
major to
quadrants are
from 150
and conquer
the matrices
faster for
divide and
large matrices
the cache
is generally
memory efficiency
slower to
of padding
quadtree representation
mithuna thottethodi
algorithms cache
arithmetic complexity
to 1024
self interference
20 faster
padding the
in memory
laid out
the performance
conflict misses
is laid
from column
four quadrants
ultrasparc ii
of recursion
and figure
r lebeck
matrix storage
alvin r
contiguous in
matrices to
original matrix
each tile
all matrix
15 of
to convert
to rows
to columns
direct mapped
on supercomputing
multiplication on
matrices and
matrices are
and larger
1024 on
winograd implementations
spacefilling curves
padding by
peeling dgefmm
dgefmm b
on padding
size5 015
as morton
odd size
speed matrix
matrix layout
that morton
matrix size5
f mokbel
columns ratio
submatrices exhibit
0 conversion
modgemm and
dgefmm figure
which strassen
0mflops contiguous
choosing tile
padding with
dgemmw 6
core routine
dgefmm we
090 0mflops
across matrix
result matrix
of winograd
of 513
dimensions independently
case padding
strassen s algorithm
of strassen s
recursion truncation point
the recursion truncation
on the ultra
001 201 40normalized
size0 801 001
40normalized execution time
201 40normalized execution
matrix size0 801
801 001 201
is generally faster
than dgefmm for
our implementation modgemm
from morton order
execution time modgemm
to morton order
faster than dgefmm
for matrix multiplication
strassen s matrix
on the alpha
of our implementation
for matrix sizes
algorithm for matrix
matrix multiplication algorithm
the padded matrix
padded matrix size
implementations of strassen
contiguous sub matrices
to minimize padding
the strassen winograd
to 15 of
strassen winograd algorithm
tile size t
of static padding
tile size selection
s matrix multiplication
5 to 15
matrix size n
the interface level
the conventional algorithm
for small matrices
of total execution
miata b sun
a dec miata
slower to 20
morton order at
and larger while
the alpha our
time to convert
to 20 faster
to from morton
30 slower to
the cray 2
large matrices 500
winograd s variant
select the recursion
generally faster for
column major to
dgefmm is generally
fixed tile size
500 and larger
matrices to from
modgemm matrix size0
highly rectangular matrices
implementation of strassen
from column major
while dgefmm is
from 30 slower
larger while dgefmm
non contiguous submatrices
multiplication proceedings of
150 to 1024
15 of total
the morton ordering
to convert matrices
convert matrices to
truncation point we
matrices 500 and
dynamic overlap dgemmw
b sun ultra
amount of padding
sun ultra figure
modgemm outperforms dgefmm
matrix multiplication proceedings
the desired ratio
time modgemm matrix
columns to rows
the non contiguous
rows to columns
dgefmm for large
dec miata b
row or column
a depth of
the performance of
from 150 to
contiguous in memory
matrix sizes from
dynamically select the
interface level and
in morton order
divide and conquer
locality of reference
two input matrices
20 faster than
faster for small
matrix sizes on
the leading dimension
the tile size
from a range
leading dimension of
quadtree representation of
is laid out
matrix multiplication on
at the interface
of tile size
of cache misses
to a depth
s algorithm for
alvin r lebeck
conference on supercomputing
up to 25
order at the
for large matrices
t theta t
the matrix size
chosen to minimize
level 3 blas
total execution time
static padding is
submatrices that are
with spacefilling curves
matrices are already
truncation point to
1024 on the
ratio exceeds the
modgemm ranges from
performance of strassen
contiguous submatrices are
overlap dgemmw 6
to columns ratio
1 cache a
dec alpha and
its cache behavior
output matrices are
single matrix elements
non standard array
morton order is
strassen winograd implementations
which strassen s
each tile dimension
dgemmw vs dgefmm
procedures for using
conversion cost age
morton conversion time
point to minimize
truncation point 13
dgefmm for nearly
truncate the recursion
alpha our implementation
to rows ratio
for matrix storage
non contiguous sub
size5 015 0
theory of cache
introduced the dynamic
matrix size50 070
setting the leading
standard array layout
best case padding
of winograd s
column major layout
and across matrix
peeling dgefmm 13
odd sized matrices
0mflops contiguous sub
cache efficient algorithms
b dgemmw vs
tile dimensions independently
level of single
size50 070 090
worse locality of
high speed matrix
theta t tile
dgefmm b dgemmw
matrices non contiguous
already in morton
that morton conversion
the alpha and
implementation modgemm ranges
platforms and across
nw and sw
exceeds the desired
sandeep sen siddhartha
dimensional space filling
of the strassen
challenging to implement
and sw quadrants
submatrix to the
sen siddhartha chatterjee
