sigma0
sigma1
m5
sigma2
c5
trees
datasets
regression
classifiers
subnode
attributes
classification
dataset
smoothed
quinlan
surrogate
sigma3
training
neural
breiman
learning
78
attribute
witten
tree
numeric
79
pruned
versicolor
decision
colic
umt
glass
smoothing
breast
leaf
76
lr
tumor
lymphography
virginica
75
instances
srt
94
cr
missing
sigma4
nominal
unsmoothed
98
seventeen
horse
iris
hypothyroid
soybean
classjx
8y0
audiology
probability
c4
leaves
probabilities
accurate
g2
oblique
eibe
inducer
waikato
torgo
opaque
prediction
74
hepatitis
ionosphere
vowel
autos
kernel
heart
sick
anneal
statlog
setosa
smooths
85
96
indians
99
88
hollow
77
84
72
sharp
il
duncan
pima
82
86
estimators
disastrous
predicted
significantly
potts
95
smyth
1993
outperforms
1984
sonar
split
labor
cedure
cancer
67
uci
625
70
outperform
83
frank
sixteen
german
waveform
1997
circles
81
fifteen
target
accuracy
92
ten
splitting
71
vote
classifications
87
wang
australian
learner
seagrass
quinlans
enko
retrofitted
karras
ressom
panayotou
virnstein
zoo
gyoerfi
accom
appice
floriana
solomatine
2877
intimates
tweedale
incomprehensibility
saso
srirangam
zorkadis
musavi
inducerm5
modated
softened
twenty
predicting
205
61
paired
density
deleted
vehicle
correcting
3772
setiono
sigma6
lagoon
rulequest
donated
siek
2865
ceci
subnodes
sammut
soklic
zwitter
1992
classifier
surprisingly
binary
validation
thirty
89
art
2310
trigg
holmes
lugosi
soften
model trees
m5 0
model tree
sigma0 1
c5 0
3 sigma0
5 sigma0
sigma0 2
linear regression
sigma0 5
7 sigma0
classifiers based
9 sigma1
sigma0 7
sigma1 0
2 sigma0
4 sigma0
6 sigma1
sigma1 3
9 sigma0
8 sigma1
sigma0 9
sigma1 5
sigma1 6
7 sigma1
for classification
of model
sigma1 2
8 sigma0
sigma0 3
0 sigma0
regression functions
attribute s
sigma1 4
decision tree
5 sigma1
3 sigma1
1 sigma1
6 sigma0
s v
that m5
smoothed model
accurate on
decision trees
trees for
and significantly
regression trees
by c5
by m5
of m5
right subnode
sigma1 1
missing values
on smoothed
sigma2 2
class probabilities
significantly more
class probability
using model
to classification
7 3
0 sigma1
training instances
significantly less
sigma0 6
binary attributes
sigma0 4
machine learning
the surrogate
0 sigma2
p il
attributes target
surrogate split
smoothed regression
sigma0 8
sigma1 8
less accurate
neural networks
more accurate
datasets and
4 sigma1
2 sigma1
8 sigma2
witten 1997
pruned decision
sigma2 8
sigma0 0
trees generated
of classifiers
5 sigma2
and witten
functions at
smoothing process
trees to
the class
the leaves
classification problems
accurate classifiers
probability functions
78 8
8 78
4 75
left subnode
trees produced
primary tumor
standard datasets
sigma3 6
oblique class
horse colic
five these
0 on
the probability
the leaf
wang and
the pruned
linear model
quinlan 1993
3 0
trees are
the model
1 98
79 8
7 82
7 sigma2
3 sigma2
art decision
the training
3 95
value v
in quinlan
predicted class
kernel density
p cr
conditional class
sigma1 7
class boundaries
5 78
dataset has
linear models
learning v
1993 the
quinlan s
class value
numeric and
datasets for
accurate than
c4 5
76 8
split s
84 5
with linear
the accuracy
and binary
assigned to
the smoothing
classifiers on
72 9
94 5
class problem
accuracy of
is significantly
each class
two attributes
79 7
probability function
trees machine
a model
model at
95 6
with missing
99 4
2 76
tree for
75 4
a smoothed
first question
on five
1 99
96 2
of training
instance is
subnode by
to m5
s pruned
outperforms m5
9 sigma3
waveform noise
nominal attribute
missing value
continuous classes
trees srt
unsmoothed model
9 sigma2
0 outperforms
heart statlog
trees umt
way classification
for m5
combined linearly
sharp step
of model trees
model trees for
classifiers based on
significantly more accurate
trees for classification
using model trees
a model tree
significantly less accurate
linear regression functions
model trees to
the model tree
3 sigma0 5
that m5 0
by m5 0
by c5 0
accuracy of classifiers
smoothed model trees
5 sigma0 1
of classifiers based
of m5 0
m5 0 with
at the leaves
based on smoothed
more accurate on
regression functions at
attribute s is
wang and witten
datasets and significantly
m5 0 is
pruned decision trees
the right subnode
model trees are
3 sigma0 1
sigma0 1 98
8 sigma0 9
c5 0 on
sigma0 3 95
and binary attributes
5 8 2
7 sigma0 7
0 4 7
0 sigma0 2
8 2 7
and significantly less
and witten 1997
assigned to the
5 7 3
with linear regression
tree for each
7 3 0
4 3 0
6 7 3
1 5 8
4 7 3
trees generated by
sigma0 1 99
and significantly more
95 6 sigma0
the surrogate split
m5 0 on
84 5 sigma0
accurate classifiers on
on smoothed regression
on model trees
generated by m5
smoothed regression trees
99 4 sigma0
94 5 sigma0
with missing values
sigma1 2 76
96 2 sigma0
4 sigma0 5
class probability functions
5 sigma1 3
m5 0 and
accurate on five
the left subnode
sigma0 7 82
conditional class probability
the smoothed model
9 sigma1 2
numeric and binary
sigma1 5 78
4 sigma0 1
model tree algorithm
trees produced by
2 6 7
compare the accuracy
for each class
3 1 5
7 3 1
3 sigma0 6
quinlan 1993 the
split s v
the art decision
on the smoothed
6 sigma0 1
less accurate on
the conditional class
more accurate classifiers
7 3 2
3 0 4
3 2 6
machine learning v
the training instances
the smoothing process
figure a 1
4 4 3
the accuracy of
based on model
used for classification
to the left
3 0 0
the linear regression
of training instances
table 3 shows
leaves of the
a decision tree
trees machine learning
an instance is
functions at the
the model at
the first question
3 shows that
at the leaf
is significantly less
the probability that
s v is
6 sigma1 5
the leaf models
multi class problem
model trees that
is that m5
smoothing process is
5 sigma1 2
on m5 0
model trees umt
whenever the model
3 sigma1 2
5 sigma0 3
of linear model
6 sigma0 0
of attribute s
8 sigma2 7
0 produces significantly
technique of model
of function approximation
a missing value
induction of model
8y0 20 61
sigma0 9 83
than c5 0
performance of model
70 0 sigma2
shows that m5
0 sigma1 5
class probability function
74 7 sigma2
model trees is
model tree inducer
45 1 sigma1
9 sigma0 8
c5 0 0
sigma0 2 99
on smoothed model
thirty three standard
0 sigma1 0
8 sigma0 2
tree with linear
the prediction passed
produced by c5
of a smoothed
s and value
the class whose
generated by c5
76 5 sigma1
training instances that
missing values are
is standard procedure
sharp step function
described by wang
model trees with
seventeen datasets and
regression process at
0 is significantly
sigma1 8 78
0 on seventeen
8 sigma1 4
67 5 sigma2
sigma0 5 92
outperforms m5 0
linear regression lr
4 sigma1 5
sigma0 4 79
the class value
1 99 4
7 82 7
98 8 sigma0
88 9 sigma1
sigma0 1 94
