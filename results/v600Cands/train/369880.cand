oc1
fat
moc1
moc2
margin
pdt
perceptron
impurity
pdts
margins
twoing
fold
classifiers
shattering
prognosis
hyperplane
cv
learning
di
trees
decision
tree
gdt
topdown
housing
generalization
separating
cancer
overfitting
erence
pruning
erent
validation
svm
split
enlarging
vc
outperforms
hyperplanes
vapnik
cross
accuracy
kasif
bupa
classification
bright
paired
capacity
instances
classifier
node
pima
iris
covering
perceptrons
4em
4192
c4
separable
subsamples
inducer
mtr
00
growth
flexibility
shattered
ect
bias
splits
category
heart
hirsch
multicategory
randomized
xy
discrimination
controlling
soft
training
salzberg
variances
risk
sample
stopping
dataset
nodes
breast
chervonenkis
leaf
erently
neural
greedy
sonar
murthy
classified
cart
dimension
probabilities
maximization
bayesian
minimizes
learnability
climbing
skeletons
erences
goodness
boosting
98
attributes
wisconsin
maximizes
classifies
outperformed
dim
ective
cdts
bottomup
bdt
combatted
2462
twoingv
bootstraping
vural
saeedi
volkan
lucene
8em
afat
linearly
reciprocal
falling
induced
tests
valued
favorable
generalize
anthony
bounding
dimensions
probability
hilbert
radius
discrepancy
lenght
analagously
burges
alue
upperbounded
subjects
lemma
cover
tune
supervised
rule
recommended
60
statistical
medical
95
subsample
cristianini
nello
covers
78
ball
variance
hill
multivariate
pseudo
delve
80s
masoud
dts
cdt
astronomical
machines
internal
leaves
splitting
baseline
criteria
adaboost
indians
overfit
73
quantity
er
incorrectly
olshen
conceive
gini
cer
chooses
80
relabeled
weight
root
editorial
diabetes
labeled
oc1 on
perceptron decision
10 fold
the margin
of oc1
large margin
and oc1
data sets
moc1 and
decision trees
fold cv
as oc1
twoing rule
impurity measure
and moc2
outperforms oc1
fold cross
separating hyperplane
the 10
fat shattering
decision nodes
cross validation
of moc1
of moc2
10 data
fat and
the generalization
generalization error
of instances
decision tree
the split
the tree
optimal separating
than oc1
support vector
the di
the impurity
w number
average accuracy
oc1 and
modified twoing
cv average
by oc1
fat moc1
capacity control
x x
of fat
category i
instances on
in category
di erence
higher means
di erent
the margins
large margins
sets studied
oc1 10
split i
algorithm fat
shattering dimension
fat has
oc1 pdt
moc2 has
oc1 fat
moc2 and
a perceptron
input space
the decision
tree size
generalization performance
enlarging the
the fat
mean than
margin at
paired t
soft margin
smaller mean
vector machines
current node
margin the
function class
margin in
performs as
not significant
tree sizes
for fat
the covering
margin is
means and
instances in
each node
pdt learning
double sample
oc1 if
versus oc1
a gdt
class right
that fat
gdt over
the twoing
maximal margin
pdts with
oc1 in
topdown growth
significant not
moc2 uses
the oc1
class left
split number
in pdts
fat pdt
oc1 as
9 out
node classifiers
leaves depth
significant x
moc1 outperforms
cv results
moc1 has
significantly higher
hyperplanes in
sets and
the perceptron
data set
00 2
learning ability
of margin
erence of
with oc1
mean on
the learning
lemma 3
controlling their
2 10
the hyperplane
multi class
learning algorithms
a paired
validation results
at current
in perceptron
margin and
t test
are significantly
c4 5
on 9
bayesian classifiers
bound the
10 3
given tree
internal node
same tree
linearly separable
left of
right of
e w
p values
erence is
2 90
line it
classifiers with
uniform convergence
validation is
p value
more likely
in 18
of 10
machine learning
oc1 is
the pdt
generalized decision
randomized hill
shattering dimensions
10 cv
margin does
impurity measures
pdt constructed
known pdt
margin hyperplanes
as impurity
fat will
and moc1
split w
fat the
pdts is
depth leaves
that moc1
log 4em
tests associated
33 96
vs oc1
fat outperforms
oc1 18
96 00
prognosis fat
moc1 moc2
oc1 to
using oc1
tree skeletons
by fat
perceptron decision trees
10 fold cross
10 fold cv
the 10 fold
fold cross validation
outperforms oc1 on
moc1 and moc2
number of instances
10 data sets
of the split
perceptron decision tree
as oc1 on
x x x
instances in category
the 10 data
oc1 on the
in category i
of instances on
instances on the
the decision nodes
data sets and
the di erence
of the 10
w number of
fat moc1 and
modified twoing rule
optimal separating hyperplane
that of oc1
of instances in
the impurity measure
the optimal separating
of the margin
the input space
the generalization error
them are significantly
split i e
and performs as
the split i
enlarging the margin
category i on
a perceptron decision
e w number
well as oc1
the fat shattering
data sets studied
fat and oc1
sets studied in
fat shattering dimension
lemma 3 7
support vector machines
on the generalization
generalization performance of
studied in 18
the tree size
paired t test
are significantly higher
is a gdt
on 9 out
fold cv average
than oc1 on
a gdt over
the node classifiers
line it indicates
large margin is
the modified twoing
slightly smaller mean
of 10 data
moc1 outperforms oc1
oc1 on it
the large margin
moc2 and oc1
the split number
oc1 fat moc1
t test is
cv average accuracy
moc2 uses a
of oc1 fat
with large margin
means and p
mean than oc1
sets and performs
versa the figure
measure the learning
point is above
oc1 if the
x p value
versus oc1 if
significant x y
results of moc1
the twoing rule
cv results of
the learning ability
of fat and
indicates the 10
9 out of
test the di
fold cv results
the perceptron decision
split number of
significant not significant
the line it
a double sample
not significant x
moc1 and oc1
at each node
of the tree
learning ability of
a paired t
di erence of
in perceptron decision
a large margin
cross validation results
at current node
2 10 3
improve the generalization
out of 10
left of the
of 10 fold
the margin the
the margin and
right of the
i e w
the point is
the support vector
and p values
for multi class
class of functions
the right of
more likely to
the left of
of the decision
above the line
cross validation is
decision trees in
if the point
it indicates the
internal node is
is above the
the same tree
i on the
di erence is
at node i
performs as well
each data set
real valued functions
support vector machine
and p a
x y figure
neural tree networks
decision nodes with
the the right
compared with oc1
current node number
within the margin
margin at each
higher means and
known pdt learning
twoing rule as
randomized hill climbing
generalized decision trees
where total number
risk of overfitting
control in pdts
smaller mean than
value x p
the tree sizes
good as oc1
multi class classification
analysis of generalization
wisconsin breast cancer
separating hyperplane with
pdt learning systems
of murthy kasif
the six data
the margin does
class problems number
results of moc2
bound on generalization
and oc1 as
fat and the
oc1 10 cv
corresponds to class
points at this
95 33 96
classes for two
the tests associated
five smaller trees
k decision nodes
decision tree skeletons
the standard topdown
other pattern classifiers
as impurity measure
large margin trees
best known pdt
results of fat
of oc1 and
an cover for
fat pdt has
20 2 10
oc1 on 6
