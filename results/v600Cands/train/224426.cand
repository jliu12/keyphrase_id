cfd
alliant
sgi
parafrase
compilers
polaris
serial
bottlenecks
loops
parallelization
speedup
parallelizing
solvep
alliants
coefv
optimizations
bus
challenge
ppolaris
parallelism
parallelized
loop
compiler
iadd
pfa
sharing
cache
fx
2800
contention
amdahl
architectural
solvev
hyperplane
false
synergetic
coefp
corect
2000000
parallelize
multiprocessors
restructuring
algorithmic
threads
processors
manual
nmax
bars
doacross
ns
alg
code
parallelizer
shades
eq
native
thread
4mb
misses
modules
exploitable
bar
execution
dismal
localizable
locality
resulted
profound
16kb
barrier
vectorization
compiled
caches
secs
parallelizers
law
benchmarking
manually
kernels
normalized
com
measurements
automatic
fluid
idiosyncrasies
sec
overhead
array
legality
delivered
improvement
commercial
undetermined
underscore
processor
multiprocessor
improvements
speedups
manufacturer
limitations
distributing
tuning
grand
findings
comprehensive
profiling
timers
dependences
msec
serially
velocity
branches
coherence
attributed
saturation
bandwidth
attainable
projection
machines
optimized
iteration
coefficient
measured
emulate
statements
accesses
solvers
fractions
interference
mimd
symbolic
001kg
payback
90serial
2800161616161
privatarizing
8016161616
446741
miccg
969841serial
topped
280016161616135789
iteration24solve
challenge16040471611
ilucr
80161616161
5sec
byte
scalability
calculations
navier
powerful
body
bottleneck
transformations
stokes
supplied
consuming
shared
80
codes
synchronization
40x40x40
prowess
excellently
indisputable
allt
experimental
hardly
sweep
incomplete
dynamics
cycled
1048576
attests
exposed
effectiveness
eliminate
bodies
planes
automatable
limi
neutralize
proc
noticeable
resisted
subcomputations
expert
corporation
targeted
boundary
unbalance
amortization
inflow
incorporated
converge
distribute
dimensional
exploited
yes
cfd code
our cfd
the sgi
sgi challenge
the alliant
parafrase 2
the cfd
false sharing
architectural bottlenecks
alliant fx
m com
bus contention
execution time
the serial
dimensional hyperplane
parallelizing compilers
local variables
parallel loop
and polaris
parallel ppolaris
ns eq
solve ns
the alliants
the loop
maximum speedup
serial execution
four compilers
fx 2800
manual optimizations
serial parallel
in do
algorithmic changes
optimizations and
amdahl s
speedup of
parallel execution
fx 80
serial and
each machine
code on
the code
hyperplane with
parallelized by
do loops
set coefficient
com alg
experimental compilers
8 do
parallel loops
automatic parallelization
with projection
loops with
system set
avoid false
8 processors
3 dimensional
m all
was compiled
the parallel
the performance
of figure
solve system
on performance
and manual
component solve
the synergetic
experimental parallelizing
1 2000000
alg m
ppolaris m
com m
doacross local
c doacross
bus bandwidth
of parallelism
local variable
execution times
10 b
compiler optimizations
code tuning
bars labeled
hyperplane method
system solve
condition boundary
cfd application
s law
code was
parallelizing compiler
and parallel
local cache
and algorithmic
compilers and
the pfa
two experimental
challenge was
parallelization on
in significant
time of
do do
do j
resulted in
the speedup
of compiler
the parallelization
2 dimensional
the delivered
non loop
loop parallelism
shared array
cache misses
the execution
compilers on
three machines
of false
of compilers
parallelism but
8 processor
the compilers
if statements
data locality
boundary condition
normalized execution
was measured
sharing and
bottlenecks in
the bars
of processors
cache line
the parallelized
do loop
coefficient of
performance of
cfd on
distributing strongly
caches 1
solvev the
delivered speedup
compilers performed
scalar expansion
eliminate by
if iadd
synergetic effect
of exploitable
performance profiling
architectural idiosyncrasies
local j
consuming modules
expansion pass
iadd 0
bottlenecks was
automatic parallelizing
execution serial
eq z
selected kernels
coefv coefp
all alliant
2 polaris
for solvep
a cfd
optimized cfd
corect normalized
entire cfd
exposed parallelism
2800 the
cannot parallelize
output result
manufacturer supplied
iadd 1
solvep and
coefv coefv
compilers tested
cfd benchmark
existing threads
in solvep
benchmarking studies
eq y
converge time
labeled parallel
localizable variables
ppolaris alliant
2000000 a
barrier entry
alliant compilers
three multiprocessors
coefp corect
be parallelized
modules of
parallelization of
our experiments
the 3
automatic and
the native
of approximately
our findings
the hand
figure 12
a loop
computational fluid
projection method
specific loops
our cfd code
the sgi challenge
on the sgi
of the cfd
of our cfd
the alliant fx
the cfd code
serial execution time
2 and polaris
solve ns eq
execution time of
parafrase 2 and
dimensional hyperplane with
2 dimensional hyperplane
hyperplane with projection
of the code
the serial and
the 3 dimensional
serial and parallel
avoid false sharing
set coefficient of
alliant fx 2800
and the alliant
cfd code on
system set coefficient
serial parallel ppolaris
coefficient of system
m com alg
on each machine
amdahl s law
alliant fx 80
to the serial
time of the
false sharing and
component solve ns
sgi challenge was
com alg m
alg m all
dimensional hyperplane method
com m com
cfd code we
cfd code and
and code tuning
do j 1
3 dimensional hyperplane
cfd code as
system solve system
j 1 2000000
ppolaris m com
manual optimizations and
the architectural bottlenecks
experimental parallelizing compilers
code on each
1 8 do
m com m
effect of compiler
in do loops
parallel ppolaris m
of system set
c doacross local
and manual optimizations
result in significant
of false sharing
of local variables
be parallelized by
the shared array
boundary condition boundary
to avoid false
condition boundary condition
and the sgi
with projection method
amount of parallelism
the 2 dimensional
of the loop
the parallel execution
the parallelization of
to cache misses
parallelized by the
of the compilers
loop of figure
the speedup of
the performance of
compiler optimizations and
an 8 processor
the serial execution
automatic and manual
time in sec
code was compiled
the execution time
to false sharing
a parallel loop
on all three
local variables and
the loop of
the code was
shown in figure
of figure 6
execution time was
parallel execution time
of figure 12
run time overhead
code on a
number of processors
alliant computer systems
alternative order of
the four compilers
time consuming modules
was more profound
i 1 8
80 and the
the compilers tested
computer systems corporation
for our cfd
eq y component
and algorithmic changes
time input data
on the alliants
in our cfd
from local cache
running the parallel
a total speedup
of approximately 7
b if statements
of bus contention
the sgi and
8 processors parallel
ns eq z
coefv coefp corect
continuous equation solve
entire cfd code
alter the order
sgi and the
1 2000000 a
the scalar expansion
challenge the alliant
and architectural idiosyncrasies
of a cfd
in do loop
the synergetic effect
synergetic effect of
solve system solve
the entire cfd
equation solve ns
bars labeled m
8 do do
an alternative order
local cache read
execution serial parallel
parallel loop iteration
cache read also
parallel ppolaris alliant
of architectural bottlenecks
read also from
coefp corect normalized
ns eq y
comprehensive performance profiling
data output result
eq z component
total speedup of
all alliant fx
the manufacturer supplied
distributing strongly connected
fx 80 and
8 do j
two experimental parallelizing
the delivered speedup
number time in
scalar expansion pass
loops whose body
local variable recognition
compiled by each
speedup of approximately
fx 2800 the
of the alliants
caches 1 of
parallel execution times
parafrase 2 is
z component solve
of proc time
corect normalized execution
the three multiprocessors
other caches 1
proc time msec
a local variables
cfd benchmark on
8 processor sgi
sgi challenge the
program to emulate
compiler architecture and
fluid dynamics code
parafrase 2 polaris
original code was
in parallel loops
doacross local j
the experimental compilers
normalized execution serial
the alliant compilers
1 of proc
ppolaris alliant fx
all four compilers
