dem
cwa
mwa
tasks
quota
scheduling
load
twa
balancing
subcube
walking
node
balanced
hypercube
avg
hops
subcubes
overloaded
prescheduling
runtime
processors
locality
normalized
cube
balance
subtree
mesh
copt
sends
communication
underloaded
exchange
migrated
calculates
calculated
nodes
row
child
jj
weight
dimension
quotas
receive
bitwise
sent
ffi
f4
ching
topologies
multiprocessors
calculation
topology
willebeek
lemair
task
d0
grain
workload
balances
spends
evenly
parent
scheduled
schedule
initiated
cooperate
fully
costs
3g
static
tree
maximize
records
jobs
256
7g
submesh
reeves
parallelized
send
adaptive
incremental
schedul
5g
transmitted
f0
preorder
vectors
utilizes
yeh
mod
liao
gammaj
ary
exchanged
overhead
jung
capacity
processor
minimized
imbalance
yeon
cdem
sungchun
hlne
hwakyung
nrics
rips
vivien
cmwa
kook
mourtos
xianliang
ximing
ojsterek
acwn
tdem
ccwa
renard
brest
janez
schedules
scan
multicomputers
heuristic
unbalanced
iteration
receiver
loads
randomized
kim
cubes
ready
d1
hypertool
topt
quickness
umer
legrand
viljem
gamman
compile
migration
massively
collect
74
exchanges
update
spirakis
pyrros
antonis
garofalakis
sunggu
milan
communications
collects
phi
minimize
exchanging
activity
reserved
automates
abstractparallel
wook
chung
corrected
quality
spend
2m
calculate
arnaud
matroids
f6
mcp
558
byung
64
eight
sender
minimizes
multipro
jang
rescheduled
listed
sacrifices
nv
spread
sharing
rim
hee
maximizes
traversal
75
scalability
keller
nicol
stealing
accurately
ahmad
edge
of tasks
parallel scheduling
walking algorithm
load balancing
dem algorithm
w avg
each node
task hops
the dem
node i
its quota
tasks to
tasks in
the load
load information
fully balanced
local tasks
communication steps
cwa and
normalized communication
to node
balance the
of w
cost weight
balanced cases
fully balance
of cwa
tasks from
avg and
scheduling algorithms
normalized cost
or mwa
cube walking
of mwa
cwa or
receive jj
of task
global information
global load
communication cost
tree walking
tasks are
scheduling algorithm
scheduling is
maximize locality
node j
static scheduling
dem is
task exchange
of dem
tree hypercube
and mwa
is child
twa cwa
4 task
mesh walking
communication costs
tasks as
in step
balanced load
non local
after execution
dynamic problems
j tasks
system phase
in node
hypercube and
iteration 0
from node
quota q
of twa
node calculates
algorithm mwa
3 quota
quota calculation
own w
w tasks
sends two
w vector
weight processors
load calculation
normalized locality
algorithm cwa
runtime parallel
sum reduction
the twa
processors 0
for load
and mesh
processors processors
total number
dynamic scheduling
optimal scheduling
w i
node has
scheduling can
collection perform
receive tasks
sends three
the cwa
node is
i 0
information collection
i tasks
w k
for node
of scheduling
step 4
dynamic load
many tasks
and node
tasks per
the overloaded
after iteration
values of
the values
load distribution
for tree
evenly divided
hops of
node pairs
the normalized
be sent
jj i
node 7
at runtime
child of
q i
subcube has
twa the
and copt
mwa is
twa is
mwa the
node d0
weight dem
ffi vectors
l tasks
mwa algorithm
quality load
overloaded subcube
algorithm twa
cwa on
subcubes f0
the quotas
quota proof
collect load
mwa algorithms
of nodes
calculates the
tasks is
the communication
four nodes
the cube
the scheduling
256 processors
is equal
minimum number
its parent
three tasks
1 global
node computes
the bitwise
processors figure
j l
the quota
runtime scheduling
negative cycle
processors cooperate
balancing decision
dimensional subcube
algorithms minimize
static problems
20 30
node 5
15 20
average load
j i
k dimensional
a balanced
the subcube
000 test
r tasks
a parallel
average number
w 0
0 r
update its
these algorithms
the average
2 average
running example
10 15
are calculated
test cases
distributed memory
can fully
vectors q
are overloaded
average weight
number of tasks
tasks to node
values of w
the dem algorithm
of tasks in
tasks in each
balance the load
of task hops
non local tasks
number of task
in each node
w avg and
fully balanced cases
fully balance the
normalized cost weight
tasks from node
of non local
cube walking algorithm
cwa or mwa
of the dem
of w avg
to its quota
node is equal
the number of
tree walking algorithm
hypercube and mesh
j i 0
parallel scheduling algorithm
parallel scheduling is
the values of
global load information
minimize the communication
cwa and mwa
the normalized communication
avg and r
mesh walking algorithm
tree hypercube and
normalized communication costs
is child of
communication steps of
child of i
j is child
each node is
the total number
of communication steps
of w k
total number of
after execution of
for node i
parallel scheduling algorithms
communication costs of
how many tasks
for load balancing
the communication cost
number of non
i 0 l
15 20 30
i 0 r
number of communication
0 5 10
in node i
each node calculates
4 task exchange
dem algorithm is
w and ffi
task hops of
evenly divided by
walking algorithm cwa
1 global information
j tasks from
execution of twa
twa cwa or
global information collection
sends three tasks
to fully balance
its quota q
sends two tasks
its own w
information collection perform
3 quota calculation
two tasks to
the tree walking
processors 0 5
runtime parallel scheduling
average load calculation
for tree hypercube
2 average load
receive jj i
three tasks to
node has the
equal to its
of tasks to
each node has
tasks to be
5 10 15
dynamic load balancing
10 15 20
to be sent
jj i 0
w k are
tasks as well
update its own
tasks per node
running example of
algorithms for tree
a balanced load
of tasks per
node i with
shown in figure
steps of this
each node computes
number of nodes
the minimum number
is equal to
scheduling algorithms for
minimum number of
a running example
the twa cwa
k are as
in parallel scheduling
own w and
1 000 test
quality load balancing
and mwa algorithms
and maximize locality
collect load information
000 test cases
algorithms minimize the
as its quota
tasks in node
can fully balance
optimal scheduling problem
load calculation 3
walking algorithm twa
parallel scheduling can
calculation 3 quota
cost weight dem
ffi vectors for
and ffi vectors
task hops is
normalized communication cost
to collect load
tasks as its
its quota proof
k dimensional subcube
node calculates the
the overloaded subcube
walking algorithm mwa
local tasks in
subtree and its
and mesh networks
high quality load
all processors cooperate
30 40 45
therefore the total
in step 4
each node i
from node i
average number of
the parallel scheduling
to node 7
node computes its
tasks are sent
able to fully
jobs or tasks
a parallel scheduling
load balancing decision
processors cooperate to
each result is
scheduling can be
20 30 40
on distributed memory
is the minimum
divided by n
w 0 i
load information and
large memory space
node p i
listed as follows
then each node
load balancing in
communication steps for
memory space to
of tasks as
of w i
the average number
number of communications
are listed as
has the same
shows the normalized
the optimal scheduling
tasks in the
is shown in
to be scheduled
node j is
and its parent
of tasks is
be sent out
load balancing for
the task graph
