learning
training
sampling
selective
neural
trained
ive
querying
uncertainty
active
concept
backpropagation
network
na
sg
queries
concepts
neuron
deltaw
bias
learner
region
engelbrecht
generalization
networks
draw
sampled
feedforward
cohn
classifiers
hwa
deltag
background
disagree
drawing
selectively
train
uncertain
valiant
haussler
classification
rebecca
inductive
layer
linguistics
configurations
oversampling
speech
membership
hidden
learnable
baum
configuration
mining
huan
learned
drawn
rectangles
batch
maytal
nicks
motherese
backpropagate
tsechansky
edmonton
sample
1990
exhibited
sigkdd
mooney
ji
consistent
alberta
utility
infinitesimally
pabitra
6ae
saar
weights
raymond
randomly
unlabeled
converge
superset
1984
prem
miles
melville
blumer
motoda
infant
1989
trainable
osborne
pathological
mackay
unclassified
classifying
classify
classifies
mitchell
angluin
1986
1982
intelligent
150
steeper
rumelhart
pass
dmitry
chervonenkis
outweigh
canada
provost
squashing
target
domingos
supervised
vapnik
harrold
domain
fit
limitations
oracle
implements
iterations
acquisition
mitra
random
santa
preference
failure
passive
discovery
recognition
2500
pac
unknown
bayesian
learn
thresholded
secure
baseline
corpora
fundamenta
informaticae
sydney
dimensions
hwang
learners
threshold
foster
distribution
baker
regions
labeling
domains
hiroshi
banff
pedro
1988
error
7b
committee
modes
iteration
corpus
letters
7a
06
kiyonori
barbanon
soderland
vorobeychik
00116
backpropa
adomavicius
tieu
baldridge
emre
fernald
anuradha
velipasaoglu
ruhlen
hockenmaier
balcan
gediminas
madani
baah
ratsaby
brits
deduplication
smartedit
aggoune
bowring
1686
steedman
00265
kocz
predisposition
selective sampling
s m
active learning
version space
neural network
r s
concept c
of uncertainty
na ive
training set
of selective
the network
training examples
most specific
region of
concept class
random sampling
neural networks
ive querying
sg network
a concept
the training
a most
a neural
inductive bias
trained on
training example
distribution p
the region
the domain
the selectively
selectively sampled
deltaw ji
pass sampling
power system
querying algorithm
the sg
background examples
s deltag
target concept
concept learning
for selective
machine learning
example x
set size
networks trained
from examples
concepts in
actual training
p engelbrecht
valiant 1984
sampled networks
t x
randomly sampled
learning from
sampling approach
generalization error
sampled data
hidden layer
feedforward neural
an sg
network implementation
of training
classification of
converge on
of examples
error of
consistent with
the na
disagree on
rebecca hwa
all concepts
space search
the concept
unknown target
of active
a point
examples s
al 1990
network s
sg net
haussler 1989
valued threshold
g networks
oversampling of
engelbrecht sensitivity
neuron j
mitchell 1982
classifies as
sampling pass
network generalization
background example
background learning
sampling the
computational linguistics
membership queries
initial random
axis parallel
train on
x t
most general
input distribution
class c
learning for
learning rate
and selective
parallel rectangles
10 examples
sample selection
sampling may
failure modes
consistent concepts
baum and
the learning
the selective
sampling in
trained with
the error
examples in
the classification
we draw
proceedings of
on knowledge
specific configuration
in generalization
in selective
and haussler
exhibited a
network c
generalization when
for neural
configurations that
of r
learning algorithm
a training
conference on
drawn from
vs training
a na
connection weights
the backpropagation
is uncertain
training data
point x
international conference
two dimensions
network configurations
of querying
backpropagation algorithm
network to
the version
distribution information
two networks
error vs
huan liu
inside r
learning p
data mining
training algorithm
concept a
reduce our
intelligent user
network is
output node
general concept
configuration c
and g
of drawing
will reduce
within r
during training
network configuration
querying and
as negative
a region
from p
acm sigkdd
examples we
learning a
for learning
concept is
as positive
learning proceedings
rectangles in
c that
learning v
to draw
learning algorithms
alberta canada
first hidden
provably more
by 150
trainable classifiers
specific general
network refers
examples alone
background patterns
new background
ive algorithm
our uncertainty
r s m
region of uncertainty
of selective sampling
of r s
a most specific
a neural network
a concept c
training set size
s and g
na ive querying
of the domain
x t x
of active learning
version space search
the selectively sampled
the version space
the region of
selective sampling approach
learning from examples
concept class c
s m the
training example x
for selective sampling
ive querying algorithm
examples s m
networks trained on
and selective sampling
active learning for
example x t
neural network implementation
selective sampling the
the selective sampling
proceedings of the
the na ive
concepts in the
converge on the
a concept class
the classification of
the training set
et al 1990
of training examples
trained on the
of 10 examples
within r s
in s deltag
an inductive bias
unknown target concept
selective sampling in
the sg network
engelbrecht sensitivity analysis
an sg network
in selective sampling
real valued threshold
rectangles in two
sampling pass sampling
neural network generalization
p engelbrecht sensitivity
randomly sampled data
network implementation of
concept c 2
selective sampling may
active learning we
a p engelbrecht
error vs training
most specific configuration
will reduce our
a na ive
selective sampling is
background learning rate
all concepts in
and haussler 1989
consistent with all
international conference on
in two dimensions
the error of
active learning proceedings
vs training set
s m will
the randomly sampled
classification of that
of that point
the unknown target
most general concept
axis parallel rectangles
the concept class
active learning with
set s m
on the training
the backpropagation algorithm
the domain that
distribution p is
in the domain
feedforward neural networks
training examples in
a region of
on intelligent user
the s and
the initial random
that the network
set of all
set size for
in r s
intelligent user interfaces
c 2 if
sensitivity analysis for
for neural networks
a point x
according to p
conference on intelligent
the target concept
machine learning v
learning proceedings of
the network s
perform selective sampling
describe the sg
the true boundary
from examples alone
t x if
neural networks fundamenta
the network adjusting
the triangle learner
we draw more
simple concept classes
valiant 1984 and
arbitrary distribution p
b on right
and g sets
do active learning
the actual training
configurations that disagree
done while david
power system static
by feedforward neural
weight update on
learning is provably
increased concept complexity
with all s
selective learning by
concept c 1
selection for statistical
iteration of selective
of uncertainty an
pass sampling pass
valued threshold function
selectively sampled data
inside r s
set size 50
need to draw
our current hypothesis
the background examples
networks exhibited a
networks fundamenta informaticae
and g networks
generalization when learning
true region of
calculate the output
first hidden layer
a pathological example
iterations of 10
maytal saar tsechansky
of the na
background examples we
s m then
selectively sampled networks
on the selectively
while david cohn
querying various parts
draw more and
denotes the true
during training in
not converge on
the weight update
adjusting weights according
active learning is
most specific general
case of maintaining
in b on
david cohn was
network adjusting weights
actual training examples
all axis parallel
and determining a
network refers to
analysis for selective
initial random sampling
provably more powerful
na ive algorithm
how one may
pathological example of
batch of training
of all axis
the sg net
hwa sample selection
size for random
by david mackay
each iteration small
true boundary of
learned by 150
is provably more
system static security
