elevator
car
cars
rl
passenger
agents
reinforcement
passengers
floor
agent
learning
lobby
floors
traffic
hall
reinforcements
decentralized
omniscient
waiting
peak
arrival
training
buttons
annealing
rld
button
policies
avgwait
squaredwait
systemtime
esa
actions
team
online
barto
trained
secs
wait
controller
arrivals
hours
event
decisions
units
queue
games
dlb
decelerate
zoning
fim
learned
doors
simulated
collective
squared
neural
controllers
stop
supervisory
receding
crites
learn
dp
huff
down
action
calls
profile
hour
fuzzy
strakosch
cassandras
siikonen
policy
greedy
exploration
1993
upward
reward
mahadevan
bao
percent
elapsed
sridhar
prisoner
bradtke
weights
temperature
1994
footprints
rewards
travel
gammon
hitachi
simulator
heavy
registered
face
td
annealed
elevators
tesauro
moving
stopped
discount
stochasticity
activation
incomplete
networks
pushed
testbed
decision
instability
unsophisticated
events
pure
schedule
horizon
train
trip
group
capacity
gammafi
payoffs
network
ascending
stationarity
feedforward
thathachar
whiteson
fstop
feudal
hajime
tobita
inept
pepyne
contg
ujihara
interfloor
intervisit
ghavamzadeh
play
opponent
activations
sutton
voting
objectives
situations
minute
longest
controlling
tendency
1991
rlr
pct
lqf
experiences
dilemma
discrete
supervised
stopping
averaged
evenly
probabilities
contingencies
wanting
shimon
unanimous
equilibrium
highest
beam
someone
1996
gradual
ai
centralized
observable
shaft
vantage
rls
markey
fujita
emerges
multi
sequential
service
unstable
60
waited
rush
ishii
critic
conveying
pas
poisson
rlc
round
lewis
1992
serviced
reinforcement learning
multi agent
agent rl
down peak
the lobby
the cars
each car
the agents
a car
hall calls
elevator group
group control
elevator system
hall call
peak traffic
up traffic
the elevator
down traffic
online reinforcements
other cars
elevator control
up peak
elevator time
rl agents
simulated elevator
next floor
q learning
waiting times
the hall
learning rate
elevator systems
value estimates
cars are
a floor
car i
squared wait
incomplete state
omniscient reinforcements
avgwait squaredwait
squaredwait systemtime
the car
wait times
each agent
hours of
input units
60 secs
moving cars
wait time
discrete event
q values
of simulated
and down
average squared
rl algorithms
passenger arrival
passenger arrivals
real elevator
elevator car
of elevator
other agents
state information
a team
of training
the learning
car and
q value
the q
a passenger
zero sum
passengers per
algorithm avgwait
car arrival
systemtime percent
car that
reinforcement signal
percent 60
car has
one elevator
the decentralized
in heavy
control policies
an elevator
stop at
over hours
average wait
button has
traffic profile
state space
profile with
41 7
of passengers
arrival event
floor with
for down
event when
cars and
the annealing
the rl
rl algorithm
of rl
agent reinforcement
the average
the networks
immediate call
causes activation
upward moving
making car
up passengers
between floors
hall button
bao et
to elevator
car causes
pure up
cars to
controlling one
peak profile
call buttons
decentralized rl
reinforcements are
global reinforcement
hall buttons
waiting passengers
receding horizon
down hall
group supervisory
sum games
of hall
the team
team of
heavy traffic
the state
real system
been pushed
floor if
a hall
sridhar mahadevan
the doors
optimal policy
a real
amount of
of incomplete
decision is
of exploration
decision point
car the
existing controller
annealing schedule
call information
arrival rates
the button
the amount
and learning
the down
off there
cars in
learning agents
non greedy
decentralized control
the simulator
agent systems
waiting time
of multi
average waiting
agents each
between events
to play
the elapsed
travel time
supervisory control
the network
learning algorithm
of time
up and
event dynamic
arrival patterns
discount factor
at time
floors and
for car
elevator cars
passengers at
highest floor
gradual annealing
floor traffic
original decentralized
building with
pure down
each floor
7 313
floors where
floors it
unsophisticated agents
rld 14
sequential multi
rl controllers
similar policies
hierarchical reinforcement
car whose
another car
multi agent rl
elevator group control
down peak traffic
at the lobby
the other cars
the next floor
simulated elevator time
of simulated elevator
q value estimates
incomplete state information
avgwait squaredwait systemtime
hours of simulated
the other agents
a real elevator
where the agents
41 7 0
real elevator system
of incomplete state
at a floor
of multi agent
of the state
the hall call
average squared wait
one elevator car
levels of incomplete
systemtime percent 60
squaredwait systemtime percent
algorithm avgwait squaredwait
for down peak
arrival event when
the down peak
percent 60 secs
the learning rate
over hours of
in a real
the average squared
a team of
agent reinforcement learning
multi agent reinforcement
the amount of
up and down
zero sum games
squared wait times
decision making car
group control system
down peak profile
for controlling one
event when a
car has a
upward moving cars
about the hall
decision is required
a car that
controlling one elevator
group supervisory control
car causes activation
hall call information
car arrival event
of the cars
each car has
bao et al
global reinforcement signal
to the lobby
pure up traffic
is on if
for each car
if the button
amount of time
and t 1
in heavy traffic
the multi agent
responsible for controlling
at a queue
unit is on
average wait time
the average wait
of the elevator
the button has
a real system
in multi agent
wants to get
the q values
state transition probabilities
of the other
multi agent systems
an optimal policy
the average waiting
very large problems
event dynamic systems
on if the
stop at the
discrete event dynamic
the agent for
floor if a
reinforcement signal which
and non stationarity
1992 1994 1995
at each floor
the decentralized rl
actually visited during
of relatively unsophisticated
peak traffic profile
a stopped car
omniscient and online
up traffic is
elevator control strategies
time t x
in two way
and learning rate
relatively unsophisticated agents
the highest floor
313 41 7
of elevator group
original decentralized algorithm
possible objective is
profile with up
7 0 07
temperature and learning
inter floor traffic
downward moving cars
has been waiting
all the cars
floors where the
passengers at a
after every event
car that is
store the q
traffic profile with
rl agents each
areas of state
of hall calls
motion between floors
peak profile with
to get off
a decentralized way
visited during control
sequential multi agent
with up and
each car i
of exploration performed
as discrete event
the car whose
exploration performed by
highest floor with
heuristic elevator control
important in multi
elevator control algorithms
whose decision is
pure down traffic
up peak traffic
next floor or
nine down hall
get off there
7 313 41
wait times of
continue past the
rld 14 7
time t y
the nine down
of rl agents
and down peak
added stochasticity and
the original decentralized
hierarchical reinforcement learning
immediate call allocation
14 7 313
a hall call
in motion between
or continue past
floor or continue
is in motion
a decision point
results for down
moving cars to
of an elevator
car whose decision
a passenger wants
passenger wants to
and down traffic
hall call buttons
team of rl
averaged over hours
problem of elevator
amount of exploration
the heuristic elevator
actions of the
the state space
of the agents
of time between
time the time
non zero sum
to play strong
agents each of
andrew g barto
stochasticity and non
past the next
button has been
prisoner s dilemma
open and close
if a passenger
