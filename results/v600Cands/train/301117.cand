loggp
sweep3d
mpi
mk
sweeps
mmi
octants
processor
octant
grid
synchronization
sp
processors
message
receive
4kb
logp
smp
cluster
wavefront
send
roundtrip
1kb
posted
communication
intra
measured
sweep
totalcomm
startp
messages
west
primitives
poems
jt
handshake
asci
micro
projections
deelman
ewa
projected
fortran
anomaly
blocking
vernon
pipelined
45x45x1000
total_comm
150
latency
calculations
vikram
adve
costs
modeled
sizes
north
benchmarks
1000
rizos
sakellariou
scalability
billion
128
usec
ruoming
gagan
developers
configurations
rajive
25000
twelve
delay
accurately
15000
bagrodia
principal
execution
overhead
10000
analytic
contention
analyzing
thousand
scaled
trip
measurements
mary
neighbor
2500
8kb
20000
angles
sending
occupancy
diminished
ibm
million
modifications
finished
000
southeast
sigmetrics
multiprocessors
sorting
greatly
agrawal
mo
hagihara
noriyuki
laity
logpc
fixups
150150250
40012000
kesselman
ino
vahi
horizonal
anastasia
buaklee
lopc
orginates
fumihiko
synch10003000500070000
berriman
wavefronts
loggps
brevik
105001500250035000
501500
gaurang
delays
quantitatively
mapped
corner
280
5000
dimensions
sent
predict
east
destination
overlapped
agreement
8533
1392
nurmi
ppopp
blythe
tampa
elucidation
1003005000
marin
prohibitive
bandwidth
supercomputing
scientific
validated
255
validation
inter
breakdown
remote
gurmeet
stukel
intercluster
koelbel
f2c
lowenthal
anirban
dube
yolanda
chuck
unexplained
jin
derive
slope
header
symmetry
kenichi
multiparadigm
intracluster
pegasus
elucidates
houstis
wolski
3d
measuring
thousands
subscripts
odd
1024
block
browne
meiko
workflows
sundaram
crummey
the loggp
loggp model
of sweep3d
the sweeps
the sweep3d
mpi send
processor grid
sp 2
the sp
the mpi
execution time
message size
mk 10
mpi communication
problem size
mpi receive
mk 1
of processors
per processor
problem sizes
loggp mmi
than 4kb
for octants
total problem
intra cluster
processor p
sweeps for
synchronization structure
send and
processors time
the roundtrip
size per
and mpi
the model
the measured
the processor
complex synchronization
sweeps from
communication parameters
fixed total
the message
j dimensions
octant pair
it jt
been posted
loggp mk
synchronization structures
measured mk
fixed problem
grid points
the west
cluster communication
150 150
sweep3d application
micro benchmarks
mmi 3
mpi software
sweep3d is
measured execution
synchronization costs
synchronization delays
receive has
at message
two neighbor
receive primitives
corresponding receive
in equation
of mpi
6 1000
for messages
message sizes
smp node
l and
t 7
four processor
processor configurations
communication times
for intra
each processor
loggp equations
billion grid
1 loggp
3 mk
4kb the
octants 7
communication micro
measured application
projected sweep3d
projected execution
roundtrip communication
for octant
twelve iterations
performance projections
sweep3d execution
measured communication
to 128
processor in
128 processors
the fortran
application developers
inter cluster
at processor
time step
t 5
l o
50 50
communication cost
each octant
neighbor processors
wavefront application
messages larger
than 1kb
processors in
task graph
processing overhead
beyond one
l local
two thousand
000 time
the sweep
this delay
ibm sp
time for
w g
primitives the
k vernon
3d grid
model estimates
mary k
sweep for
model of
models of
processor is
equations for
for analyzing
parameters l
the logp
ewa deelman
is modeled
of table
smp nodes
a message
the application
block of
of angles
grid is
message from
communication on
bandwidth in
and j
and bandwidth
to receive
size equal
p n
end to
to end
given processor
last block
for message
in execution
to accurately
application execution
execution times
the models
scaling beyond
of octants
that startp
corner processor
under logp
for loggp
g parameter
yields greatly
numbered p
future processor
mpi primitives
end cost
wavefront applications
mmi mk
large future
pipelined sweeps
mmi 6
octant 6
model projections
jt k
octants 5
west is
startp i
until processor
roundtrip time
6 mk
receive models
for total_comm
with mk
octants 1
separate smp
c micro
mmi and
mk it
steps appears
projects performance
the loggp model
the sp 2
of the mpi
and mpi receive
mpi send and
model of sweep3d
send and mpi
number of processors
loggp model of
the mpi send
the processor grid
problem size per
the sweeps for
size per processor
sweeps for octants
for the sweeps
t 5 6
version of sweep3d
processor p n
t 7 8
of processors time
fixed problem size
loggp model for
o and g
and j dimensions
the sweep3d application
mpi receive primitives
6 6 1000
fixed total problem
of the sweep3d
the application developers
for intra cluster
execution time for
the message from
of the processor
on the sp
the corresponding receive
l o and
the mpi communication
of table 2
m 1 l
up to 128
measured execution time
processor in the
and bandwidth in
of the sweep
at processor p
communication on the
message from the
interest to the
groups and 10
projected sweep3d execution
the sweeps from
mpi communication on
from the west
total problem size
two neighbor processors
for octants 7
message size equal
10 000 time
system is scaled
mmi 3 mk
sweep for octant
time of sweep3d
intra cluster communication
beyond one or
total problem sizes
1 loggp mmi
mk 1 loggp
the projected execution
parameters l o
of mpi send
the roundtrip communication
has been posted
to two neighbor
corresponding receive has
for fixed total
at message size
of the loggp
octants 7 and
in equation 6
the i and
the 3d grid
grid is mapped
and t 7
for messages larger
intra cluster and
messages larger than
the processing overhead
and 10 000
receive has been
or two thousand
processor grid is
to 128 processors
processors time sec
the model estimates
the ibm sp
one time step
models of the
p n m
number of angles
that are expected
n 2 l
000 time steps
from the north
values of l
6 and t
mary k vernon
last block of
1 l and
end to end
i and j
to the application
the execution time
in execution time
p i j
this research is
the time until
7 and 8
the measured execution
and the measured
in the grid
size equal to
the send and
in this research
mapped to a
of interest to
values of g
p n 1
processors in the
the critical path
the message size
complex synchronization structures
model communication cost
four processor smp
time steps appears
loggp mmi 3
single four processor
very large future
a principal factor
for the sweep3d
measured communication times
the sweeps to
pair of octants
cluster and inter
than 4kb the
problem sizes on
measured mk 10
path time for
mmi mk it
deterministic task graph
mk 10 c
sorting under logp
b 1 billion
until processor p
loggp model to
communication micro benchmarks
structure of sweep3d
m has finished
of fixed problem
in the loggp
greatly increases our
application performance and
the symmetry in
per processor grid
to 2500 processors
the sweep for
large future processor
of pipelined sweeps
of angles and
from the loggp
fast parallel sorting
smaller than 4kb
sp 2 in
c measured mk
fixed per processor
startp i j
yields greatly diminished
time with mk
to end cost
measured using simple
size time usec
neighbor processors and
sp 2 mpi
sweep3d configuration parameters
smp node in
mpi primitives the
projected execution time
equation for t
150 150 150
loggp mmi 1
grid points b
of wavefront applications
20 20 1000
one energy group
processors to its
four processor runs
mpi software for
mmi 6 mk
measured mk 1
