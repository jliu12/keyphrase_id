watanabe
singularities
learning
asymptotic
neural
bayesian
perceptron
stochastic
layered
parametric
statistical
algebraic
generalization
likelihood
estimation
artificial
layer
hierarchical
machines
mixtures
geometry
regular
contained
probability
networks
asymptotics
mathematical
statistics
curve
mixture
ork
machine
asymptotic expansion
generalization error
stochastic complexity
hierarchical learning
learning machines
learning machine
bayesian estimation
regular statistical
f n
true distribution
probability density
g n
true probability
parameter space
hidden units
neural networks
largest pole
training samples
parameter w
artificial neural
layer perceptron
algebraic geometrical
singularities in
learning curves
analytic function
parametric case
kullback information
non identifiable
distribution is
density function
priori distribution
regression function
algebraic geometry
error g
w 0
ciently large
statistical model
independently taken
priori probability
statistical models
parametric model
contained in
learning theory
probability distribution
neural network
samples independently
information matrix
analytically continued
kullback distance
algebraic variety
complexity f
statistical estimation
maximum likelihood
learning in
fisher information
geometrical structure
asymptotic theory
n w
singular points
likelihood method
natural number
parametric models
learning curve
approximation error
dimensional vectors
likelihood function
conditional probability
learning model
function approximation
ciently small
log n
previous paper
samples watanabe
true parameters
three layer perceptron
true distribution is
hierarchical learning machines
artificial neural networks
probability density function
hierarchical learning machine
regular statistical models
contained in the
regular statistical model
generalization error by
true regression function
bayesian estimation is
watanabe 1999b watanabe
resolution of singularities
true probability distribution
generalization error g
su ciently large
case when the
number of training
asymptotic expansion of
priori probability density
parameter that minimizes
parametric case when
1999b watanabe 2001a
true probability density
shun ichi amari
layer perceptron with
conditional probability density
generalization error of
error of the
extensively large then
algebraic geometrical structure
asymptotic expansion for
samples independently taken
non parametric case
geometrical structure of
asymptotic property of
respectively the largest
function approximation error
learning machine is
pole and its
perceptron with k
independently taken from
trained using samples
analytic function of
using samples independently
watanabe 2001a in
stochastic complexity f
largest pole and
number of parameters
distribution is not
neighborhood of the
neural computation v
upper bounds of
density function on
regression function is
maximum likelihood method
generalization error is
singularities in the
natural number n
ciently large n
0 is a
function on the
su ciently small
probability distribution is
distribution is contained
geometrical method we
arbitrary natural number
