classiers
mdts
meta
mdt
classier
cdp
mlc4
stacking
attributes
bla
odts
scann
c4
bagging
predictions
voting
learning
boosting
classication
vote
predicted
trees
induced
expertise
ac4
base
cml
odt
ordinary
diversity
signicantly
ltree
tac
toe
decision
hypothyroid
hepatitis
soya
diabetes
echocardiogram
ionosphere
ensembles
wine
waveform
glass
combiner
tic
correlation
classifiers
inducing
dierent
training
conf1
breast
entropy
plurality
german
bridges
australian
combining
accuracy
chess
improvement
c45
condence
conf
iris
td
heart
prediction
leaf
insignificant
distributions
cn2
car
625
nn
siers
induce
clas
bayes
probability
cdps
weka
impurity
j48
outperform
domains
comprehensible
certainty
propositional
signicant
areas
balance
knn
combine
nearest
stratied
classi
errors
correlated
frameworks
stacked
naive
modication
mining
cascading
dependence
cd
worse
pruning
dierence
0jx
1jx
14144
brazdil
111111
00385
lml
gama
maxprob
aml
accuracies
uci
ensemble
info
classied
appendix
image
discriminant
regression
weight
arbiters
classications
losses
c2
rst
ve
leaves
neighbor
density
error
ect
condent
875
expressive
c1
3a
na
validation
attribute
pavel
dierences
twenty
repository
species
predicting
votes
comprehensibility
induces
classify
identifying
concise
vs
post
ilp
asterisk
predicts
select
slope
fold
endfor
settings
cv
suite
dened
paired
renement
constructing
calculate
bernard
signicance
cn
conquer
unseen
tree
art
3b
wins
neural
pred
cross
calculated
split
expressiveness
avg
returned
enko
domingo
ibl
base level
level classiers
meta level
level attributes
meta decision
decision trees
mlc4 5
the base
class probability
level classier
c4 5
combining classiers
error correlation
relative improvement
the mdt
data set
induced using
of meta
the meta
ordinary decision
for combining
classiers the
mdts induced
predicted by
of expertise
classier c
level learning
the predictions
predictions of
learning algorithms
with mdts
induced with
of mdts
accuracy improvement
diversity of
level data
ac4 5
multiple classiers
the class
class value
tic tac
tac toe
distributions predicted
expertise of
that mdts
select best
combining multiple
probability distributions
the diversity
mdts are
predicted class
stacking with
relative areas
cdp bla
classiers in
classier is
class values
signicantly worse
ordinary attributes
bridges td
insignificant significant
breast w
using cdp
mdts over
mdts is
probability distribution
signicantly better
the classier
of classiers
k nn
examples in
plurality vote
mdt leaves
balance breast
chess diabetes
and signicantly
trees mdts
distribution properties
diabetes echocardiogram
short line
induced from
decision tree
line density
for inducing
voting schemes
class distribution
example x
learning algorithm
image ionosphere
hypothyroid image
iris soya
bagging and
its prediction
soya tic
trees induced
ionosphere iris
conf1 0
classiers c
w bridges
level classifiers
toe vote
wine insignificant
td car
echocardiogram german
constructing ensembles
vote waveform
glass heart
classication errors
given example
waveform wine
german glass
car chess
hepatitis hypothyroid
australian balance
classifiers australian
and bagging
classiers induced
heart hepatitis
of base
attributes used
boosting and
classiers are
improvement of
areas of
improvement over
data sets
the voting
is signicantly
the entropy
between base
the odt
0 625
voting scheme
ensembles of
correlation between
set m
to induce
of accuracy
of decision
the predicted
improvement achieved
see table
condence of
dierent learning
attributes bla
significant figure
mdts can
original base
conf 1
and boosting
by mdts
the combiner
tree induced
mdt induced
classication error
mdt cdp
density 5
better than
the examples
to combine
for learning
errors of
c 1
naive bayes
worse in
of c4
classiers with
trees vs
attributes are
machine learning
table 6
set l
induced on
the classication
the c4
on combining
relative accuracy
the certainty
c j
conf 2
distribution predicted
mdts with
using class
error c
cdp set
inducing meta
of mdt
stacking framework
learning mdts
the cdp
boosting of
odts induced
bagging of
with odts
current subset
with mlc4
values predicted
combine classiers
using bla
maximum probability
base level classiers
the base level
meta level attributes
meta decision trees
of the base
base level classier
the meta level
the class probability
class probability distributions
by the base
of meta level
level data set
meta level data
for combining classiers
ordinary decision trees
areas of expertise
level classiers the
class probability distribution
predicted by the
predictions of the
tic tac toe
the error correlation
distributions predicted by
base level attributes
of expertise of
of base level
combining multiple classiers
the predictions of
diversity of the
the diversity of
error correlation between
relative areas of
meta level learning
data set m
mdts induced using
set of meta
the predicted class
relative improvement over
level classiers are
probability distributions predicted
balance breast w
level classiers in
decision trees mdts
of the mdt
short line density
of decision trees
relative improvement of
the class value
tac toe vote
soya tic tac
toe vote waveform
bridges td car
hepatitis hypothyroid image
w bridges td
a given example
breast w bridges
constructing ensembles of
wine insignificant significant
german glass heart
image ionosphere iris
glass heart hepatitis
level learning algorithms
is signicantly better
of meta decision
ionosphere iris soya
conf1 0 625
australian balance breast
echocardiogram german glass
car chess diabetes
heart hepatitis hypothyroid
boosting and bagging
level classier c
hypothyroid image ionosphere
diabetes echocardiogram german
waveform wine insignificant
induced using cdp
vote waveform wine
td car chess
between base level
level classifiers australian
base level classifiers
classifiers australian balance
and signicantly worse
iris soya tic
chess diabetes echocardiogram
signicantly worse in
correlation between base
meta decision tree
ensembles of classiers
see table 6
a a a
the examples in
set of base
of the class
decision trees induced
each base level
mlc4 5 and
with mdts is
class distribution properties
decision tree induced
decision trees vs
original base level
used to induce
insignificant significant figure
of c4 5
the original base
condence of the
line density 5
achieved with mdts
dierent learning algorithms
of the diversity
shows that mdts
examples in l
of error correlation
sets of meta
diversity of errors
as meta level
relative accuracy improvement
signicantly better than
of the examples
in appendix a
of errors of
c j i
single data set
errors of the
of the error
the c4 5
6 and table
of the predictions
decision trees and
c 1 and
table 6 and
trees induced with
expertise of the
induced using the
level attributes bla
the cdp set
class values predicted
boosting of decision
two base level
of mdts over
used for classication
examples in s
classiers with mdts
the maximum probability
a meta decision
the certainty and
for combining multiple
certainty and condence
odts induced with
classiers induced by
classiers can be
domains and signicantly
area of expertise
table 10 in
induced with mlc4
cdp set of
and condence of
highest class probability
bagging of decision
the plurality vote
mlc4 5 is
base level learning
for learning mdts
the linear regression
10 in appendix
values predicted by
level classiers induced
level classiers c
c4 5 17
for constructing ensembles
distribution predicted by
method for combining
combining classiers with
accuracy of mdts
ect the certainty
with mlc4 5
and ac4 5
that mdts outperform
level attributes used
base level predictions
density 5 0
to combine classiers
bagging and boosting
one data sets
4 relative improvement
linear regression line
twenty one data
and c 2
1 and c
for further work
degree of error
decision trees is
improvement achieved with
the relative improvement
the highest class
degree of the
side of figure
a single data
used to classify
