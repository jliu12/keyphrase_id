pol
datasets
dataset
training
c4t
mda
lda
ss5
c4r
ic0
attributes
qda
categorical
fm2
ind
ocu
fm1
sas
uci
ss20
ql0
qu0
loh
tae
st0
ibo
ocm
ftl
ftu
rates
statlog
splits
quest
smo
cart
median
ic1
imo
dec
discriminant
st1
ten
records
dna
attribute
learning
univariate
pda
trees
thy
validation
classification
ocl
9m
rbf
statistical
bcw
lmt
lvq
qu1
ql1
stat
fold
provost
cmc
veh
neural
oc1
logistic
statistically
mining
bld
cal5
vot
c4
ranks
se
ib
t1
regression
plurality
rank
f90
wisc
decision
tree
breast
wolberg
classifier
cross
error rates
mean error
error rate
training time
numerical attributes
median training
training times
cross validation
decision tree
ten fold
categorical attributes
mean rank
noise attributes
uci dataset
ss5 c
fold cross
rates are
tree algorithms
from pol
this uci
the datasets
are estimated
the algorithms
using ten
the training
test set
of pol
loh and
0 se
categorical attribute
discriminant analysis
training set
s plus
each dataset
machine learning
data mining
statlog project
statistical algorithms
the statlog
1 se
statistically significantly
of mean
not statistically
other algorithms
plurality rule
fm1 and
ss20 s
combination splits
univariate splits
x marks
mean ranks
se dec
decision trees
each algorithm
pol is
c4 5
the error
from http
dec f90
www stat
stat wisc
classification accuracy
the mean
attributes and
estimated using
and st1
datasets are
logistic regression
attributes the
foster provost
rates of
significantly different
algorithms are
wisc edu
neural networks
the dataset
dec 3000
ten minutes
ic0 and
median sec
statlib s
ind cart
ibo im
shih 1997
5 9m
style ss5
pda mda
the statlib
dec sas
and shih
mean error rate
rates are estimated
error rates are
the error rates
ten fold cross
median training time
fold cross validation
estimated using ten
this uci dataset
mean error rates
the mean error
numerical attributes and
terms of mean
decision tree algorithms
using ten fold
error rates of
obtained from http
of noise attributes
different from pol
are estimated using
not statistically significantly
the statlog project
of the algorithms
statistically significantly different
for each dataset
linear combination splits
significantly different from
the test set
the training set
of the datasets
stat wisc edu
are three classes
http www stat
www stat wisc
records the error
mean rank of
number of leaves
is to predict
for each algorithm
of error rates
are estimated from
are not statistically
are two classes
statlib s archive
the plurality rule
and shih 1997
se dec f90
from pol in
style ss5 c
than ten minutes
loh and shih
at the 10
that of pol
addition of noise
the statlib s
training time versus
pol in terms
rank of error
each categorical attribute
error rate and
less than ten
rates of the
decision trees a
the fastest algorithm
the training times
peter l hammer
the class attribute
st0 and st1
error rate is
the other algorithms
from http www
data mining and
are denoted by
cross validation the
from the test
problem is to
number of datasets
analysis of variance
estimated from the
records in the
radial basis function
an error rate
set of size
error rate for
ocu ocl ocm
by ten fold
sec ftu c4r
with univariate splits
have median training
ib ibo im
