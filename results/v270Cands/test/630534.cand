featuremine
mining
winnow
pruning
fire
features
wwwwww
frequent
spelling
training
feature
classification
ignite
eid
idlist
bulldozer
maxw
raises
classifiers
freq
sequences
parity
idlists
betting
classifier
distinctive
bb
bayes
bet
time8
subsequence
word
time0
mine
plan
irrelevant
sequential
suffix
bets
ab
tag
subsequences
bd2
digat
spade
bd1
y8
naive
prune
p3
pos
mined
accuracy
covers
label
bc
000
y4
domains
frequency
redundant
vs
terrain
x3
547
x7
millions
boolean
dataset
folds
p2
weights
bulldozers
truei
burned
moveto
politics
time32
time6
subsumes
chess
selecting
correction
traces
meta
dna
correlated
subsumed
corpus
eids
classification algorithms
features produced
feature set
by featuremine
frequent sequences
the fire
wwwwww wwwwww
feature value
class c
the features
feature f
pruning rules
training examples
data mining
value pairs
naive bayes
feature mining
d bb
featuremine algorithm
1 raises
bb d
min freq
the featuremine
pruning rule
of features
features to
classification accuracy
boolean features
fire world
sequential domains
f 1
features for
produced by
d d
standard classification
spelling correction
features which
mining techniques
f i
features that
selecting features
as features
execution traces
each example
ith bet
frequent for
b pruning
www www
the idlists
mined features
sequence mining
non sequential
the feature
the training
class label
exponentially large
decision lists
max l
be frequent
sequence data
target word
of feature
of examples
dataset there
all frequent
features and
for classification
to class
features are
the frequency
1 covers
test examples
examples and
correlated with
examples in
mining algorithm
frequent and
potential features
features out
2 covers
meta features
betting sequences
75 75
features features
1 547
sequence lattice
sequential features
parent partition
observable features
raises twice
features produced by
the features produced
produced by featuremine
feature value pairs
wwwwww wwwwww wwwwww
feature f i
d bb d
the featuremine algorithm
p 1 raises
bb d d
data mining techniques
that the features
number of features
class c j
examples in d
f 1 covers
d d bb
dataset there were
of feature value
test examples and
a b pruning
the ith bet
standard classification algorithms
all frequent sequences
criteria for selecting
the target word
d d d
for class c
to class c
mining techniques to
in the training
f 2 covers
this feature set
www www www
of potential features
sequences as features
be subsumed by
the feature set
used a min
each feature f
each parent partition
the mining algorithm
1 547 122
the idlist for
a sequence is
will be subsumed
the idlists of
features out of
the fire world
context sensitive spelling
sensitive spelling correction
for selecting features
features should be
space of all
an instance is
the a b
for each example
that f 2
of examples in
features that are
data mining algorithms
of all subsets
set of examples
the frequency of
all subsets of
we used a
the training examples
of the features
an example simulation
and disk based
examples d and
and max l
the sequence lattice
the meta features
mined features from
web usage data
a b relations
evidence for different
1 raises and
freq maxw and
1 true f
be frequent for
for describing each
the resulting idlist
this enables it
ones played by
scalable and disk
75 75 75
based on length
we search over
