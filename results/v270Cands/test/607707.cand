watanabe
singularities
learning
eq
asymptotic
neural
pole
bayesian
amari
blowing
identifiable
perceptron
stochastic
2001a
kullback
expansion
dw
layered
parametric
statistical
algebraic
samples
analytic
density
poles
1999b
shun
murata
geometrical
generalization
units
ciently
likelihood
exp
estimation
training
regression
neighborhood
clarify
ichi
priori
dxdy
opper
sumio
su
artificial
hidden
layer
resolution
hierarchical
ups
aic
hironaka
haussler
machines
fisher
ect
ozeki
1974
mixtures
curves
nh
inequality
meromorphic
zeta
unrealizable
fukumizu
atiyah
solla
tomoko
geometry
regular
nk
compact
singular
jw
inequalities
bias
trained
foregoing
entropic
sato
tishby
bic
largest
contained
mellin
conic
minimizes
probability
asymptotic expansion
generalization error
stochastic complexity
hierarchical learning
learning machines
the true
learning machine
bayesian estimation
regular statistical
h w
f n
the generalization
the parameter
j z
three layer
true distribution
probability density
g n
an asymptotic
the stochastic
true probability
parameter space
the bayesian
y x
hidden units
x w
neural networks
largest pole
p y
w j
training samples
parameter w
artificial neural
layer perceptron
n has
watanabe 2001a
algebraic geometrical
w dw
singularities in
learning curves
assumption a
of singularities
analytic function
the learning
parametric case
kullback information
non identifiable
by eq
distribution is
density function
su ciently
the kullback
w is
pole of
priori distribution
regression function
algebraic geometry
1999b watanabe
true regression
watanabe 1999b
error g
q x
the parametric
w 0
not contained
the asymptotic
w w
blowing up
an analytic
neural computation
ciently large
q y
statistical model
g u
of learning
of w
blowing ups
extensively large
shun ichi
independently taken
ichi amari
priori probability
a priori
statistical models
of training
parametric model
neighborhood of
compact then
eq 30
and eq
the generalization error
the stochastic complexity
an asymptotic expansion
p y x
has an asymptotic
y x w
n has an
the true distribution
the parameter space
three layer perceptron
true distribution is
hierarchical learning machines
when the true
the true probability
g n has
a three layer
the largest pole
of training samples
in the parametric
artificial neural networks
probability density function
the kullback information
hierarchical learning machine
regular statistical models
of j z
the a priori
of the stochastic
of the generalization
the learning machine
a priori distribution
if g n
contained in the
h w is
regular statistical model
the true regression
the bayesian estimation
h w z
generalization error by
true regression function
a regular statistical
an analytic function
bayesian estimation is
watanabe 1999b watanabe
resolution of singularities
true probability distribution
q y x
generalization error g
the assumption a
the asymptotic expansion
error g n
parameter w j
su ciently large
of the parameter
case when the
the parameter w
in the parameter
is not contained
not contained in
number of training
asymptotic expansion of
on the parameter
g n is
a priori probability
hidden units and
function of w
priori probability density
z w dw
the parametric model
of singularities in
parameter that minimizes
a hierarchical learning
parametric case when
support of w
f n has
1999b watanabe 2001a
then g n
true probability density
y x q
the parameter that
shun ichi amari
layer perceptron with
of learning curves
conditional probability density
w z w
the maximum likelihood
generalization error of
that the generalization
for an arbitrary
that the true
error of the
